{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____\n",
    "# Exploring Word Frequencies\n",
    "\n",
    "**Description:**\n",
    "This [notebook](https://constellate.org/docs/key-terms/#jupyter-notebook) shows how to find the most common words in a\n",
    "[dataset](https://constellate.org/docs/key-terms/#dataset). The following processes are described:\n",
    "\n",
    "* Using the `constellate` client to create a Pandas DataFrame\n",
    "* Filtering based on a pre-processed ID list\n",
    "* Filtering based on a [stop words list](https://constellate.org/docs/key-terms/#stop-words)\n",
    "* Using a `Counter()` object to get the most common words\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 60 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* [Working with Dataset Files](./working-with-dataset-files.ipynb)\n",
    "* [Pandas I](./pandas-1.ipynb)\n",
    "* [Counter Objects](./counter-objects.ipynb)\n",
    "* [Creating a Stopwords List](./creating-stopwords-list.ipynb)\n",
    "\n",
    "**Data Format:** [JSON Lines (.jsonl)](https://constellate.org/docs/key-terms/#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* [constellate](https://constellate.org/docs/key-terms/#tdm-client) client to collect, unzip, and read our dataset\n",
    "* [NLTK](https://constellate.org/docs/key-terms/#nltk) to help [clean](https://constellate.org/docs/key-terms/#clean-data) up our dataset\n",
    "* [Counter](https://constellate.org/docs/key-terms/#python-counter) from **Collections** to help sum up our word frequencies\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Build a dataset\n",
    "2. Create a \"Pre-Processing CSV\" with [Exploring Metadata](./exploring-metadata.ipynb) (Optional)\n",
    "3. Create a \"Custom Stopwords List\" with [Creating a Stopwords List](./creating-stopwords-list.ipynb) (Optional)\n",
    "4. Complete the word frequencies analysis with this notebook\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset\n",
    "\n",
    "We'll use the `constellate` client to automatically retrieve the dataset in the JSON file format. \n",
    "\n",
    "Enter a [dataset ID](https://constellate.org/docs/key-terms/#dataset-ID) in the next code cell.\n",
    "\n",
    "If you don't have a dataset ID, you can:\n",
    "* Use the sample dataset ID already in the code cell\n",
    "* [Create a new dataset](https://constellate.org/builder)\n",
    "* [Use a dataset ID from other pre-built sample datasets](https://constellate.org/dataset/dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the `constellate` client, passing the `dataset_id` as an argument using the `get_dataset` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing your dataset with a dataset ID\n",
    "import constellate\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stopwords List\n",
    "\n",
    "If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we'll load the NLTK [stopwords](https://constellate.org/docs/key-terms/#stop-words) list automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom data/stop_words.csv if available\n",
    "# Otherwise, load the nltk stopwords list in English\n",
    "\n",
    "# Create an empty Python list to hold the stopwords\n",
    "stop_words = []\n",
    "\n",
    "# The filename of the custom data/stop_words.csv file\n",
    "stopwords_list_filename = 'data/stop_words.csv'\n",
    "\n",
    "if os.path.exists(stopwords_list_filename):\n",
    "    import csv\n",
    "    with open(stopwords_list_filename, 'r') as f:\n",
    "        stop_words = list(csv.reader(f))[0]\n",
    "    print('Custom stopwords list loaded from CSV')\n",
    "else:\n",
    "    # Load the NLTK stopwords list\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    print('NLTK stopwords list loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather unigrams again with extra cleaning steps\n",
    "In addition to using a stopwords list, we will clean up the tokens by lowercasing all tokens and combining them. This will combine tokens with different capitalization such as \"quarterly\" and \"Quarterly.\" We will also remove any tokens that are not alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather unigramCounts from documents in `filtered_id_list` if available\n",
    "# and apply the processing.\n",
    "\n",
    "word_frequency = Counter()\n",
    "\n",
    "for document in constellate.dataset_reader('data/my_data.jsonl.gz'):\n",
    "    unigrams = document.get(\"unigramCount\", [])\n",
    "    for gram, count in unigrams.items():\n",
    "        clean_gram = gram.lower()\n",
    "        if clean_gram in stop_words:\n",
    "            continue\n",
    "        if not clean_gram.isalpha():\n",
    "            continue\n",
    "        if len(clean_gram) < 4:\n",
    "            continue\n",
    "        word_frequency[clean_gram] += count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "Finally, we will display the 20 most common words by using the `.most_common()` method on the `Counter()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the most common processed unigrams and their counts\n",
    "for gram, count in word_frequency.most_common(25):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to a CSV File\n",
    "The word frequency data can be exported to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output method to csv\n",
    "import csv\n",
    "\n",
    "with open(f'./data/word_frequencies.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['unigram', 'count'])\n",
    "    for gram, count in word_frequency.most_common():\n",
    "        writer.writerow([gram, count])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Word Cloud to Visualize the Data\n",
    "A visualization using the WordCloud library in Python. To learn more about customizing a wordcloud, [see the documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download cloud image for our word cloud shape\n",
    "import urllib.request\n",
    "download_url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sample_cloud.png'\n",
    "urllib.request.urlretrieve(download_url, './data/sample_cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud from our data\n",
    "\n",
    "# Adding a mask shape of a cloud to your word cloud\n",
    "# By default, the shape will be a rectangle\n",
    "# You can specify any shape you like based on an image file\n",
    "cloud_mask = np.array(Image.open('./data/sample_cloud.png')) # Specifies the location of the mask shape\n",
    "cloud_mask = np.where(cloud_mask > 3, 255, cloud_mask) # this line will take all values greater than 3 and make them 255 (white)\n",
    "\n",
    "### Specify word cloud details\n",
    "wordcloud = WordCloud(\n",
    "    width = 800, # Change the pixel width of the image if blurry\n",
    "    height = 600, # Change the pixel height of the image if blurry\n",
    "    background_color = \"white\", # Change the background color\n",
    "    colormap = 'viridis', # The colors of the words, see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    max_words = 150, # Change the max number of words shown\n",
    "    min_font_size = 4, # Do not show small text\n",
    "    \n",
    "    # Add a shape and outline (known as a mask) to your wordcloud\n",
    "    contour_color = 'blue', # The outline color of your mask shape\n",
    "    mask = cloud_mask, # \n",
    "    contour_width = 1\n",
    ").generate_from_frequencies(word_frequency)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20,20) # Change the image size displayed\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
