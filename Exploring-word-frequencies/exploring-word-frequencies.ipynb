{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\" align=left alt=\"CC BY license logo\" /><br /><br />\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "**For questions/comments/improvements, email nathan.kelber@ithaka.org.**<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Word Frequencies\n",
    "\n",
    "**Description:**\n",
    "This notebook shows how to find the most common words in a\n",
    "dataset. The following processes are described:\n",
    "\n",
    "* Filtering based on a pre-processed ID list\n",
    "* Filtering based on a stop words list\n",
    "* Using a `Counter()` object to get the most common words\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 60 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](../Python-basics/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* [Python Intermediate 2](../Python-intermediate/python-intermediate-2.ipynb)\n",
    "* [Pandas I](../Pandas-basics/pandas-basics-1.ipynb)\n",
    "* [Counter Objects](../Counter-objects/counter-objects.ipynb)\n",
    "* [Creating a Stopwords List](../Stopwords/creating-stopwords-list.ipynb)\n",
    "\n",
    "**Data Format:** JSON Lines (.jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* NLTK to help clean up our dataset\n",
    "* Counter from **Collections** to help sum up our word frequencies\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Build a dataset\n",
    "2. Create a \"Pre-Processing CSV\" with [Exploring Metadata](../Exploring-metadata/exploring-metadata.ipynb) (Optional)\n",
    "3. Create a \"Custom Stopwords List\" with [Creating a Stopwords List](../Stopwords/creating-stopwords-list.ipynb) (Optional)\n",
    "4. Complete the word frequencies analysis with this notebook\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install wordcloud\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# For making wordclouds\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# For making wordclouds\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset\n",
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the compressed JSONL file containing metadata, ngrams and full texts to the current working directory.&lt; / &gt; </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path to the jsonl file in the current directory\n",
    "dataset_file = '' # copy and paste the path to the jsonl.gz file \n",
    "\n",
    "# function that reads a jsonl.gz file into a generator\n",
    "def dataset_reader(file_path):\n",
    "    \"\"\"\n",
    "    Helper to read in gzip files and yield Python dictionary\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Unigram Counts from the JSON file (No cleaning)\n",
    "\n",
    "The dataset file is a compressed JSON Lines file (jsonl.gz) that contains all the metadata information found in the metadata CSV *plus* the textual data necessary for analysis including:\n",
    "\n",
    "* Unigram Counts\n",
    "* Bigram Counts\n",
    "* Trigram Counts\n",
    "* Full-text (if available)\n",
    "\n",
    "To complete our analysis, we are going to pull out the unigram counts for each document and store them in a Counter() object. We will import `Counter` which will allow us to use Counter() objects for counting unigrams. Then we will initialize an empty Counter() object `word_frequency` to hold all of our unigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Counter()\n",
    "from collections import Counter\n",
    "\n",
    "# Create an empty Counter object called `word_frequency`\n",
    "word_frequency = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read in each document using the `.dataset_reader()` method. This method unzips each document and yields the document's data one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gather unigramCounts from documents\n",
    "i = 0\n",
    "for document in dataset_reader(dataset_file):\n",
    "    unigrams = document.get(\"unigramCount\", [])\n",
    "    for gram, count in unigrams.items():\n",
    "        word_frequency[gram] += count\n",
    "    i += 1\n",
    "\n",
    "# Print success message\n",
    "print(f'The unigrams from {i} documents were collected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most Common Unigrams\n",
    "Now that we have a list of the frequency of all the unigrams in our corpus, we need to sort them to find which are most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for gram, count in word_frequency.most_common(25):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some issues to consider\n",
    "\n",
    "We have successfully created a word frequency list. There are a couple small issues, however, that we still need to address:\n",
    "1. There are many function words, words like \"the\", \"in\", and \"of\" that are grammatically important but do not carry as much semantic meaning like content words, such as nouns and verbs.\n",
    "2. The words represented here are actually case-sensitive strings. That means that the string \"the\" is a different from the string \"The\". You may notice this in your results above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Unigram Counts from the JSON File (with cleaning)\n",
    "To address these issues, we need to find a way to remove common function words and combine strings that may have capital letters in them. We can address these issues by:\n",
    "\n",
    "1. Using a stopwords list to remove common function words\n",
    "2. Lowercasing all the characters in each string to combine our counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stopwords List\n",
    "\n",
    "If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we'll load the NLTK stopwords list automatically.\n",
    "\n",
    "We recommend storing your stopwords in a CSV file as shown in the [Creating Stopwords List](../Stopwords/creating-stopwords-list.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load a custom data/stop_words.csv if available\n",
    "# Otherwise, load the nltk stopwords list in English\n",
    "import csv\n",
    "# Create an empty Python list to hold the stopwords\n",
    "stop_words = []\n",
    "\n",
    "# The filename of the custom data/stop_words.csv file\n",
    "stopwords_path = Path.cwd() / 'data' / 'stop_words.csv'\n",
    "\n",
    "if stopwords_path.exists():\n",
    "    with stopwords_path.open() as f:\n",
    "        stop_words = list(csv.reader(f))[0]\n",
    "    print('Custom stopwords list loaded from CSV')\n",
    "else:\n",
    "    # Load the NLTK stopwords list\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Create a CSV file to store an initial set of stopwords\n",
    "    with open('./data/stop_words.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(stop_words)\n",
    "    print('NLTK stopwords list loaded and saved to stopwords.csv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather unigrams again with extra cleaning steps\n",
    "In addition to using a stopwords list, we will clean up the tokens by lowercasing all tokens and combining them. This will combine tokens with different capitalization such as \"quarterly\" and \"Quarterly.\" We will also remove any tokens that are not alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gather unigramCounts from documents and apply the processing\n",
    "\n",
    "word_frequency = Counter()\n",
    "\n",
    "# path to the jsonl file in the current directory\n",
    "dataset_file = '' # copy and paste the path to the JSONL file \n",
    "\n",
    "for document in dataset_reader(dataset_file):\n",
    "    unigrams = document.get(\"unigramCount\", [])\n",
    "    for gram, count in unigrams.items():\n",
    "        clean_gram = gram.lower()\n",
    "        if clean_gram in stop_words:\n",
    "            continue\n",
    "        if not clean_gram.isalpha():\n",
    "            continue\n",
    "        if len(clean_gram) < 4:\n",
    "            continue\n",
    "        word_frequency[clean_gram] += count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "Finally, we will display the 20 most common words by using the `.most_common()` method on the `Counter()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the most common processed unigrams and their counts\n",
    "for gram, count in word_frequency.most_common(25):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to a CSV File\n",
    "The word frequency data can be exported to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add output method to csv\n",
    "\n",
    "csv_file = Path.cwd() / 'data' / 'word_counts.csv'\n",
    "\n",
    "with csv_file.open('w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['unigram', 'count'])\n",
    "    for gram, count in word_frequency.most_common():\n",
    "        writer.writerow([gram, count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Word Cloud to Visualize the Data\n",
    "A visualization using the WordCloud library in Python. To learn more about customizing a wordcloud, [see the documentation](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It is not required to have a shape to create a word cloud\n",
    "cloud_path = '../All-sample-files/sample_cloud.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a wordcloud from our data\n",
    "\n",
    "# Adding a mask shape of a cloud to your word cloud\n",
    "# By default, the shape will be a rectangle\n",
    "# You can specify any shape you like based on an image file\n",
    "cloud_mask = np.array(Image.open('../All-sample-files/sample_cloud.png')) # Specifies the location of the mask shape\n",
    "cloud_mask = np.where(cloud_mask > 3, 255, cloud_mask) # this line will take all values greater than 3 and make them 255 (white)\n",
    "\n",
    "### Specify word cloud details\n",
    "wordcloud = WordCloud(\n",
    "    width = 800, # Change the pixel width of the image if blurry\n",
    "    height = 600, # Change the pixel height of the image if blurry\n",
    "    background_color = \"white\", # Change the background color\n",
    "    colormap = 'viridis', # The colors of the words, see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    max_words = 150, # Change the max number of words shown\n",
    "    min_font_size = 4, # Do not show small text\n",
    "    \n",
    "    # Add a shape and outline (known as a mask) to your wordcloud\n",
    "    contour_color = 'blue', # The outline color of your mask shape\n",
    "    mask = cloud_mask, # \n",
    "    contour_width = 1\n",
    ").generate_from_frequencies(word_frequency)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20,20) # Change the image size displayed\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
