{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca78ac9-73f6-48d5-acec-de4cda66a422",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"../All-sample-files/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d90cbb-6457-4eed-855f-4306db752076",
   "metadata": {},
   "source": [
    "# Language Models 1: The Basics\n",
    "\n",
    "**Description:** This lesson is an introduction to language models. Learners will:\n",
    "\n",
    "* Learn about language models\n",
    "* Learn about Hugging Face\n",
    "* Create API Keys for Open AI and Hugging Face\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "\n",
    "**Data Format:** \n",
    "\n",
    "**Libraries Used:** \n",
    "* [Huggingface_hub](https://github.com/huggingface/huggingface_hub)- connects the user to models through an API\n",
    "* [Open AI](https://platform.openai.com/docs/overview)- connects the user to models through an API\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9a6ad-c1b1-4c58-a24c-56ed2979d9b7",
   "metadata": {},
   "source": [
    "# What is generative AI?\n",
    "Generative AI is a subset of artificial intelligence technologies focused on *generating* novel outputs, such as text, image, audio, and video content. For example, textual content can be generated with a *language model*, a statistical model trained on a large group of texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f40318-f6cc-4872-a87d-40b3169347c8",
   "metadata": {},
   "source": [
    "# What are language models?\n",
    "Language models are statistical models made for predicting text, which can be used to generate textual outputs in the language(s) of their training data. A model is trained to produce the kinds of language patterns it sees in its training data; the complexity of the language it generates depends on the nature and diversity of the training data. The model itself is a series of files containing \"weights\" which determine how to generate a text piece-by-piece. The \"weights\" are the optimized results of a neural network learning to predict the language found in its training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58304018-5e7b-40a0-b132-77da9713efc8",
   "metadata": {},
   "source": [
    "# What are the limitations of language models?\n",
    "\n",
    "## Ethical Concerns\n",
    "\n",
    "*Bias*- Language models generate outputs based on their training data. If the training data contains biases, then they will be reflected in the language generated. \n",
    "\n",
    "*Hallucination*- Models are tasked with generating outputs, but those outputs may be false if the model lacks sufficient training data. The issue is exacerbated by the fact that the choices made by most language models are made using statistical analysis, which can make explaining the source of the hallucination in human-accessible language impossible.\n",
    "\n",
    "*Transparency*- Model builders vary greatly in the level of transparency of their model. For example, the only way to use models from Open AI, Google, and Anthropic is to send a message to them directly and they will give you the model's response. Meta has made the underlying weights of its Llama model available, but it does restrict licensing of the model for particular uses and it does not reveal the sources of its training data. This differs from EleutherAI, which makes its model weights and training data available.\n",
    "\n",
    "*Reproducibility*- By design, there is some randomness to the language generated by models. We can adjust a parameter known as \"temperature\" in some models which greatly reduces randomness (but also creativity). \n",
    "\n",
    "*Ecological Impact*- The training of large language models uses significant resources which contribute to climate change. Once a model has been trained, querying the model—called inference—uses significantly less energy. However, for the largest models (such as GPT 4.5), the energy usage for inference is still significant.\n",
    "\n",
    "## Knowledge Gaps\n",
    "\n",
    "A language model cannot access language it is not trained on, nor can it access language produced after it was created. The underlying model is \"frozen\" at the time it is created and limited by the training data it receives. There have been some developments to address these gaps, namely the introduction of:\n",
    "\n",
    "*Fine-Tuning and Transfer Learning*- Instead of training a new model from scratch, the methods of fine-tuning and transfer learning update the model's neural network layers. This is important since training very large models can be time-consuming, expensive, and ecologically damaging. \n",
    "\n",
    "*Retrieval Augmented Generation (RAG)*- Instead of changing the model itself, the model is supplied relevant context from a *knowledge base* (a set of current, relevant documents). By supplying the model with relevant information at the time of the query, it can craft better responses without having to modify the original model weights. Another benefit is that when models hallucinate, we can usually trace the source of the information to a specific document and chunk of text in the base. In essence, they can cite knowledge base sources, and we can consider the impact of those sources on their outputs.\n",
    "\n",
    "*Chain of Thought and Reasoner Models*- Instead of offering the fastest output, models are prompted to chain together a series of step-by-step outputs, what is sometimes called \"chain of thought\". The output of each step becomes and input for the next step. This can be helpful for more complex tasks such as research, and it can help discover the source of hallucinations and logic errors since each step contains a \"thought\", which helps indicate how the model is tackling the problem.\n",
    "\n",
    "*Agentic Models*- In 2025, there has been shift to thinking of models as working in teams of *agents*. At the same time, we are witnessing the first AI models to traverse the world wide web. With the advent of models that can interpret images (and video as a sequence of images), models such as Open AI's Operator can use computers by analyzing discrete images and then issuing mouse clicks and keyboard presses. Complex tasks can be split up and assigned to various agents who then take actions and share outputs with one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3113ad1b-ddc6-468e-845f-8d0d73273ec7",
   "metadata": {},
   "source": [
    "# How to use Language Models\n",
    "\n",
    "## Chat App vs. Programmatic Access\n",
    "Most users currently interact with language models through chat application interfaces such as ChatGPT or Gemini. A user can submit a single prompt that may contain text, images, and/or files and receive a single response back. In addition to being very accessible (no programming required!), chat applications often have specialized features that a language model will not. For example, Google Gemini has some capabilities that work with Google Drive and Google Assistant. If you're interested in trying chat applications focused on educational users, take a look at ITHAKA S&R's [Product Tracker](https://docs.google.com/document/d/1yg7KJmMl7d_xZAGgHiXc-9iSNT5vmmp1iyK5zYcS2IE/edit?tab=t.0).\n",
    "\n",
    "While chat interfaces are the most accessible, there are times when working with a language model in a programmatic fashion is better. \n",
    "It is also possible to work with a language model directly through an API or on your local machine. Working with a model directly is a good choice in the following circumstances:\n",
    "\n",
    "* The model does not have a chat interface (or is not designed for chat)\n",
    "* You need more control over parameters not found in the chat interface (temperature, role, or some other facet)\n",
    "* You want to keep sensitive data on your local network or machine\n",
    "* You want to automate a series of tasks without submitting a new prompt each time\n",
    "\n",
    "For those who do not code (and even some who do), it is possible to create automation workflows with language models through GUI tools like [Zapier](https://zapier.com/) and [Make](https://www.make.com/en). For those comfortable with Python, the opportunities are significant, especially for working with cutting-edge technologies such as RAG and agents.\n",
    "\n",
    "## API vs. Local\n",
    "\n",
    "*Using an API*- In general, using a model through an Application Programming Interface is the best choice. The model is hosted on commercial server hardware and you send a request through Python (or another language). The server processes the request and sends a response. If the model is proprietary such as GPT-4o, then an API may be the only option for using it directly. Besides licensing issues, the largest models (such as Llama 3.1 405b) are not going to run well (or at all) on even high-end, consumer-grade hardware. \n",
    "\n",
    "*Running models locally*- Models with open weights and licensing can be downloaded and run on a local machine. This could be useful if you are working with sensitive data that should not leave your machine or if you would like to improve the model, such as through fine-tuning or transfer learning. The downside to this approach is that your machine may need cutting-edge hardware to work with large models or execute training. A strong graphics card and significant RAM may be necessary. However, if you are using a medium model for inference (not training), then a relatively recent laptop can be perfectly adequate. If you are interested in running models locally, then check out [GPT4All](https://www.nomic.ai/gpt4all), which can get you started with a desktop chat client and/or Python development. Another great source for working with models locally on the command line is [ollama](https://ollama.com/).\n",
    "\n",
    "# How many language models are there?\n",
    "\n",
    "There are hundreds of thousands of language models, which vary in application, size, complexity, accessibility, cost, openess, and other factors. Some can be easily run on a laptop, while others can only be run on high-end consumer hardware or commercial-grade servers. Popular chat services like ChatGPT, Gemini, and Claude rely on a variety of large language models, whose names and specifications may not be apparent to casual users. As of 2025, the free version of ChatGPT relies on a model called GPT-4o. However, paid users can access [over a dozen models from Open AI](https://platform.openai.com/docs/models) which vary in size, cost, and application. \n",
    "\n",
    "## Examples of Popular Models from Large American Technology Companies in 2025\n",
    "\n",
    "**Anthropic**\n",
    "* Claude 3.7 Sonnet\n",
    "* Claude 3.5 Haiku\n",
    "* Claude 3 Opus\n",
    "\n",
    "**Google**\n",
    "* Chirp (audio language)\n",
    "* Gemini 2.0 Pro\n",
    "* PaLm 2 for Text\n",
    "\n",
    "**Microsoft**\n",
    "* Phi 3.5\n",
    "\n",
    "**Meta**\n",
    "* Llama 3.2\n",
    "* Seamless (audio language)\n",
    "\n",
    "**Open AI**\n",
    "* GPT-4o\n",
    "* o1\n",
    "* Whisper (audio language)\n",
    "\n",
    "# What is Hugging Face?\n",
    "\n",
    "Hugging Face is a community for AI development with a variety of resources such as models, spaces, datasets, and forums. Hugging Face contains [the largest repository of AI models](https://huggingface.co/models), including large language models. As of 2025, there are about 1.5 million models on Hugging Face. These models tend to be more transparent and permissive with licenses than Big Tech models. In terms of capability and benchmarks, cutting-edge open models are often comparable or slightly behind Big Tech models. They usually do not have a comparable level of software development support in the form of documentation, software development kits, and community support. They offer better freedom, access, and control over accessibility and support. Here are some additional corporations and organizations building open-ish LLMs available on Hugging Face:\n",
    "\n",
    "**Allen Institute for AI (AI2)**- A non-profit creating leading open models, including Tülu 3 and OLMoE\n",
    "\n",
    "**Alibaba Cloud**- Part of the Chinese multinational Alibaba Group, makers of the Qwen models which benchmark on par with the best models in the world\n",
    "\n",
    "**AI21Labs**- An Israeli company, creators of the Jamba family of open models\n",
    "\n",
    "**Baidu**- A Chinese multinational, famous for the Ernie model open sourcing in June 2025\n",
    "\n",
    "**Cohere4AI**- A Canadian multinational created by some co-authors of the \"Attention is All you Need\" paper, known for the command models\n",
    "\n",
    "**Deepseek**- A Chinese company, which shook the AI industry with the state of the art Deepseek-R1 model in 2024\n",
    "\n",
    "**Mistral**- A French startup, specializing in open-weight models with a variety of licenses\n",
    "\n",
    "**Perplexity**- American startup, creators of Sonar which offer the Perplexity AI platform\n",
    "\n",
    "**Technology Innovation Institute**- A research institute in UAE, creators of the Falcon family of models\n",
    "\n",
    "If you are interested in trying prompts on a large variety of models without using Python, try using the [Hugging Face Playground](https://huggingface.co/playground) or [Perplexity](https://www.perplexity.ai/) (Requires Pro Account)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a5981-0d23-45d1-b380-a7b617c79835",
   "metadata": {},
   "source": [
    "# Create an API Key for Hugging Face\n",
    "\n",
    "1. [Create a Hugging Face Account](https://huggingface.co/)\n",
    "   \n",
    "![Steps for creating an access token](../All-sample-files/hf-api-key.png)\n",
    "\n",
    "2. Hugging Face calls their keys \"Access Tokens\". You can create one by selecting your profile in the upper-righthand corner, then \"Settings\", then \"Access Tokens\", and finally \"+ Create new token\".\n",
    "\n",
    "![Permissions to select for your token](../All-sample-files/hf-token-permissions.png)\n",
    "\n",
    "3. Give the token a descriptive name and make sure that under \"Inference\" you have checked the box \"Make calls to the serverless Inference API\" if you're using a \"Fine-grained\" token type. The \"Read\" and \"Write\" token types have permission to make calls by default.\n",
    "\n",
    "4. Store the API token in a secure place, such as a text file on your local machine. It's a good practice not to paste the token into a code cell or other notebook location where it may be visible when the notebook is shared with others. If you lose your API token, delete the old token and create a new one.\n",
    "\n",
    "5. Optionally, you may create a \"pro account\" for increased rate limits and access to high-end models. To create the account, select your profile in the upper-righthand corner, then \"Settings\", and finally \"Billing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81860714",
   "metadata": {},
   "source": [
    "# Try the Hugging Face API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d64063-415c-4ff0-bd9d-57d726d6a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface_hub to connect to the provider's models\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fbb4c-008e-458f-a4e8-1037d78f6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import getpass and Hugging Face Login and Inference Client\n",
    "import getpass\n",
    "from huggingface_hub import InferenceClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f21a4ea-23d6-4e3f-9eab-9d7297145aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Hugging Face API Key in a variable\n",
    "# The getpass function obscures your token if you share the notebook with others\n",
    "\n",
    "hf_api_token = getpass.getpass('Enter your Hugging Face API Token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70a797-cbbd-46ee-97ac-27a9382c492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize InferenceClient with your Hugging Face token and the model ID\n",
    "client = InferenceClient(token=hf_api_token) \n",
    "\n",
    "# Define the prompt for text generation\n",
    "prompt = \"What is Hugging Face?\"\n",
    "\n",
    "# Generate text using the specified model and prompt\n",
    "generated_text = client.text_generation(prompt=prompt, model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16650b1-a519-4537-a68d-f69b3673a234",
   "metadata": {},
   "source": [
    "# Create an API Key for Open AI\n",
    "\n",
    "Open AI is one of the best model providers, offering state of the art models at various price points. See more information on [Open AI models](https://platform.openai.com/docs/models). On the privacy front, Open AI does not use data sent through the API for training.\n",
    "\n",
    "1. [Create an Open AI account](https://platform.openai.com/docs/overview)\n",
    "\n",
    "![To add credits, select the gear icon, then \"billing\", then \"Add to credit balance\".](../All-sample-files/openai-add-credits.png)\n",
    "\n",
    "2. Add a credit to your account. We recommend keeping \"auto-recharge\" off, so charges cannot be run up if your API key becomes compromised. $5 is plenty to get started.\n",
    "\n",
    "![To create a key, select \"Dashboard\", then \"API Keys\", then \"+ Create a new secret key\"](../All-sample-files/openai-create-key.png)\n",
    "\n",
    "3. Create a new secret key. Treat the key like a password and store it somewhere safe, such as a digital text file on your local machine. When the key is created, copy it somewhere safe immediately. Once the window closes, the key cannot be shown again. If you lose the key, the only option is to delete the old key and create a new one. Do not write the key directly into the notebook, especially if you plan to share it with others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e37a1",
   "metadata": {},
   "source": [
    "# Try the Open AI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878eeb82-d672-400f-a426-5e19e9516f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Open AI \n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b41a63-e1f8-40a7-a47a-da8c866b011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import getpass and Open AI\n",
    "import getpass\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3d7fd-38ef-45b5-b59f-eda911efa71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Hugging Face API Key in a variable\n",
    "# The getpass function obscures your token if you share the notebook with others\n",
    "\n",
    "openai_api_key = getpass.getpass('Enter your Open API Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0837b4-0d55-47b9-a9f2-47f5dc4510bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load key from environment variable\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4149e8-698f-4718-af88-25c271b2dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\", \n",
    "    input=\"What is a language model?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d35e22-6ac1-44ac-9577-14d308c71a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
