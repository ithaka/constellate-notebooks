{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5cc179e-7173-4809-b04a-90a5b93fb321",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b027f5-6fa4-4494-82c1-aaa48a2e957b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Models 1: Jupyter AI\n",
    "\n",
    "**Description:** This lesson is an introduction to the Jupyter AI Extension. Learners will:\n",
    "\n",
    "* Create LLM API Keys with Open AI and Hugging Face\n",
    "* Configure the Jupyternaut chatbot\n",
    "* Explore Jupyternaut commands, including document vectorization\n",
    "* Explore Jupyter Magic commands `%ai` and `%%ai`\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "* Basic knowledge of Large Language Models (LLMs)\n",
    "\n",
    "**Data Format:** .txt\n",
    "\n",
    "**Libraries Used:** \n",
    "* [Huggingface_hub](https://github.com/huggingface/huggingface_hub)- connects the user to models through an API\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b93a8f-7b60-48b5-868f-f9a7b920438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install huggingface_hub to connect to the provider's models\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84a638-2dcc-4b52-a6c9-cd23e45db614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Introduction to Jupyter AI\n",
    "\n",
    "The [Jupyter AI extension](https://jupyter-ai.readthedocs.io/en/latest/) is useful for two reasons. First, it contains a chatbot named Jupyternaut which opens in a separate pane. The chatbot application can help you write code by:\n",
    "\n",
    "* Answering general programming questions\n",
    "* Answering specific questions about your code\n",
    "* Answering specific questions about your files\n",
    "* Suggesting improvements to your code\n",
    "* Writing a basic notebook from scratch\n",
    "\n",
    "Second, the Jupyter AI extension contains a series of Python \"magic commands\" that can be used *inside a notebook* to connect with various AI models. Jupyternaut is like a helpful assistant that answers questions, but the magic commands connect your notebook to a large variety of models for particular text analysis tasks such as:\n",
    "\n",
    "* Text classification\n",
    "* Question Answering\n",
    "* Feature Extraction\n",
    "* Text Generation\n",
    "* Sentiment Analysis\n",
    "* Translation\n",
    "\n",
    "Moreover, you can use these magic commands for multi-modal tasks, such as turning text into an image or audio into text. The Jupyter magic commands offer a way to easily *switch between* models with a minimum amount of code and complexity. In the next lesson, we will examine how to work with Hugging Face models in a more sophisticated fashion with specialized Python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95420599-c772-4604-b991-5cbb01e61c90",
   "metadata": {},
   "source": [
    "# Jupyternaut\n",
    "\n",
    "To use Jupyternaut, you need to connect the chatbot to a provider and then issue questions and/or commands. Let's look at each of these tasks separately.\n",
    "\n",
    "## Connecting Jupyternaut to a Provider\n",
    "\n",
    "Under the hood, the Jupyternaut chatbot interface must be powered by an AI infrastructure provider. The speed and quality of Jupyternaut depends on the provider and model chosen. Some options include:\n",
    "\n",
    "* [AI21 Labs](https://www.ai21.com/)\n",
    "* [Amazon Bedrock](https://aws.amazon.com/bedrock/)\n",
    "* [Amazon Sagemaker](https://aws.amazon.com/sagemaker/)\n",
    "* [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service)\n",
    "* [Cohere](https://cohere.com/)\n",
    "* [ERNIE Bot](http://research.baidu.com/Blog/index-view?id=183)\n",
    "* [GPT4All](https://www.nomic.ai/gpt4all) (early stage)\n",
    "* [Hugging Face](https://huggingface.co/)\n",
    "* [Open AI](https://openai.com/index/openai-api/)\n",
    "* [Together AI](https://www.together.ai/)\n",
    "\n",
    "Most providers offer access to high-speed compute (far beyond consumer-grade computers) and simplified infrastructure for model flexibility. For example, a provider like Open AI makes it trivial to switch between state-of-the-art models to inexpensive models.  These services usually come with a cost; you will need a credit card to use them. The cost for running Jupyternaut can be measured in fractions of a cent per response. \n",
    "\n",
    "When using a provider, you will need an API Key. The API Key is like a password, and it should not be shared publicly. When you run Python code that connects to the provider's servers, the API Key tells them you are authorized to make the request and the account that should be billed for the compute. If your API Key is compromised, another person could make requests and your account would be billed. For this reason, we recommend to set limits on your account and not let any provider \"auto-recharge\" your credit balance. $10 is plenty to get started with a model provider.\n",
    "\n",
    "Jupyternaut is also beginning to support some free options, such as GPT4All and Hugging Face. If you happen to have a high-end computer with multiple GPUs already, running some medium-sized models may be possible using GPT4All or Hugging Face. A better performance option is to outsource the heavy lifting to industrial, server-grade hardware through an API. The cost to use an API is tiny compared to building a high-end, research computer.\n",
    "\n",
    "Here, we demonstrate how to work with three providers: GPT4All, Hugging Face, and Open AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a53343-b26c-4cb0-a9ee-1395fdfac335",
   "metadata": {},
   "source": [
    "### GPT4All (Local Installation)\n",
    "GPT4All is a model provider focused on privacy, local installations, and \"open-source\" language models. Support for GPT4All with Jupyternaut and Jupyter AI is still under development. Expect it to be slow and a little buggy. The benefit is that GPT4All is free and runs models on local hardware. It may be a suitable solution for working with smaller models on a powerful consumer-grade computer, but the results will be slower and poorer quality compared to large language models hosted on commercial-grade infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf26a7-52aa-4c1e-a1d4-b0c6f016fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the gpt4all library\n",
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe272db1-4aae-4cae-bb3f-e84e787292fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a small model called orca_mini_3b\n",
    "# Read the model card at https://huggingface.co/pankajmathur/orca_mini_3b\n",
    "!curl -LO --output-dir ~/.cache/gpt4all \"https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf\" --create-dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3c4878-65f9-4085-a4b7-445df712aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused GPT4All models to free up Disk space\n",
    "!rm -rf ~/.cache/gpt4all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421ceb9-3bbf-4d29-958f-5651d51a359b",
   "metadata": {},
   "source": [
    "### Hugging Face (API)\n",
    "\n",
    "Hugging Face is the best place to find a large variety of AI models with hundreds of thousands to choose from. They offer a rate-limited [Inference API](https://huggingface.co/docs/api-inference/en/index) that is free for people interested in experimenting. Creating a [Pro Account](https://huggingface.co/blog/inference-pro) increases rate limits and access to high-end models. For production applications, developers can use a dedicated service called [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index). \n",
    "\n",
    "Given the number of models on Hugging Face, it is a good idea to investigate the model card. (Here's an example for [Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B).) They can supply information such as:\n",
    "* Specifications of the model\n",
    "* Source of training data\n",
    "* Benchmark scores for the model\n",
    "* Licensing and rights for the model\n",
    "* Whether approval is required to use the model\n",
    "* Responsibility and safety details\n",
    "* Sample code for using the model\n",
    "* Demonstrations of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b043a-5f13-4cac-a379-3b1d40a3a876",
   "metadata": {},
   "source": [
    "#### Get a Hugging Face API Key\n",
    "1. [Create a Hugging Face Account](https://huggingface.co/)\n",
    "   \n",
    "![Steps for creating an access token](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/hf-api-key.png?)\n",
    "\n",
    "2. Hugging Face calls their keys \"Access Tokens\". You can create one by selecting your profile in the upper-righthand corner, then \"Settings\", then \"Access Tokens\", and finally \"+ Create new token\".\n",
    "\n",
    "![Permissions to select for your token](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/hf-token-permissions.png?)\n",
    "\n",
    "Give the token a descriptive name and make sure that under \"Inference\" you have checked the box \"Make calls to the serverless Inference API\" if you're using a \"Fine-grained\" token type. The \"Read\" and \"Write\" token types have permission to make calls by default.\n",
    "\n",
    "4. Optionally, you may create a \"pro account\" for increased rate limits and access to high-end models. To create the account, select your profile in the upper-righthand corner, then \"Settings\", and finally \"Billing\".\n",
    "\n",
    "#### Configure Jupyternaut\n",
    "1. Select the Jupyternaut chat icon and enter the settings.\n",
    "2. Under \"Completion model\", choose \"Hugging Face Hub :: *\".\n",
    "3. Under \"Local Model ID\", you can copy and paste text directly from the model page. A good choice is [Mistral-7b-Instruct-v0.3](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3). You will need to request access to it. Here is what to paste into \"Local Model ID\":\n",
    "\n",
    "`mistralai/Mistral-7b-Instruct-v0.3`\n",
    "\n",
    "\n",
    "![Model page that shows where to copy the model name and that the gated model access has been granted](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/hf-model-example.png?)\n",
    "\n",
    "If the model requires access granted, make sure you have been granted access on Hugging Face. (Not all models are supported! Make sure you're using a text generation model, and that it is supported by the Inference API.)\n",
    "\n",
    "4. Enter your Hugging Face Access Token and select \"Save changes\"\n",
    "5. Click on the back arrow and you're ready to chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18851305-9a7d-4fd5-803e-c78730869d6d",
   "metadata": {},
   "source": [
    "### Open AI (API)\n",
    "Open AI is one of the best model providers. While you could use Jupyternaut with a high-end, state-of-the-art model, Open AI offers a variety of smaller, inexpensive models that give decent results with Jupyternaut including GPT-3.5 Turbo and Davinci-002. See more information on [Open AI models](https://platform.openai.com/docs/models). On the privacy front, Open AI does not use data sent through the API for training.\n",
    "\n",
    "#### Get an Open AI API Key\n",
    "1. [Create an Open AI account](https://platform.openai.com/docs/overview)\n",
    "\n",
    "![To add credits, select the gear icon, then \"billing\", then \"Add to credit balance\".](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/openai-add-credits.png)\n",
    "\n",
    "2. Add a credit to your account. We recommend keeping \"auto-recharge\" off, so charges cannot be run up if your API key becomes compromised. $5 is plenty to get started with Jupyternaut.\n",
    "\n",
    "![To create a key, select \"Dashboard\", then \"API Keys\", then \"+ Create a new secret key\"](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/openai-create-key.png)\n",
    "\n",
    "3. Create a new secret key. Treat the key like a password. When it is created, copy it somewhere safe immediately. Once the modal closes, the key cannot be shown again. If you lose the key, the only option is to delete the old key and create a new one. A project key or user key will work for Jupyternaut.\n",
    "\n",
    "#### Configure Jupyternaut\n",
    "1. Select the Jupyternaut chat icon and enter the settings.\n",
    "2. Choose a language model from OpenAI. Some good choices are `OpenAI::gpt-3.5-turbo` or the instruction-tuned variant `OpenAI::gpt-3.5-turbo-instruct`. See more information on [Open AI models](https://platform.openai.com/docs/models).\n",
    "3. Optionally, you can also choose an embedding model if you would like Jupyternaut to learn about your files, so you can ask questions about them. At this point, it is better at answering questions about text files than working with Python or notebook files. We recommend `text-embedding-3-small`. Learn more about [Open AI embedding models](https://openai.com/index/new-embedding-models-and-api-updates/).\n",
    "4. Enter your Open AI API Key and select \"Save changes\"\n",
    "5. Click on the back arrow and you're ready to chat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac72ab-12a9-4f10-b151-3833b9d836fd",
   "metadata": {},
   "source": [
    "## Working with Jupyternaut\n",
    "\n",
    "We can chat with Jupyternaut just like any other chatbot, but it has special features designed to help with coding.\n",
    "\n",
    "### General coding knowledge\n",
    "\n",
    "We can use Jupyternaut to ask questions to the underlying language model. If the model has been trained on Python data, it can be a useful for answering basic questions. For example, we could ask Jupyternaut, \"What is the difference between a Python list and tuple?\"\n",
    "\n",
    "![Asking Jupyternaut a question and receiving an answer](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/jupyternaut-answer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ccacf1-aa40-42ea-ad67-49df2471479a",
   "metadata": {},
   "source": [
    "### Jupyternaut commands\n",
    "We can also use particular commands to help with coding tasks. The commands and their functions are displayed if we enter the `/help` command into Jupyternaut. They also appear as options when we type a `/` into Jupyternaut.\n",
    "```\n",
    "/ask — Ask a question about your learned data\n",
    "/clear — Clear the chat window\n",
    "/generate — Generate a Jupyter notebook from a text prompt\n",
    "/learn — Teach Jupyternaut about files on your system\n",
    "/export — Export chat history to a Markdown file\n",
    "/fix — Fix an error cell selected in your notebook\n",
    "/help — Display this help message\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ce725-a94e-4b85-bec5-9a36aee45947",
   "metadata": {},
   "source": [
    "#### `/fix` Command\n",
    "Highlight the code with an error and then use the `/fix` command in the Jupyternaut chat. It will suggest how to fix the code. \n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**There is an error in the following code cell. Can you prompt Jupyternaut to offer a solution?**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e4f9c0-e36b-42ce-9595-09c5263360d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb02a6-ba38-40b1-8069-fb2093275c5d",
   "metadata": {},
   "source": [
    "#### `/export` Command\n",
    "If you want to save your conversation with Jupyternaut for reference later, you can export it to a markdown file with `/export`.\n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try exporting your conversation and then viewing the resulting file.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcafe2e-b0aa-43d8-9859-612e973d52a2",
   "metadata": {},
   "source": [
    "#### `/clear` Command\n",
    "The `/clear` command clears out the the conversation in the chat window.\n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try clearing your conversation and then exporting it. What do you notice?**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327cee5-95c8-4213-8003-ecc6dafe8ba1",
   "metadata": {},
   "source": [
    "#### `/learn` and `/ask` Commands\n",
    "These commands work together, allowing Jupyternaut to first learn about the content of local files and then answer questions about them. Note, if you have a general question for Jupyternaut, you can just ask it. The `/ask` command is only for asking about local files it has first learned about.\n",
    "\n",
    "At this point, the `/learn` and `/ask` commands work best with natural language text files, such as documentation. The Jupyter team are working on improving it's ability to work with code files. Let's try learning the documentation page for Jupyter AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231edb51-5a32-4dcc-b072-dd2d85298678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the documentation page for Jupyter AI\n",
    "import urllib.request\n",
    "\n",
    "document_url = 'https://jupyter-ai.readthedocs.io/en/latest/_sources/users/index.md.txt'\n",
    "urllib.request.urlretrieve(document_url, 'jupyter-ai-documentation.txt')\n",
    "    \n",
    "## Success message\n",
    "print('Document downloaded. Preview follows:')\n",
    "with open('jupyter-ai-documentation.txt') as f:\n",
    "    contents = f.read()\n",
    "    print(contents[:100], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a742245-425a-482d-b5ce-7d90d6277695",
   "metadata": {},
   "source": [
    "___\n",
    "Use the `/learn` command on the file:\n",
    "\n",
    "`constellate-notebooks/Applying-large-language-models/jupyter-ai-documentation.txt`\n",
    "\n",
    "Then try asking questions whose answer are in the documentation, like:\n",
    "* `/ask How do I install Jupyter AI?`\n",
    "* `/ask What providers are supported by Jupyter AI`\n",
    "* `/ask What is the difference between %ai and %%ai?`\n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try using the `/ask` command to ask a question about the Jupyter AI documentation file.**\n",
    "___\n",
    "\n",
    "Remember, every time you ask about learned documents, the query must begin with `/learn`. Otherwise, the model will only have access to the information it was trained on. \n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**To improve results, try playing with the Jupyternaut completion model and embedding model. For example, with Open AI, you might select the `gpt-4` model and `text-embedding-3-large`.**\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5413aa-ec69-4220-a570-ea336236dec4",
   "metadata": {},
   "source": [
    "Use `/learn -d` to delete information in the learned data. \n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try deleting the learned data and then using the `/ask` command.**\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a7c35-1aea-4b3f-a90a-0588e85b2af0",
   "metadata": {},
   "source": [
    "#### Generate a new notebook\n",
    "Use the `/generate` command to have Jupyternaut construct a new notebook based on your prompt. For example:\n",
    "\n",
    "* `/generate Create a notebook that downloads the wikipedia entry on \"sharks\" and then tokenizes it using NLTK.`\n",
    "* `/generate Create a wordcloud from the text in the file jupyter-ai-documentation.txt.`\n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try generating a notebook to accomplish a basic programming task. Can you improve the results by improving your prompt and/or using a different model?**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b7a90-9f32-4bde-bc5a-1b3be5bd15ef",
   "metadata": {},
   "source": [
    "# Jupyter AI Python Magic Commands\n",
    "\n",
    "A Python magic command is special command for Jupyter Notebooks. They come in two varieties:\n",
    "\n",
    "1. Line magics- Start with a single `%` and apply to a single line of code\n",
    "2. Cell magics- Start with a double `%%` and apply to an entire code cell\n",
    "\n",
    "The Jupyter AI extension enables the magics `%ai` and `%%ai`. Generally, we use `%ai` to configure the models and options. And we use `%%ai` to invoke the models and actually use them. Before we can use either, however, we have to load the magics using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9f4f3-5b70-46ed-a1c7-0e7ec9fdf3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Jupyter AI Magics\n",
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed297e54-a514-4b8a-9ea5-eba3e5d39f79",
   "metadata": {},
   "source": [
    "## Configuring the models\n",
    "Now that the magic commands are loaded, we can start using them. First, we should check which providers are configured using `%ai list`. A configured provider will display a green checkmark under the \"Set?\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b021550-13bb-4dff-8596-d34a6b07ae95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List the configured models\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644b1ec-2d20-46c5-b025-fc649a955c18",
   "metadata": {},
   "source": [
    "None of our providers are configured! This may be confusing since it seems like we configured providers to work with Jupyternaut. However, the magics are completely separate from the Jupyternaut chatbot interface. We must configure the providers here separately. (If you looked closely, you may have even noticed that the potential providers for the magics *are different* from Jupyternaut.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874c9c2-2955-4b3d-be89-eae0ea40d4cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T03:55:38.296688Z",
     "iopub.status.busy": "2024-07-26T03:55:38.296280Z",
     "iopub.status.idle": "2024-07-26T03:55:38.303567Z",
     "shell.execute_reply": "2024-07-26T03:55:38.303141Z",
     "shell.execute_reply.started": "2024-07-26T03:55:38.296670Z"
    }
   },
   "source": [
    "### Configuring providers for AI magics\n",
    "\n",
    "There are three ways to configure the providers. In order of complexity, they are:\n",
    "\n",
    "1. Set an environment variable with the relevant API key (e.g. Open AI, Hugging Face Hub) \n",
    "2. Authenticate to the provider using `boto3` (e.g. AWS Bedrock, Sagemaker) \n",
    "3. Download, install, and run the model locally (e.g. GPT4All, Ollama)\n",
    "\n",
    "Here, we give two examples using the first option. The second requires experience with AWS user roles and the third option requires facility with hardware configuration, such as GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25cc8d-bf25-4672-8174-05f26c12f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API Key \n",
    "%env OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d4ba3-ff86-4dfa-bd2e-0e5da4cfa7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face Hub API Key\n",
    "%env HUGGINGFACEHUB_API_TOKEN="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac866fd-2c49-45c1-bd59-c89138bdbfa1",
   "metadata": {},
   "source": [
    "## The `%%ai` Magic Command\n",
    "\n",
    "When your provider is configured, you can use the `%%ai` magic to connect to a variety of models. The syntax is:\n",
    "\n",
    "`%%ai [provider]:[model]`\n",
    "\n",
    "We can grab the provider and model designation directly from the output of `%ai list`. (Note, when we use `%%ai` it must be on the first line of the code cell. We cannot put a comment above it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c24d60-ad59-42a7-9cd7-b3cc22a98267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai openai-chat:gpt-3.5-turbo\n",
    "What did Shakespeare write?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8061e-71ae-4be1-bcd2-c369908342de",
   "metadata": {},
   "source": [
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Try loading some other models from Open AI. Did the generated text change? For example, Open AI's state-of-the-art model is `openai-chat:gpt-4o`. **\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b2cbe-c5f5-4a81-8fbe-c92c4106c632",
   "metadata": {},
   "source": [
    "Now let's see an example from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc3f03-5689-4733-8eef-ffc4f2e5bc99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%ai huggingface_hub:mistralai/Mistral-7B-Instruct-v0.3\n",
    "What are the best Python libraries for working with tabular data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e6cbe8-5ef3-4feb-9070-a7cd0950aa1d",
   "metadata": {},
   "source": [
    "We can also pass a code cell in as the context to a model. Referencing the number in the hard brackets next to the code cell after it was run.\n",
    "\n",
    "___\n",
    "<h4 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h4>\n",
    "\n",
    "**Find the number next to the code cell above that retrieved the Jupyter AI documentation and insert it below in place of `INSERT_NUMBER`.**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52984bcd-0d0f-479d-8315-c22798cf5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai huggingface_hub:mistralai/Mistral-7B-Instruct-v0.3\n",
    "Please explain the code below:\n",
    "--\n",
    "{In[INSERT_NUMBER]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbfbfd-61e3-40da-a7b8-05f2654d22d3",
   "metadata": {},
   "source": [
    "### Aliases\n",
    "You may have noticed that `%ai` listed some aliases and targets:\n",
    "\n",
    "```\n",
    "Aliases and custom commands:\n",
    "\n",
    "Name\tTarget\n",
    "gpt2\thuggingface_hub:gpt2\n",
    "gpt3\topenai:davinci-002\n",
    "chatgpt\topenai-chat:gpt-3.5-turbo\n",
    "gpt4\topenai-chat:gpt-4\n",
    "ernie-bot\tqianfan:ERNIE-Bot\n",
    "ernie-bot-4\tqianfan:ERNIE-Bot-4\n",
    "titan\tbedrock:amazon.titan-tg1-large\n",
    "```\n",
    "These are basically shortcuts. We can use\n",
    "`%%ai chatgt` for `%%ai openai-chat:gpt-3.5-turbo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1694009-1fd1-4280-bb36-e2e0b20b0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt\n",
    "Is Python or R a better programming language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761f651-b04a-420c-a1d4-0357e430e7ca",
   "metadata": {},
   "source": [
    "___\n",
    "We can create our own alias like so:\n",
    "\n",
    "`%ai register [alias name] [provider]:[model]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47136255-aa8c-42ab-a687-3db09ad3025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ai register good_buddy huggingface_hub:mistralai/Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac91c9-d950-4d9c-b6d4-57ebebc803d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai good_buddy \n",
    "What is the best smell in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c080490-f380-4762-88c6-cbce60c221e9",
   "metadata": {},
   "source": [
    "## Additional outputs\n",
    "We can also ask for other outputs using either the `-f` or `--format`` flag followed by:\n",
    "\n",
    "* `code`\n",
    "* `image` (for Hugging Face Hub’s text-to-image models only)\n",
    "* `markdown`\n",
    "* `math`\n",
    "* `html`\n",
    "* `json`\n",
    "* `text`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbcaaa-46c4-40e7-b57b-7e463ad573b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai huggingface_hub:stabilityai/stable-diffusion-xl-base-1.0 --format image\n",
    "A medieval manuscript containing constellations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ee92b-3e8f-4d15-9316-59eedbd4003d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The Jupyter AI extension offers a simple path to getting started with a variety of models. The Jupyternaut chatbot is a helpful assistant for writing code and the Jupyter Magic commands can quickly connect your notebooks to the model of your choice. There's a lot more to discover, so it is definitely worth checking out the [Jupyter AI Documentation](https://jupyter-ai.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "The Jupyter AI extension was released in August of 2023, so it is still very young and under active development. Jupyter AI excels at creating a simple interface for basic access to models. However, model development is moving at a breakneck pace, and it is common to run into issues with models, especially from Hugging Face since there are literally hundreds of thousands to choose from. Some models use non-standard methods which do not work with Jupyter AI, sometimes models change their API causing Jupyter AI Magics to fail with a model that worked perfectly fine a week ago, and models can require more sophisticated interaction than the interface can offer.\n",
    "\n",
    "In our next class, we will look at how to access models in a more sophisticated and flexible fashion. You will learn how to find and deploy models with Python code, opening up an enormous world of possibilities for research. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee3ec42-a926-4d19-8022-70e034cea1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
