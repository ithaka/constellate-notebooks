{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9357d7-7721-4d80-8533-37241c93753e",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e77886-9fb4-4afc-948b-0191b4941272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Language Models 2: ü§ó Hugging Face\n",
    "\n",
    "**Description:** \n",
    "\n",
    "Learners will use ü§ó Hugging Face Transformers library to explore aspects of language models including:\n",
    "\n",
    "* Text Generation\n",
    "* Sentiment Analysis\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Summarization\n",
    "\n",
    "We will primarily use two libraries: transformers and huggingface_hub's inference client.\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "* Pandas Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "* Pandas Intermediate\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** \n",
    "* [ü§ó Transformers](https://huggingface.co/docs/transformers/index)- provides APIs and tools to easily download and train pretrained models\n",
    "* [Pytorch](https://pytorch.org/)- a popular machine learning framework\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba7e9a-3d10-401a-bc0d-3789b03ba581",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "Hugging Face is an online community focused on AI models, datasets, apps, and infrastructure. It is the best place for finding and working with a large variety of models.\n",
    "\n",
    "This notebook works primarily with the ü§ó Hugging Face libraries [huggingface_hub](https://huggingface.co/docs/hub/index) and [transformers](https://huggingface.co/docs/transformers/index). The `huggingface_hub` library connects your code with a variety of resources including models, datasets, and the Inference API. The `transformers` library, not to be confused with the AI architecture called [transformer](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)), can download models, create pipelines for inference, train models, and fine-tune models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59bade-47b1-41e1-892f-881dc7b90089",
   "metadata": {},
   "source": [
    "## The Hugging Face Website\n",
    "\n",
    "### Models\n",
    "\n",
    "* Filtering model search- Searching for the right model? You can filter models in a large variety of ways.\n",
    "* Model cards- Get background information on how a model was constructed and how to use.\n",
    "* Gated models- Request permission to use a particular model.\n",
    "* Model files- See the file names and sizes that make up the model.\n",
    "* Community- See, submit, and fix current issues in the model.\n",
    "* Deploy- See example code for deploying the model in various environments.\n",
    "* Use this model- See how to use the model with the `transformers` library\n",
    "\n",
    "### Apps and Demos\n",
    "* Spaces- Build small demonstration apps using software development kits (SDKs) like Streamlit, Gradio, and Docker\n",
    "* Inference API- Try a model right in the browser\n",
    "* Huggingchat- Try a chat model right in the browser\n",
    "\n",
    "### Documentation\n",
    "* Docs- Learn more about how to use the Hugging Face libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fedfb4-fa6f-429e-a570-ddaaf5759e93",
   "metadata": {},
   "source": [
    "## Choosing a Model\n",
    "In the previous class, we used Jupyter AI and Jupyter Magics to interact with foundation models designed for general purpose tasks. Today, we will consider the wider variety of models available and how to get started with them. Here are some questions to consider.\n",
    "\n",
    "### What is the task you are trying to accomplish?\n",
    "There are models designed for a wide variety of research tasks. The models on Hugging Face work in a variety of modalities:\n",
    "\n",
    "* **Natural Language Processing**: text classification, named entity recognition, question answering, summarization, translation, multiple choice, text generation, vector databases\n",
    "* **Computer Vision**: image classification, object detection, segmentation\n",
    "* **Audio**: automatic speech recognition, audio classification\n",
    "* **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, visual question answering, text to audio/image/video\n",
    "\n",
    "### Can you use an existing model?\n",
    "\n",
    "Models are trained on a particular dataset. If your data is similar to the material in the model's training data, you may be able to use the model \"off-the-shelf\" without any changes. The process of using a model is called \"inference\", and it uses significantly less resources than training a model from scratch or \"fine-tuning\" an existing model. \n",
    "\n",
    "### What kind of compute is necessary for the task?\n",
    "\n",
    "If a model has already been trained to do your task, then you can simply run inference on the existing model. If your data is slightly different than the model's training data, you may be able to \"fine-tune\" it to work better with your data. If you need a model to do a new kind of task, you may need to train it from scratch.\n",
    "\n",
    "Very large models and/or complex tasks require more resources. While simple models can be run locally on a modest laptop, a very big model or complex task may require a high-end computer or server-grade hardware. It's a fool's errand to use too small of a model or low-grade hardware for a difficult task, but it is also a waste to use too much model or hardware for a simple task. The best way to discover if you have a good fit is to try running a variety of tasks before throwing all your data at the model. Sometimes, it may make sense to use different models for different parts of the data or to fine-tune an existing model. Training a model from scratch can be reasonable if you have lots of high-quality, labeled data and access to significant compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c1bf6-8a19-4a20-ac39-c342ccedc47c",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2db38d-5dce-4860-8036-286a6f7f5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install ü§ó Transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235be26-6532-4494-aad9-ffc265707478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Sentencepiece, a  a subword tokenizer and detokenizer for natural language processing\n",
    "# that uses byte-pair-encoding (BPE)\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c3796-254e-4846-9b7f-d5750a07514a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sacremoses, a Python port of the Moses tokenizer\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2d815-cceb-4cd8-a885-260d441ba401",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a19ebe-f810-4262-9d51-075a2091c312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient\n",
    "from huggingface_hub import AsyncInferenceClient\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe539d-0067-45c6-b4ea-73d0bd4d3205",
   "metadata": {},
   "source": [
    "# Local models\n",
    "For researchers, one of the primary benefits of working with models locally is more transparency about the code and model weights. Most LLMs provided by recognizable tech companies share very little detail on their models including:\n",
    "\n",
    "* How they are constructed\n",
    "* What was in their training data\n",
    "* The model weights\n",
    "* The prompts and guardrails\n",
    "\n",
    "This kind of opacity might be helpful for a commercial product, but it is also antithetical to the values of good research, including the [FAIR Guiding Principles](https://www.nature.com/articles/sdata201618) which assert data should be:\n",
    "* Findable\n",
    "* Accessible\n",
    "* Interoperable\n",
    "* Reusable\n",
    "\n",
    "There are some additional advantages for using models locally:\n",
    "\n",
    "* No API means no internet connection required\n",
    "* Have the model weights\n",
    "* Can fine-tune the weights\n",
    "\n",
    "The downsides are:\n",
    "\n",
    "* Largest models are often too big for local inference\n",
    "* Fine-tuning may require expensive hardware\n",
    "* Models are usually not state-of-the-art"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353d4d-022a-4bdc-a990-af4c22f160ae",
   "metadata": {},
   "source": [
    "## Managing Local Models\n",
    "\n",
    "Language models and datasets come in many sizes. The models and datasets for this notebook were tested on the given tasks, but for other models/tasks it is a good idea to check the file size and requirements. If you load or use a language model that is too big, you may fill all of the available space (30 GB) and/or memory (8 GB) in your lab. If the memory is full, try restarting the kernel (or restarting the lab). If the disk space is full, before deleting your own files, delete the .cache directory to clear out downloaded datasets and models from your space. You can do this by running the following code cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fe65c-f54e-423c-b35d-bbcc60a5cac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the .cache folder\n",
    "!rm -r /home/jovyan/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc6e99-1b0e-4001-9a0f-d7459cf5cc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check current disk space usage\n",
    "!df -h /home/jovyan/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390255f9-950a-4bab-b0a5-cfe90597ff22",
   "metadata": {},
   "source": [
    "If you are familiar with the command line, you can use a terminal session to remove individual models and datasets. ü§ó Hugging Face stores them in the following places.\n",
    "\n",
    "**Datasets**\n",
    "```~/.cache/huggingface/datasets```\n",
    "\n",
    "**Models**\n",
    "```~/.cache/hub```\n",
    "\n",
    "See the `manage-disk-space.ipynb` notebook in the root directory for more information, strategies, and code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0f886-4bec-4118-82b7-2948a0ffd124",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b95c0-c735-424d-8139-3398257e6c8c",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers Pipelines\n",
    "\n",
    "The Transformers library  contains a variety of pipelines for common model tasks. The `pipeline()` can help you accomplish a variety of tasks, including:\n",
    "\n",
    "* `feature extraction`- Extracting features from a model for transfer learning\n",
    "* `text-classification`- Classifying texts into groups, including sentiment analysis\n",
    "* `sentiment-analysis`- Classifying texts into positive or negative sentiment\n",
    "* `token-classification`- Group tokens, including Named Entity Recognition (NER)\n",
    "* `ner`- Finding named entities in a text\n",
    "* `question-answering`- Answering questions, often based on context\n",
    "* `fill-mask`- Predicting masked tokens\n",
    "* `summarization`- Create a shorted version summary of a longer document\n",
    "* `translation_xx_to_yy`- Translation from language xx to language yy\n",
    "* `text2text-generation`- Generate text from a text instructions\n",
    "* `text-generation`- Predictive text generation based on a starting prompt\n",
    "* `zero-shot-classification`- Attempt to classify texts without additional training with labeled data\n",
    "* `conversational`- Conversational responses\n",
    "\n",
    "There are also pipelines for working with other common model tasks, such as automatic speech recognition, audio classification, text-to-speech, text-to-image, image-segmentation,  etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a46a3-1201-4e4a-bfcd-9da613be8b4a",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "By default, the ü§ó Transformers library text generation pipeline uses the Generative Pre-trained Transformer 2 (GPT-2) model by [OpenAI](https://openai.com/). This is a precursor of GPT-3.5, the model used for ChatGPT. This model was released in 2019 and you can find more information by reading its [model card](https://huggingface.co/gpt2/tree/main) on the ü§ó Transformers website. We include here several parameters:\n",
    "\n",
    "* `set_seed` Remove the randomness of the text generation by supplying the same seed value each time.\n",
    "* `prompt` The prompt that the text generator uses to build the sequence.\n",
    "* `truncation=True` The length of the response should be limited.\n",
    "* `max_length` If truncation is set, this is the length of the text returned. More text requires more time and the limit is defined by the model.\n",
    "* `num_return_sequences` Allows more than one sequence to be returned for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bb0d0-bd68-4ca9-9fdd-a8b809ae1ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1796a5e2-3288-49cb-bc97-868a3ef27506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def create_text(prompt):\n",
    "    #set_seed(42)\n",
    "    output = generator(prompt, truncation=True, max_length=100, num_return_sequences=3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721d894-f6c4-4d80-9fac-c16ca092ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a prompt into the task function\n",
    "prompt = \"In the year 2067, the American people were forced to admit\"\n",
    "create_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb8929-b2f3-4d3d-976c-c2513135a84b",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "By default, the ü§ó Transformers library text-classification pipeline uses the [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model. This model is based on a distilled, uncased version of [BERT](https://huggingface.co/bert-base-uncased) that has been fine-tuned on the [Stanford Sentiment Treebank 2](https://huggingface.co/datasets/sst2) (SST-2) dataset. The SST-2 dataset is a binary classification dataset for training models to learn the sentiment of words, phrases, and sentences. It contains 215,154 unique manually labeled texts of varying lengths. The model card describes SST-2:\n",
    "\n",
    ">\n",
    "The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fdf6b-b3d1-44e6-96fa-5d84f5f71574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Prompts\n",
    "\n",
    "### Deadpool and Wolverine Reviews\n",
    "# A positive movie review of Deadpool and Wolverine from Metacritic\n",
    "prompt1 = \"\"\"\n",
    "Deadpool & Wolverine is pure joy and such an enjoyable, entertaining ride. From beginning to end, the film takes you on a journey filled with laughter, absurdity, and a good bit of heart. This is a love letter to the Fox Marvel Universe, Hugh Jackman and Wolverine, and ultimate fan service to the long-time comic book and film adaptation fans of these characters. While it can be a bit thin on the overall depth of story, it more than makes up for this with big action set pieces, great oneliners and jokes throughout, along with giving us some of the most exhilarating cameos the MCU has seen yet. If this is the kick-off of the mutant saga and more, fans are in for a great ride. Deadpool & Wolverine deserves to be seen over and over again in theaters.\n",
    "\"\"\"\n",
    "\n",
    "# A negative movie review of Deadpool and Wolverine from Metacritic\n",
    "prompt2 = \"\"\"\n",
    "Feige‚Äôs mainstream instincts are easy to detect here. The prior Deadpool films were scuzzy and cobbled together, even as the budget grew; the cameos from other Marvel characters felt half-hearted and perfunctory, inclusions for Deadpool to roll his eyes at, not for fans to cheer over. Deadpool & Wolverine, on the other hand, has that bland MCU sheen that makes all of its movies look expensive but nonthreatening, happily accepting of mediocrity rather than attempting something artsy or daring.\n",
    "\"\"\"\n",
    "\n",
    "### Tomorrow and Tomorrow and Tomorrow Reviews\n",
    "# A positive book review of Tomorrow and Tomorrow and Tomorrow from Goodreads\n",
    "prompt3 = \"\"\"\n",
    "Tomorrow, and Tomorrow, and Tomorrow, is a multilayered novel about friendship, love, and video games.\n",
    "Sam and Sadie met when they are kids and quickly bonded over their love of video games. They develop a friendship that spans almost 30 years. The novel follows the highs and lows of their friendship, including falling in love, falling out, a love triangle, successes, and failures. Throughout it all, the one constant in their lives is video games.\n",
    "The narrative alternates primarily between Sadie and Sam's POVs. Sam and Sadie are both loveable, arrogant, infuriating, and flawed. The dynamics of their friendship are complicated by love, jealousy, and misunderstanding. I got a little sick of the friends to frenemies cycle between Sadie and Sam (more of Sadie‚Äôs anger towards Sam, but I understood her point of view). I loved them, but I also wanted to shake some sense in them.\n",
    "Sam‚Äôs mother, Anna; Marx, Sam‚Äôs college roommate; Dov, Sadie's professor; are some additional characters who make an impact. My favorite characters were Sam‚Äôs grandparents, Dong Hyun and Bong Cha.\n",
    "The novel blends reality and game worlds, and parts of the narrative take place in a virtual open world.\n",
    "All characters are well-developed and multidimensional. Even the avatars are multidimensional.\n",
    "I am not a huge fan of video games, but this book made me nostalgic for the video games of my childhood. I got all of the Oregon Trail and Mario references, but there were times that I was a little lost, but I didn‚Äôt mind because I learned so much about gaming. The reader doesn‚Äôt need to know much about video games to enjoy this book (but it might help!). There are also a lot of 80s, 90s, and early 2000s pop culture references mixed in. I loved reading the details behind creating a game and the gaming industry as I was introduced to a whole new world.\n",
    "This is a well-written, complex, thought-provoking, and original novel. I was invested in the characters, and some moments hit me on an emotional level. I got teary-eyed towards the end. I won't forget these characters; this is a book that is going to stay with me for a long time.\n",
    "\"\"\"\n",
    "\n",
    "# A negative book review of Tomorrow and Tomorrow and Tomorrow from Goodreads\n",
    "prompt4 = \"\"\"\n",
    "This book is so utterly pretentiousness and trying so hard to be woke that I should have given up on it instead of seeing it to the end. I would have if the beginning hadn‚Äôt been so beautifully done. There‚Äôs a line in the book about a video game sequel being awful because it was farmed out to Indian programmers who had no interest in the game and that‚Äôs how this book feels after the incredible start.\n",
    "\n",
    "The story began with Sadie and Sam central to the story. Sam was the obviously the more sympathetic of the two and the one you as a reader care about. Sadie was often annoying and then fell apart in a ridiculous way. I hoped her awful college self with the horrible college boyfriend would evolve and grow up but she never does. Even worse for the story is the tangents that from that point became the story. We suddenly get a new character who is rightly called boring later on. He is a NPC. He‚Äôs just too good and uninteresting to take up so much space. We get his backstory we don‚Äôt need. In a similar way later on we get two new characters that happen to be gay that bring nothing to the story other than a celebration of their sexuality which apparently is worth their inclusion. Much like tangents about their game that take up unnecessary page time and continue to dilute any attempt at storytelling. There‚Äôs plenty of politics, even to a ridiculously degree like actual comical bad guys intent on violence against those in favor of gay relationships and marriage. Ironically for a book full of wokeness with characters never being straight, celebrating gender fluidity, the book managed to ridicule cultural appropriation. The book is very focused on the race of the characters but never explores them in more than a superficial way.\n",
    "One of the author‚Äôs worst faults was her pretentious word choices. Instead of writing in way that flowed she chose to constantly check her thesaurus for jarring words like jejune and verdigris every couple of pages. Ironically much like the criticism of a game her character created this book is pretentious and full of itself. The worst part is that could have been amazing if it had stayed as focused as it was in the beginning. This is not a story worth the journey so do not push play. I received a complimentary copy of this book.\n",
    "\"\"\"\n",
    "\n",
    "### Dave the Diver\n",
    "# A positive video game review of Dave the Diver from Metacritic\n",
    "prompt5 = \"\"\"\n",
    "This is demanding work, but the game‚Äôs distinct but complementary loops of playful labour are highly compelling. The satisfaction of completing a challenging dive without needing to be rescued, then watching the rave reviews on ‚ÄúCooksta‚Äù pour in, is profound. Stylish, witty and exquisitely designed, Dave the Diver uses several hooks to achieve its goal, while establishing the relationship between the food we eat and the world from which its harvested with useful urgency.\n",
    "\"\"\"\n",
    "\n",
    "# A negative video game review of Dave the Diver from Metacritic\n",
    "prompt6 = \"\"\"\n",
    "I thought it would be fun, but the exposition/mechanic dump in the opening hours really soured my experience. I spent as much time watching cutscenes and having dialogue spewed at me as playing the game, it felt like. The alternating management/action sections sounded interesting in the reviews I watched, but playing them was much slower and more monotonous than I would have anticipated. It's definitely the kind of game for which I'm thankful that Steam offers a 2 hour refund window.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdd0f1-4e47-4d5a-b879-aafabec29d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553196f-fda6-47db-85d5-506b1d02f629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def classify_sentiment(prompt):\n",
    "        output = classifier(prompt)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd03fe9-cd19-415a-ad1b-484f538592e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a prompt into the task function\n",
    "classify_sentiment(prompt6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccb6a9-45ff-41bc-a1ab-e8225fc7e041",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "The `aggregation_strategy` parameter defines the strategy used to group entity tokens together, like \"New York\". Remember, the tokenization may also be at the subword level, so you could see \"Microsoft\" broken up into \"Micro\" and \"soft\". Additional aggregration strategies such as \"first\", \"average\", and \"max\" are discussed in the ü§ó Transformers [documentation](https://huggingface.co/transformers/v4.7.0/_modules/transformers/pipelines/token_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a95b8c-a1db-41fd-8f58-d10497755967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Named Entity Recognition\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f69d8-3e28-4847-a7e0-2e718d4f626c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def extract_entities(prompt):\n",
    "    output = ner_tagger(prompt)\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131a2b4-dd5d-4209-8abf-85b6992d23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the prompt into the task function\n",
    "extract_entities(prompt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df519-beab-48fd-816b-da277f4da864",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "The Question Answering pipeline has two required parameters: \n",
    "\n",
    "* `question` The question being asked\n",
    "* `context` The source material that should be used to answer this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a459-f47e-4771-8867-a696bc30b8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Question Answering\n",
    "reader = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbd425-4e64-4105-9561-9f7186e23bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def answer_question(question, context):\n",
    "    output = reader(question=question, context=context)\n",
    "    return pd.DataFrame([output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6115d6-c4d7-4072-a094-81591443485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the prompt into the task function\n",
    "question = \"What are the best parts?\"\n",
    "answer_question(question, prompt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e817-dae4-44a2-a9e8-20603c2361bc",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "The `clean_up_tokenization_spaces` parameter removes extraneous spaces created through the detokenization process. If tokenization breaks up a string into separate tokens, then detokenization joins together a series of tokens into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167b118-95ff-4d95-b2ab-7f7d755a9b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarization\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a41fa5-84c8-4bb2-8fa0-a1aea43eb43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def summarize(text):\n",
    "    outputs = summarizer(text, max_length=75, clean_up_tokenization_spaces=True)\n",
    "    return outputs[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363750c-93eb-4bae-99f3-f13c1a055e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass the prompt into the task function\n",
    "summarize(prompt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b9a4-dcb3-491b-abaa-fd8100235a73",
   "metadata": {},
   "source": [
    "## Translation\n",
    "\n",
    "The translation pipeline may have length limitations based on the model selected. If your text is long, you may need to break it up into smaller chunks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d407a96-02b5-4c40-8746-6b9e99e64740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation\n",
    "ger_translator = pipeline('translation_en_to_de', model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "\n",
    "\n",
    "# outputs = translator(prompt4, clean_up_tokenization_spaces=True, min_length=100)\n",
    "# print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132afe9-e1c1-4386-9bc9-c3882cc32e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a task function\n",
    "def translate_to_german(text):\n",
    "    output = ger_translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa0183-370b-44c8-9153-08e8a09ea82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the prompt into the task function\n",
    "translate_to_german(prompt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbf2de-c87a-4766-a301-b49072e79789",
   "metadata": {},
   "source": [
    "# Clear the memory and cache\n",
    "It is a good practice to clear your cache and any variables in memory after using a notebook that loads a significant amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc679988-484a-419a-b4fa-043e4618c859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all variables from memory\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ef337-d6de-442c-b760-e62e86fa01d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the .cache folder\n",
    "!rm -r /home/jovyan/.cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29d2453-39bc-459d-acda-9741c301f971",
   "metadata": {},
   "source": [
    "# Using the huggingface_hub `InferenceClient`\n",
    "For certain projects, it may make sense to offload the compute load to a provider. For example, even if the model weights and training data are \"open\", you may want to run your inference on a server if the model or the research dataset is very large. Hugging Face offers API inference in two different services:\n",
    "\n",
    "* [Serverless Inference API](https://huggingface.co/docs/api-inference/index)- Small cost to access through PRO plan (~$9/month), but provides shared resources suitable for research and prototyping\n",
    "* [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index)- Expensive enterprise, dedicated, auto-scaling designed for production applications\n",
    "\n",
    "The cost of Inference Endpoints depends on the provider (AWS, MS Azure, Google Cloud) and the hardware (from \\\\$0.03/hr to more than \\\\$100/hr). Assuming you have a PRO account already, you can login and supply an API key to get started.\n",
    "\n",
    "## Log in to InferenceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50cbda3-0e82-4482-a50d-0ec5aa45f040",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Log in using an access token\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26a168-7c1c-4a09-824f-ea0522eb111a",
   "metadata": {},
   "source": [
    "## Generate images with `text_to_image`\n",
    "Just like `transformers` has pipelines, the `inference_client` has methods for [a variety of NLP tasks](https://huggingface.co/docs/huggingface_hub/v0.24.2/en/package_reference/inference_client#huggingface_hub.InferenceClient) such as:\n",
    "\n",
    "* `document_question_answering`\n",
    "* `feature_extraction`\n",
    "* `image_to_text`\n",
    "* `text_classification`\n",
    "* `translation`\n",
    "\n",
    "and many more for audio and images. Let's try a `text_to_image` example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3ebdf-5e4f-447f-baa6-d8daa01c7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default text to image inference model\n",
    "client = InferenceClient()\n",
    "client.text_to_image(\"An astronaut riding a horse on mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa1458-cf36-4dec-b216-864be5c7d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a text to image model\n",
    "client = InferenceClient(model=\"prompthero/openjourney-v4\")\n",
    "\n",
    "# Save the image locally\n",
    "image = client.text_to_image(\"an astronaut riding a horse on mars\")\n",
    "image.save(\"astronaut.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe040fc-e8ac-4370-a839-9957e3650213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pass the model to text_to_image instead of InferenceClient\n",
    "client = InferenceClient()\n",
    "client.text_to_image(\"an astronaut riding a horse on mars\", model=\"prompthero/openjourney-v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571b802-0772-44ac-905a-0ace1328c7f9",
   "metadata": {},
   "source": [
    "We can choose another model and add additional inference steps to increase the quality of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781313f-6dd0-4903-b01a-ee535926742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A higher quality example from Stable Diffusion\n",
    "client = InferenceClient(model=\"stabilityai/stable-diffusion-xl-base-1.0\")\n",
    "client.text_to_image(\"A photograph of a shark jumping out of a swimming pool\", num_inference_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1602672-947c-40fc-871f-d2799e7e886e",
   "metadata": {},
   "source": [
    "For additional parameters, see the `text_to_image` [documentation](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/inference_client#huggingface_hub.InferenceClient.text_to_image).\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a008c24-9ce9-4406-8f48-f3359cb0574f",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with `text_classification`\n",
    "If we want to do sentiment analysis, we could change to a `text_classification` task and specify a model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3103d-1e1b-4948-ae4b-ff8d89b5c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a text_classification model for sentiment analysis\n",
    "client = InferenceClient(model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "client.text_classification(\"This guy is a jerk. I would like to beat him up. He ruined my party and ate all my ice cream.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfc2f9-d24e-42fd-ab60-5bb9f571e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a text_classification model for emotion analysis\n",
    "client = InferenceClient(model=\"SamLowe/roberta-base-go_emotions\")\n",
    "client.text_classification(\"This guy is a jerk. I would like to beat him up. He ruined my party and ate all my ice cream.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e39ea-5bb8-4ab1-9a5b-69b35890105d",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with `token_classification`\n",
    "Similarly, we can quickly do Named Entity Recognition using `token_classification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff826118-0222-41dd-a483-7c0f373f4498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a token_classification model for NER\n",
    "client = InferenceClient(model='dslim/bert-base-NER')\n",
    "client.token_classification(prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e3bb7-0f04-4130-aea6-2dcede9e15f7",
   "metadata": {},
   "source": [
    "## Translate with `translation`\n",
    "Here are some examples using translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f76da-731e-47b1-90d3-955baf7305a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a translation model: English to French\n",
    "client = InferenceClient(model=\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "client.translation(\"I would like to eat ice cream in the Louvre, but they told me to jump in the Seine.\", src_lang=\"en_XX\", tgt_lang=\"fr_XX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab857cc3-15c6-4ac2-b2ac-0b7195582a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a translation model: English to Hindi\n",
    "client = InferenceClient(model=\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "client.translation(\"I would like to eat ice cream in the National Museum, but they told me to jump in the Yamuna.\", src_lang=\"en_XX\", tgt_lang=\"hi_IN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b2c87-c62f-4a9f-8b6c-dcd512d36716",
   "metadata": {},
   "source": [
    "## Classify without labeled data with `zero_shot_classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d56a58-7c39-439b-a6ab-224960fffc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a Zero Shot Classification model\n",
    "client = InferenceClient()\n",
    "client.zero_shot_classification(\n",
    "    text=prompt4,\n",
    "    labels=[\"positive\", \"negative\", \"pessimistic\", \"optimistic\", \"indifferent\"],\n",
    "    multi_label=True,\n",
    "    hypothesis_template=\"This text is {} towards Sadie\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404b332-2e03-4561-beb1-970589476f39",
   "metadata": {},
   "source": [
    "## Convert text to speech with `text_to_speech`\n",
    "Here we use the asynchronous version of the inference client, which allows Python to wait until the results come back from the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbd46e-5349-42df-ad84-8e3cdda408fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a text to audio model\n",
    "# Run this model async\n",
    "client = AsyncInferenceClient(model='suno/bark')\n",
    "audio = await client.text_to_speech(\"I would like to eat ice cream in the Louvre, but they told me to jump in the Seine!\")\n",
    "Path(\"ice_cream.flac\").write_bytes(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e25286-7273-41a5-ad96-e59c7b5e5165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
