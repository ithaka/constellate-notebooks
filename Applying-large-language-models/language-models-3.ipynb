{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3837c7c-ff78-4782-aa8e-1807157ab977",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"../All-sample-files/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622275e5-4266-42e2-be28-71448c15da7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Models 3: ðŸ¤— Hugging Face with RAG and Open AI Web Search\n",
    "\n",
    "**Description:** \n",
    "\n",
    "Learners will use ðŸ¤— Hugging Face Inference Client combined with Llama Index to create a basic Retrieval Augmented Generation (RAG) system. They will also use the Open AI Responses API in order to answer prompts with web data.\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** \n",
    "* [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index)- provides APIs and tools to easily download and train pretrained models\n",
    "* [Llama_index](https://docs.llamaindex.ai/en/stable/)- helps index our documents\n",
    "* [Open AI Responses API](https://platform.openai.com/docs/api-reference/responses)- allows us to combine LLM completions with web search\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16978b20-d1b2-4136-9924-7c4c14adaff8",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval Augmented Generation\n",
    "\n",
    "Large Language Models (LLMs) are trained on an enormous variety of content, including books, wikipedia, and social media. They are often able to answer basic questions in a wide-ranging variety of contexts. Researchers, on the other hand, tend to specialize in their research areaâ€”going deep rather than wide. Researchers also tend to be interested in the latest articles and research in their field, while language models â€œknowledgeâ€ is frozen in time once trained. (There are some ways to update the knowledge in a language model, but they can be impractical.) Finally, researchers are concerned with citation and reference. In brief, *LLMs often lack knowledge that is specialized, current, and citable*: the type of knowledge researchers want most.\n",
    "\n",
    "Retrieval Augmented Generation (RAG), formalized in â€œRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasksâ€ ([Lews, et. al 2020](https://arxiv.org/abs/2005.11401)), has emerged as a solution for these problems. While other methods have focused on re-training an existing model, RAG introduces a new step into the process: retrieval.\n",
    "\n",
    "In the retrieval step, the userâ€™s query is matched with a vector database of reference documents (called a â€œknowledge baseâ€) in order to find document chunks that are likely to contain the answer. Once the document chunks have been retrieved, they can be submitted as context with the userâ€™s query to the LLM. \n",
    "\n",
    "![The steps of RAG described below in visual form.](../All-sample-files/rag-process.png)\n",
    "\n",
    "While RAG systems can be quite sophisticated, the basic steps remain the same:\n",
    "\n",
    "1. User submits a query\n",
    "2. Relevant document chunks are returned from the vector database\n",
    "3. A prompt containing the chunks is submitted to the LLM with the userâ€™s query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1007f-16ea-4dff-949f-a3f0d6c0dbb5",
   "metadata": {},
   "source": [
    "## What about transfer learning, fine-tuning, parameter-efficient fine-tuning, etc.?\n",
    "\n",
    "RAG can be combined with fine-tuning and other techniques to improve outputs. An ideal solution may combine RAG with other techniques. At the current time, some research suggests RAG has a more profound effect than fine-tuning. In other words, RAG may improve LLM benchmark scores by a greater degree than other techniques, but the highest scores usually come from a combination of techniques.\n",
    "\n",
    "![Table showing RAG has a greater affect than finetuning](../All-sample-files/ragvsfinetune.png)\n",
    "\n",
    "From \"Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\" ([Soudani, et. al. 2024](https://arxiv.org/abs/2403.01432))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0322c3c-c386-4856-b161-d905dfd12856",
   "metadata": {},
   "source": [
    "# Building a basic RAG system\n",
    "\n",
    "By combining our knowledge of working with Hugging Face with a vector database, we can create a basic RAG system. First, we will need to create a knowledge base, including the following steps:\n",
    "\n",
    "1. Curate a body of relevant documents\n",
    "2. Extract the texts and chunk them\n",
    "3. Embed the chunks\n",
    "4. Create vector database from embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7fddd-96e8-4acd-a9bb-1aed807d4a8e",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6c8f-870d-462e-bd2d-7b75a4f33f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers and llama-index libraries\n",
    "!pip install transformers\n",
    "!pip install llama-index\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1ca27-79c4-4297-9d14-72e599c08efd",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faa807-50e8-4cad-9511-7d9896993845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006c8ce-14cd-4351-bfcc-1418f5d040b7",
   "metadata": {},
   "source": [
    "## Gather documents for knowledge base\n",
    "\n",
    "Let's create a knowledge base that relies on recent, specialized knowledge. Our LLM for this system will be Meta's [Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct), released July 23, 2024. We can find the \"freshness\" of the model from the model card:\n",
    "\n",
    ">Data Freshness: The pretraining data has a cutoff of December 2023.\n",
    "\n",
    "Let's include some specialized, recent knowledge that the model could not have been trained on. Even in early 2025, Llama 3.1 405B is an enormous model which ranks in the top 30 best in the world on key benchmarks. In our knowledge base, we'll include technical reports for some more recent models:\n",
    "\n",
    "1. [Google's Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)- Released March 10, 2025\n",
    "2. [Deepseek V3](https://github.com/deepseek-ai/DeepSeek-V3)- Released December 26, 2024\n",
    "3. [Microsoft's Phi-4](https://huggingface.co/microsoft/phi-4)- Released December 12, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb964-c4d0-436c-a68b-00a8ae55ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the documents and put them in a directory called \"documents\"\n",
    "dir_path = Path.cwd() / \"documents\"\n",
    "dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "files ={\n",
    "    \"phi-4-technical-report.pdf\" : 'https://arxiv.org/pdf/2503.01743',\n",
    "    \"gemma-3-technical-report.pdf\" : 'https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf',\n",
    "    \"deepseek-v3-technical-report.pdf\" : 'https://arxiv.org/pdf/2412.19437'\n",
    "}\n",
    "    \n",
    "for file_name, url in files.items():\n",
    "    urllib.request.urlretrieve(url, f'./documents/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d278f21-7ac2-4f61-8c67-1a68d1efddbe",
   "metadata": {},
   "source": [
    "## Simple Directory Reader\n",
    "\n",
    "The simple directory reader will gather up all the files in a directory and turn them into a list of document objects. It can parse many kinds of files including pdfs, text files, markdown files, etc. It will intelligently select the right reader for the right file, and it will process them differently. For example, a text file is treated as a single document whereas a markdown file is broken down by headings.\n",
    "\n",
    "### Using other files for the knowledge base\n",
    "\n",
    "You don't need to use our example documents. Our code is creating the knowledge base from the documents in a directory called documents. You can create this directory and put any kind of files you would like in there for your own knowledge base. We recommend using text or markdown files for this example, but you can consult [the documentation](https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader/) if you're curious about how `SimpleDirectoryReader()` interacts with other kinds of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d28a58-10bb-46cb-a376-df5b499d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect documents into a list\n",
    "docs = SimpleDirectoryReader(\"documents\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e78c94-22ca-4507-8952-6174207a292b",
   "metadata": {},
   "source": [
    "All of our files are saved as text files (.txt), so they will be individual document objects. They are also valid markdown files (.md), however, so we could have saved them with the `.md` extension. By default, `SimpleDirectoryReader()` will chunk markdown files into smaller files based on their structure. We will do some basic chunking ourselves, but this kind of intelligent chunking may give better results. For our example, we get 3 documents. How many documents would the markdown versions generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef3f72-8319-424a-ba29-0631ac19363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62301e-168a-42f2-99eb-ea499f37d590",
   "metadata": {},
   "source": [
    "## Embedding Settings\n",
    "\n",
    "We will use [LlamaIndex](https://docs.llamaindex.ai/en/stable/) to create our vector database. We will select an embedding model from Hugging Face. We are free to choose any embedding model, since the embedding model *does not* have to match our LLM. We have chosen a popular embedding model from Hugging Face, but feel free to update or change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7a599b-bc00-48f5-a21a-be662de6a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face using our API token\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f6a88-b791-4026-a152-43ec71484fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Embedding Model from Hugging Face\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd527c-e4f7-4401-b5cf-dcc502b0cf45",
   "metadata": {},
   "source": [
    "We will create some additional settings:\n",
    "1. Not specifying an LLM model\n",
    "2. Choosing a chunk size\n",
    "3. Choosing a chunk overlap\n",
    "\n",
    "## Chunking documents\n",
    "The chunk size is important for the performance of the vector database and the LLM. There are many ways to chunk, including fixed sizes, random chunk sizes, sliding windows, and context-aware chunking. The right size and method will take into consideration the documents, the LLM's context window, and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e642e66-013e-47a3-95e4-9019e9226374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Hugging Face embedding model\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b2a22-2ef1-4333-8326-8d743f0eded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database from docs object\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b07608-6902-40c9-bf4d-f1b9cec87475",
   "metadata": {},
   "source": [
    "## Search function\n",
    "Now we can set up our retrieval system. The most significant thing we can adjust here is how many documents to retrieve under the variable `top_k`. We can also change the similarity cutoff using `similarity_cutoff`. Essentially, this changes how similar a document needs to be in order to be included. Both of these are worth experimenting with. Keep in mind that there is a limit on the context that can be supplied for the model. More is not always better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d849a-5bdc-42f9-8597-dad18706482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents to retrieve\n",
    "top_k = 3\n",
    "\n",
    "# Retriever configuration\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k=top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c98bec-a176-4a2b-9b2d-d9293d913f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff96744-3707-474f-95cd-9ef6cffb1751",
   "metadata": {},
   "source": [
    "## Query\n",
    "Here we craft our query and receive a response from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed761d79-5175-4cbd-94f2-28eaff875b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = 'How does DeepSeek-V2-Base compare to DeepSeek-V3-Base?'\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca84bd0-f095-42bb-b7ec-0c8c9c1bdaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the responses\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b840-89cd-4667-8bf6-f73cbd34f104",
   "metadata": {},
   "source": [
    "## Create LLM prompt (without RAG context)\n",
    "\n",
    "First, let's create a prompt to pass to the LLM. We'll automatically insert the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601febf0-beb2-4176-9fe9-fcfaa1700d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some instructions for the model\n",
    "\n",
    "ragless_prompt = f\"\"\"\n",
    "[INST] ResearchBuddy, a virtual consultant for research tasks communicates in clear, accessible language helping answer technical questions on documentation.\n",
    "\n",
    "Please respond to the following comment.\n",
    "{query}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40c7db-a2b4-4faf-9664-b3754e8744e6",
   "metadata": {},
   "source": [
    "## Add RAG context to our LLM Prompt\n",
    "Now let's create a context string from our responses received above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432584-8ae2-4d25-aed9-422955582c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context string from response\n",
    "context = \"Context:\\n\"\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe17744-9711-468d-bb0e-810ba2828768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RAG prompt with the context\n",
    "ragful_prompt = ragless_prompt + context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a7f76-c3db-46bf-90e9-b5e770346195",
   "metadata": {},
   "source": [
    "Now we have two versions of LLM prompt:\n",
    "\n",
    "* `ragless_prompt`- Has basic instructions with our query\n",
    "* `ragful_prompt`- Has basic instructions, our query, and the context from our vector database\n",
    "\n",
    "We are ready to pass these prompts to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c2b36-f68c-4e4a-90c8-bb241e93cdd5",
   "metadata": {},
   "source": [
    "## Pass the prompts to the LLM\n",
    "We can choose to pass these prompts to the LLM of our choice. In this case, we are using Llama 3.1-405B-Instruct, but we could easily choose another model using the `InferenceClient()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83417a-0f48-4429-a676-37c837846b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model\n",
    "client = InferenceClient(model = \"meta-llama/Llama-3.1-405B-Instruct\", provider = 'nebius')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e2c2f-d78b-4992-8c7e-cc98a7a741bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model without RAG context\n",
    "completion = client.chat.completions.create(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": ragless_prompt}],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e1c9b7-c5ab-42b0-b199-520084b15ce0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ask the model with RAG context\n",
    "completion = client.chat.completions.create(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": ragful_prompt}],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77a387-2276-491e-888e-a06de981809f",
   "metadata": {},
   "source": [
    "# LLM Agents Basics with Open AI\n",
    "\n",
    "The concept of AI agents gained significant traction in 2024. This seachange compelled Open AI to release a new API [Responses](https://platform.openai.com/docs/api-reference/responses), which will be the eventual successor to the [Chat Completions](https://platform.openai.com/docs/api-reference/chat) API. With this shift, Open AI is planning to create more built-in tools for web search, file search, and computer use. The differences in the two APIs are detailed in the [Open AI documentation](https://platform.openai.com/docs/guides/responses-vs-chat-completions).\n",
    "\n",
    "Let's install the Open AI library and enter an API token in order to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f040ae5-eda2-4f82-91b7-a634e37bb6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Open AI \n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002dac8d-d364-4cb7-adcf-73ce11de29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import getpass and Open AI\n",
    "from getpass import getpass\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b44c1-2da8-44de-8fc4-91032426abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Hugging Face API Key in a variable\n",
    "# The getpass function obscures your token if you share the notebook with others\n",
    "\n",
    "openai_api_key = getpass('Enter your Open API Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683af62-8546-4051-9fa4-d72c6b6ef5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load key from environment variable\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787aac8-16c0-4f50-98b1-ceb785ad9e2b",
   "metadata": {},
   "source": [
    "# Chat Completions API\n",
    "The older chat completions API is very similar to the Hugging Face Inference API. We use create a variable to store the output of `client.chat.completions.create` while specifying:\n",
    "\n",
    "* `model`- the Open AI model we would like to use\n",
    "* `messages`- a list containing a dictionary with the `role` and `content` of the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8144e11-008e-4a5d-8e29-0b4f1bfadc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Completions API\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"What is a language model?\"\n",
    "      }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6ea2c-0e39-4f8b-a0e3-fd9e557598e2",
   "metadata": {},
   "source": [
    "The completions object is quite complex to navigate to get to the response text. See the [full documentation for details](https://platform.openai.com/docs/api-reference/chat/list-object). At each level of the object, there are many attributes that we need to drill down into in order to get the response at: `completion.choices[0].message.content`. Compare the original API syntax with the streamlined version in responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ae02-db97-47de-a43e-c6436a334a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Responses API\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\", \n",
    "    input=\"What is a language model?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6e30f-e225-430e-82f4-97f8616f5423",
   "metadata": {},
   "source": [
    "We can now simply include a string for the `input` parameter. With the older **Chat Completions** API, the messages list must be manually updated each time, adding what was said to a growing list that forms a conversation. The **Responses** API automatically assigns an ID number for each response and stores it. (If we do not want it stored, we can set the parameter `store = False`.\n",
    "\n",
    "So we see that the API has been significantly streamlined, with the most important data being easier to surface. This is also clear if we take a look at the difference between our `completion` with the older Chat Completions API and our `response` with the newer Responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e2fc4-e0f3-4676-aa19-234b99f9f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat completions object\n",
    "print(completion)\n",
    "\n",
    "# Responses object\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3ffb5-5b46-475f-8ccc-d491ca32e058",
   "metadata": {},
   "source": [
    "The syntax for streaming is a little different. The stream comes through in a series of events and we check to see if there is a delta (or difference) in each event. If there is, we print it out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ff563-232e-470d-88bf-585f8a89cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Responses API with streaming response\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\", \n",
    "    input=\"What is a language model?\",\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for event in response: # Unfortunately, this still has a bug in Jupyter where it overwrites characters\n",
    "    if hasattr(event, 'delta'):\n",
    "        print(event.delta, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85318e9-c144-4b75-975b-1aa7f1d91265",
   "metadata": {},
   "source": [
    "# The Building Blocks of Agentic Workflows\n",
    "\n",
    "The Responses API is designed with agent tasks and workflows in mind, prioritizing web search alongside new tools such as file search, computer use, and a forthcoming code interpreter. The concept is that agent models do not respond simply by providing text, images, or content, but that they work actively to solve problems. This could require multiple steps or multiple models working together on a task. \n",
    "\n",
    "A recent post by Anthropic [Building Effective Agents](https://www.anthropic.com/engineering/building-effective-agents) describes the recent shift into agentic systems with two categories:\n",
    "\n",
    "* **Workflows** are systems where LLMs and tools are orchestrated through predefined code paths.\n",
    "* **Agents**, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\n",
    "\n",
    "Workflows are almost always the better choice since they define a clear process for using particular tools at particular times with clear end states. Some workflows include:\n",
    "\n",
    "* **Building Block**- An LLM is augmented with additional tools and retrieval such as Retrieval Augmented Generation\n",
    "* **Prompt Chaining**- A series of LLM prompts are chained together with a \"gate\" to check progress after each step is completed\n",
    "* **Routing**- A specialized router or triage agent routes the task through to a particular agent designed for the task\n",
    "* **Parellelization**- Multiple LLMs run similar tasks at the same time. The results are then either combined into a single result or voted on for the best solution\n",
    "* **Orchestrator-workers**- A central LLM breaks down a task and then assigns it to worker LLMs before synthesizing the results\n",
    "* **Evaluator-optimizer**- One LLM generates solutions and works in tandem with an evaluator/optimizer, improving a solution through successive iterations\n",
    "\n",
    "These workflows can be connected in sophisticated pathways to accomplish difficult tasks.\n",
    "\n",
    "In distinction, **Agents** \"...are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\" Since the agent is making strategic decisions on the process, tools, and solution-state, the results of agents can be highly variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec857f-5ea9-436b-b319-bbf89beedeb0",
   "metadata": {},
   "source": [
    "# File Search, Computer Use, and Web Search\n",
    "\n",
    "The Responses API offers basic capabilities with File Search, Computer Use, and Web Search. There are additional tools planned, including a code interpreter.\n",
    "\n",
    "## File Search\n",
    "\n",
    "The File Search is an implementation of RAG, where you create a vector store which can be stored on Open AI infrastructure. The data storage does have a small cost: \"You first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations\" ([File Search Documentation](https://platform.openai.com/docs/assistants/tools/file-search). There are arguably better players in this space, and it is not too hard to build a RAG system (as we demonstrated above). However, the nice thing about this service is it allows Open AI to be a one-stop shop for your models and your data.\n",
    "\n",
    "## Computer Use\n",
    "\n",
    "It is still very early days for computer use models, but Open AI's Computer-Using Agent (CUA) model shows promise. Basically, the CUA model operates in a loop. An image is sent to the model and then it can take an action such as: clicking, typing, or scrolling. After each action, a new image is sent to the model, so it can decide what choice to make next. The CUA model can use a browser automation framework such as [Playwright](https://playwright.dev/) or [Selenium](https://www.selenium.dev/) to control a web browser on your machine. The CUA Model can also control a local virtual machine through [Docker](https://www.docker.com/).\n",
    "\n",
    "## Web Search\n",
    "\n",
    "The web search tool enables the LLM to search the web in order to find relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe243d61-f2b3-43bc-9315-e9b2bb77c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search Tool\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\"type\": \"web_search_preview\"}],\n",
    "    input=\"What happened in AI news today?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae45e8-6861-4f8f-8599-e06cdc5ca0f6",
   "metadata": {},
   "source": [
    "Notice we did not ask the model to search the web. We can make the web search tool available, and the model *decides* whether it should check the web based on the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83174b-5e42-458a-92f1-2249b84f829d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The model does not use web search to answer\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\"\n",
    "        \n",
    "        }],\n",
    "    input=\"What is a unicorn?\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5e335-0d3b-43ca-b4cd-cddd119953f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T03:21:16.155137Z",
     "iopub.status.busy": "2025-03-21T03:21:16.154762Z",
     "iopub.status.idle": "2025-03-21T03:21:16.158449Z",
     "shell.execute_reply": "2025-03-21T03:21:16.157863Z",
     "shell.execute_reply.started": "2025-03-21T03:21:16.155120Z"
    }
   },
   "source": [
    "We can force the model to search the web using the `tool_choice` parameter: `tool_choice= {type: \"web_search_preview\"}`. We can also set `search_context_size` to `high` in order to retrieve more context from the web. Using more context will impact the quality, cost, and speed of the response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f0877-c829-4253-bb28-d8d749737a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is forced to use the web browser search to help answer\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"search_context_size\": \"low\"\n",
    "        \n",
    "    }],\n",
    "    tool_choice={\"type\": \"web_search_preview\"},\n",
    "    input=\"What is a unicorn?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb1b7a-52f6-4b21-92af-8b112f449cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T03:28:16.829672Z",
     "iopub.status.busy": "2025-03-21T03:28:16.829396Z",
     "iopub.status.idle": "2025-03-21T03:28:16.833417Z",
     "shell.execute_reply": "2025-03-21T03:28:16.832881Z",
     "shell.execute_reply.started": "2025-03-21T03:28:16.829651Z"
    }
   },
   "source": [
    "We can also set a user location in our tools in order to influence the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc697b4-6507-4ee5-a306-ee042b4c1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is given a location in order to answer the question\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"web_search_preview\",\n",
    "        \"user_location\": {\n",
    "            \"type\": \"approximate\",\n",
    "            \"country\": \"US\",\n",
    "            \"city\": \"Detroit\",\n",
    "            \"region\": \"Detroit\",\n",
    "        }\n",
    "    }],\n",
    "    tool_choice={\"type\": \"web_search_preview\"},\n",
    "    input=\"What is a burger place open tomorrow for lunch?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
