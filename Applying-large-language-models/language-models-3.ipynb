{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3837c7c-ff78-4782-aa8e-1807157ab977",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622275e5-4266-42e2-be28-71448c15da7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Models 3: ðŸ¤— Hugging Face with RAG\n",
    "\n",
    "**Description:** \n",
    "\n",
    "Learners will use ðŸ¤— Hugging Face Inference Client combined with Llama Index to create a basic Retrieval Augmented Generation (RAG) system.\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "* Pandas Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "* Pandas Intermediate\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** \n",
    "* [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index)- provides APIs and tools to easily download and train pretrained models\n",
    "* [Pytorch](https://pytorch.org/)- a popular machine learning framework\n",
    "* [Llama_index](https://docs.llamaindex.ai/en/stable/)- helps index our documents\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16978b20-d1b2-4136-9924-7c4c14adaff8",
   "metadata": {},
   "source": [
    "# Introduction to Retrieval Augmented Generation\n",
    "\n",
    "Large Language Models (LLMs) are trained on an enormous variety of content, including books, wikipedia, and social media. They are often able to answer basic questions in a wide-ranging variety of contexts. Researchers, on the other hand, tend to specialize in their research areaâ€”going deep rather than wide. Researchers also tend to be interested in the latest articles and research in their field, while language models â€œknowledgeâ€ is frozen in time once trained. (There are some ways to update the knowledge in a language model, but they can be impractical.) Finally, researchers are concerned with citation and reference. In brief, *LLMs often lack knowledge that is specialized, current, and citable*: the type of knowledge researchers want most.\n",
    "\n",
    "Retrieval Augmented Generation (RAG), formalized in â€œRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasksâ€ ([Lews, et. al 2020](https://arxiv.org/abs/2005.11401)), has emerged as a solution for these problems. While other methods have focused on adapting an existing model, RAG introduces a new step into the process: retrieval.\n",
    "\n",
    "In the retrieval step, the userâ€™s query is matched with a vector database of reference documents (called a â€œknowledge baseâ€) in order to find document chunks that are likely to contain the answer. Once the document chunks have been retrieved, they can be submitted as context with the userâ€™s query to the LLM. \n",
    "\n",
    "![The steps of RAG described below in visual form.](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/rag-process.png)\n",
    "\n",
    "While RAG systems can be quite sophisticated, the basic steps remain the same:\n",
    "\n",
    "1. User submits a query\n",
    "2. Relevant document chunks are returned from the vector database\n",
    "3. A prompt containing the chunks is submitted to the LLM with the userâ€™s query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1007f-16ea-4dff-949f-a3f0d6c0dbb5",
   "metadata": {},
   "source": [
    "## What about transfer learning, fine-tuning, parameter-efficient fine-tuning, etc.?\n",
    "\n",
    "RAG can be combined with fine-tuning and other techniques to improve outputs. An ideal solution may combine RAG with other techniques. At the current time, some research suggests RAG has a more profound effect than fine-tuning. In other words, RAG may improve LLM benchmark scores by a greater degree than other techniques, but the highest scores usually come from a combination of techniques.\n",
    "\n",
    "![Table showing RAG has a greater affect than finetuning](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/ragvsfinetune.png)\n",
    "\n",
    "From \"Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\" ([Soudani, et. al. 2024](https://arxiv.org/abs/2403.01432))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0322c3c-c386-4856-b161-d905dfd12856",
   "metadata": {},
   "source": [
    "# Building a basic RAG system\n",
    "\n",
    "By combining our knowledge of working with Hugging Face with a vector database, we can create a basic RAG system. First, we will need to create a knowledge base, including the following steps:\n",
    "\n",
    "1. Curate a body of relevant documents\n",
    "2. Extract the texts and chunk them\n",
    "3. Embed the chunks\n",
    "4. Create vector database from embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7fddd-96e8-4acd-a9bb-1aed807d4a8e",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6c8f-870d-462e-bd2d-7b75a4f33f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers and llama-index libraries\n",
    "!pip install transformers\n",
    "!pip install llama-index\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f1ca27-79c4-4297-9d14-72e599c08efd",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51faa807-50e8-4cad-9511-7d9896993845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient\n",
    "import urllib.request\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006c8ce-14cd-4351-bfcc-1418f5d040b7",
   "metadata": {},
   "source": [
    "## Gather documents for knowledge base\n",
    "\n",
    "Let's create a knowledge base that relies on recent, specialized knowledge. Our LLM for this system will be [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), released July 23, 2024. We can find the \"freshness\" of the model from the model card:\n",
    "\n",
    ">Data Freshness: The pretraining data has a cutoff of December 2023.\n",
    "\n",
    "Let's include some information that the model would not have access to:\n",
    "\n",
    "1. **Jupyter AI documentation**- The Jupyter AI project launched in August of 2023. It is possible Meta Llama 3.1 was trained on a very early version of the documentation. When we ask the model about it, however, it hallucinates giving information that is wrong. So, either the information was not in the training data or it is too specific for it to be retained.\n",
    "2. **Llama 3.1 Model Card**- Most likely, the model was not trained on its own model card. It probably did not exist yet.\n",
    "3. **Mistral Large Instruct 2407 Model Card**- This model was released the day after Llama 3.1 (July 24). There is no way Llama 3.1 would know about this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb964-c4d0-436c-a68b-00a8ae55ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the documents and put them in a directory called \"documents\"\n",
    "dir_path = Path.cwd() / \"documents\"\n",
    "dir_path.mkdir(exist_ok=True)\n",
    "\n",
    "files ={\n",
    "    \"jupyter-ai-documentation.txt\" : 'https://jupyter-ai.readthedocs.io/en/latest/_sources/users/index.md.txt',\n",
    "    \"llama-3.1b-405.txt\" : 'https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md',\n",
    "    \"mistral-large-instruct-2407.txt\" : 'https://huggingface.co/mistralai/Mistral-Large-Instruct-2407/resolve/main/README.md'\n",
    "}\n",
    "    \n",
    "for file_name, url in files.items():\n",
    "    urllib.request.urlretrieve(url, f'./documents/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d278f21-7ac2-4f61-8c67-1a68d1efddbe",
   "metadata": {},
   "source": [
    "## Simple Directory Reader\n",
    "\n",
    "The simple directory reader will gather up all the files in a directory and turn them into a list of document objects. It can parse many kinds of files including pdfs, text files, markdown files, etc. It will intelligently select the right reader for the right file, and it will process them differently. For example, a text file is treated as a single document whereas a markdown file is broken down by headings.\n",
    "\n",
    "### Using other files for the knowledge base\n",
    "\n",
    "You don't need to use our example documents. Our code is creating the knowledge base from the documents in a directory called documents. You can create this directory and put any kind of files you would like in there for your own knowledge base. We recommend using text or markdown files for this example, but you can consult [the documentation](https://docs.llamaindex.ai/en/stable/examples/data_connectors/simple_directory_reader/) if you're curious about how `SimpleDirectoryReader()` interacts with other kinds of files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d28a58-10bb-46cb-a376-df5b499d0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect documents into a list\n",
    "docs = SimpleDirectoryReader(\"documents\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e78c94-22ca-4507-8952-6174207a292b",
   "metadata": {},
   "source": [
    "All of our files are saved as text files (.txt), so they will be individual document objects. They are also valid markdown files (.md), however, so we could have saved them with the `.md` extension. By default, `SimpleDirectoryReader()` will chunk markdown files into smaller files based on their structure. We will do some basic chunking ourselves, but this kind of intelligent chunking may give better results. For our example, we get 3 documents. How many documents would the markdown versions generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ef3f72-8319-424a-ba29-0631ac19363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62301e-168a-42f2-99eb-ea499f37d590",
   "metadata": {},
   "source": [
    "## Embedding Settings\n",
    "\n",
    "We will use [LlamaIndex](https://docs.llamaindex.ai/en/stable/) to create our vector database. We will select an embedding model from Hugging Face. We are free to choose any embedding model, since the embedding model *does not* have to match our LLM. We have chosen a popular embedding model from Hugging Face, but feel free to update or change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f6a88-b791-4026-a152-43ec71484fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the Embedding Model from Hugging Face\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd527c-e4f7-4401-b5cf-dcc502b0cf45",
   "metadata": {},
   "source": [
    "We will create some additional settings:\n",
    "1. Not specifying an LLM model\n",
    "2. Choosing a chunk size\n",
    "3. Choosing a chunk overlap\n",
    "\n",
    "## Chunking documents\n",
    "The chunk size is important for the performance of the vector database and the LLM. There are many ways to chunk, including fixed sizes, random chunk sizes, sliding windows, and context-aware chunking. The right size and method will take into consideration the documents, the LLM's context window, and other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e642e66-013e-47a3-95e4-9019e9226374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a Hugging Face embedding model\n",
    "Settings.llm = None\n",
    "Settings.chunk_size = 256\n",
    "Settings.chunk_overlap = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5b2a22-2ef1-4333-8326-8d743f0eded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database from docs object\n",
    "index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b07608-6902-40c9-bf4d-f1b9cec87475",
   "metadata": {},
   "source": [
    "## Search function\n",
    "Now we can set up our retrieval system. The most significant thing we can adjust here is how many documents to retrieve under the variable `top_k`. We can also change the similarity cutoff using `similarity_cutoff`. Essentially, this changes how similar a document needs to be in order to be included. Both of these are worth experimenting with. Keep in mind that there is a limit on the context that can be supplied for the model. More is not always better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d849a-5bdc-42f9-8597-dad18706482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents to retrieve\n",
    "top_k = 3\n",
    "\n",
    "# Retriever configuration\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k=top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c98bec-a176-4a2b-9b2d-d9293d913f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff96744-3707-474f-95cd-9ef6cffb1751",
   "metadata": {},
   "source": [
    "## Query\n",
    "Here we craft our query and receive a response from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed761d79-5175-4cbd-94f2-28eaff875b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "query = 'What providers does Jupyter AI support?'\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca84bd0-f095-42bb-b7ec-0c8c9c1bdaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the responses\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346b840-89cd-4667-8bf6-f73cbd34f104",
   "metadata": {},
   "source": [
    "## Create LLM prompt (without RAG context)\n",
    "\n",
    "First, let's create a prompt to pass to the LLM. We'll automatically insert the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601febf0-beb2-4176-9fe9-fcfaa1700d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some instructions for the model\n",
    "\n",
    "ragless_prompt = f\"\"\"\n",
    "[INST] ResearchBuddy, a virtual consultant for research tasks communicates in clear, accessible language helping answer technical questions on documentation.\n",
    "\n",
    "Please respond to the following comment.\n",
    "{query}\n",
    "\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40c7db-a2b4-4faf-9664-b3754e8744e6",
   "metadata": {},
   "source": [
    "## Add RAG context to our LLM Prompt\n",
    "Now let's create a context string from our responses received above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432584-8ae2-4d25-aed9-422955582c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context string from response\n",
    "context = \"Context:\\n\"\n",
    "for i in range(top_k):\n",
    "    context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe17744-9711-468d-bb0e-810ba2828768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RAG prompt with the context\n",
    "ragful_prompt = ragless_prompt + context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a7f76-c3db-46bf-90e9-b5e770346195",
   "metadata": {},
   "source": [
    "Now we have two versions of LLM prompt:\n",
    "\n",
    "* `ragless_prompt`- Has basic instructions with our query\n",
    "* `ragful_prompt`- Has basic instructions, our query, and the context from our vector database\n",
    "\n",
    "We are ready to pass these prompts to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c2b36-f68c-4e4a-90c8-bb241e93cdd5",
   "metadata": {},
   "source": [
    "## Pass the prompts to the LLM\n",
    "We can choose to pass these prompts to the LLM of our choice. In this case, we are using Llama 3.1-8B-Instruct, but we could easily choose another model using the `InferenceClient()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dd995-0223-4ae6-ba8c-7d788e451d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Log in using an access token\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83417a-0f48-4429-a676-37c837846b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model\n",
    "client = InferenceClient(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fd57c-91e8-4fd1-9ead-950ebe370580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model without context\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": ragless_prompt}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ea8a5-e6d2-427a-ab27-d93fdde93f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the model with RAG context\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": ragful_prompt}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
