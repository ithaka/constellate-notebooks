{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364003e8-8307-4352-b336-a167af07d489",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"All-sample-files/CC_BY.png\"><br />\n",
    "\n",
    "Created by Amy Kirchhoff under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306dca7c-c9ec-4c17-8f37-489b17b8079b",
   "metadata": {},
   "source": [
    "On June 15, 2025, JSTOR has launched the [new text analysis support service](https://www.jstor.org/ta-support/) to replace the old Data for Research service provided by [Constellate](https://labs.jstor.org/projects/text-mining/), which has sunset June 30, 2025. For Constellate users who prefer to work with the old Constellate dataset format, this notebook helps you convert the new JSTOR text analysis format to the Constellate format. In this way, the existing Constellate notebooks can continue to work on your data files. \n",
    "\n",
    "Please read the instructions in the code cells closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1bba9-7793-4aa6-818c-d881c92b8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "JSTOR Text Analysis to Constellate Format Converter\n",
    "\n",
    "This notebook converts the new JSTOR text analysis format back to the Constellate \n",
    "format so existing Constellate notebooks can continue to work.\n",
    "\n",
    "Input files:\n",
    "- JSTOR metadata JSONL file (compressed with gzip)\n",
    "- JSTOR full-text dataset JSONL file (compressed with gzip)\n",
    "\n",
    "Output:\n",
    "- Constellate-format JSONL file with metadata, ngrams, and full text combined\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "class JSTORToConstellateConverter:\n",
    "    \"\"\"\n",
    "    Converts JSTOR text analysis format to Constellate format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metadata_cache = {}\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def load_metadata(self, metadata_file_path):\n",
    "        \"\"\"\n",
    "        Load JSTOR metadata file and create a lookup dictionary\n",
    "        \n",
    "        Args:\n",
    "            metadata_file_path (str): Path to the JSTOR metadata JSONL.gz file\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting metadata load from {metadata_file_path}\")\n",
    "        logger.info(\"This may take several minutes for large files...\")\n",
    "        \n",
    "        # Determine if file is compressed\n",
    "        is_compressed = metadata_file_path.endswith('.gz')\n",
    "        open_func = gzip.open if is_compressed else open\n",
    "        mode = 'rt' if is_compressed else 'r'\n",
    "        \n",
    "        logger.info(f\"Opening {'compressed' if is_compressed else 'uncompressed'} metadata file...\")\n",
    "        \n",
    "        with open_func(metadata_file_path, mode, encoding='utf-8') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                # Update progress on same line every 10,000 records, log milestone every 100,000\n",
    "                if line_num % 100000 == 0:\n",
    "                    print(f\"\\rProcessing metadata record {line_num:,}...\", end=\"\", flush=True)\n",
    "                #if line_num % 100000 == 0:\n",
    "                 #print()  # Move to new line for milestone\n",
    "                  #  logger.info(f\"✓ Processed {line_num:,} metadata records so far...\")\n",
    "                    \n",
    "                try:\n",
    "                    metadata = json.loads(line.strip())\n",
    "                    item_id = metadata.get('item_id')\n",
    "                    if item_id:\n",
    "                        self.metadata_cache[item_id] = metadata\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print()  # Move to new line for warning\n",
    "                    logger.warning(f\"Failed to parse metadata line {line_num}: {e}\")\n",
    "                    \n",
    "        print()  # Move to new line when done\n",
    "        logger.info(f\"✓ Metadata loading complete! Loaded {len(self.metadata_cache):,} metadata records into cache\")\n",
    "    \n",
    "    def clean_text_for_ngrams(self, text):\n",
    "        \"\"\"\n",
    "        Clean and tokenize text for n-gram generation, following Constellate preprocessing\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text to clean\n",
    "            \n",
    "        Returns:\n",
    "            list: List of cleaned tokens\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize using NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Clean tokens similar to Constellate preprocessing\n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            # Keep only alphabetic tokens with length >= 4\n",
    "            if token.isalpha() and len(token) >= 4 and token not in self.stop_words:\n",
    "                cleaned_tokens.append(token)\n",
    "                \n",
    "        return cleaned_tokens\n",
    "    \n",
    "    def generate_ngrams(self, tokens, n):\n",
    "        \"\"\"\n",
    "        Generate n-grams from tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): List of tokens\n",
    "            n (int): N-gram size (1, 2, or 3)\n",
    "            \n",
    "        Returns:\n",
    "            Counter: Counter object with n-gram counts\n",
    "        \"\"\"\n",
    "        if len(tokens) < n:\n",
    "            return Counter()\n",
    "            \n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i + n])\n",
    "            ngrams.append(ngram)\n",
    "            \n",
    "        return Counter(ngrams)\n",
    "    \n",
    "    def convert_jstor_to_constellate_record(self, jstor_full_text_record):\n",
    "        \"\"\"\n",
    "        Convert a single JSTOR full-text record to Constellate format\n",
    "        \n",
    "        Args:\n",
    "            jstor_full_text_record (dict): JSTOR full-text record\n",
    "            \n",
    "        Returns:\n",
    "            dict: Constellate-format record or None if metadata not found\n",
    "        \"\"\"\n",
    "        item_id = jstor_full_text_record.get('item_id')\n",
    "        if not item_id:\n",
    "            logger.warning(\"Full-text record missing item_id\")\n",
    "            return None\n",
    "            \n",
    "        # Get metadata for this item\n",
    "        metadata = self.metadata_cache.get(item_id)\n",
    "        if not metadata:\n",
    "            logger.warning(f\"No metadata found for item_id: {item_id}\")\n",
    "            return None\n",
    "            \n",
    "        # Get full text - it's a list of strings (one per page)\n",
    "        full_text_pages = jstor_full_text_record.get('full_text', [])\n",
    "        full_text = ' '.join(full_text_pages) if full_text_pages else ''\n",
    "        \n",
    "        # Generate tokens for n-grams\n",
    "        tokens = self.clean_text_for_ngrams(full_text)\n",
    "        \n",
    "        # Generate n-grams\n",
    "        unigrams = self.generate_ngrams(tokens, 1)\n",
    "        bigrams = self.generate_ngrams(tokens, 2)\n",
    "        trigrams = self.generate_ngrams(tokens, 3)\n",
    "        \n",
    "        # Build Constellate-format record\n",
    "        constellate_record = {\n",
    "            # Basic identifiers - map from JSTOR format\n",
    "            'id': metadata.get('url', f\"jstor:{item_id}\"),  # Use JSTOR URL as ID\n",
    "            'item_id': item_id,\n",
    "            \n",
    "            # Metadata fields\n",
    "            'title': metadata.get('title', ''),\n",
    "            'creator': self._extract_creators(metadata),\n",
    "            'isPartOf': metadata.get('isPartOf', ''),\n",
    "            'publisher': self._extract_publishers(metadata),\n",
    "            'publicationYear': self._extract_year(metadata.get('published_date')),\n",
    "            'datePublished': metadata.get('published_date', ''),\n",
    "            'language': self._extract_primary_language(metadata.get('languages', [])),\n",
    "            'docType': self._map_content_type(metadata.get('content_type')),\n",
    "            'docSubType': metadata.get('content_subtype', ''),\n",
    "            \n",
    "            # DOI and identifiers\n",
    "            'doi': metadata.get('ithaka_doi', ''),\n",
    "            'identifier': metadata.get('identifiers', {}),\n",
    "            \n",
    "            # Volume/issue info\n",
    "            'volumeNumber': metadata.get('issue_volume', ''),\n",
    "            'issueNumber': metadata.get('issue_number', ''),\n",
    "            \n",
    "            # Categories and subjects\n",
    "            'tdmCategory': metadata.get('discipline_names', []),\n",
    "            'sourceCategory': metadata.get('discipline_names', []),\n",
    "            \n",
    "            # Full text\n",
    "            'fullText': full_text_pages,\n",
    "            \n",
    "            # N-grams in Constellate format\n",
    "            'unigramCount': dict(unigrams),\n",
    "            'bigramCount': dict(bigrams),\n",
    "            'trigramCount': dict(trigrams),\n",
    "            \n",
    "            # Additional fields\n",
    "            'wordCount': len(tokens),\n",
    "            'outputFormat': ['unigrams', 'bigrams', 'trigrams', 'fullText'],\n",
    "            \n",
    "            # References if available\n",
    "            'references': jstor_full_text_record.get('references', [])\n",
    "        }\n",
    "        \n",
    "        return constellate_record\n",
    "    \n",
    "    def _extract_creators(self, metadata):\n",
    "        \"\"\"Extract creator names in Constellate format\"\"\"\n",
    "        creators_string = metadata.get('creators_string', '')\n",
    "        creators_list = metadata.get('creators', [])\n",
    "        \n",
    "        if creators_list:\n",
    "            # Use structured creator data if available\n",
    "            creator_names = []\n",
    "            for creator in creators_list:\n",
    "                first = creator.get('first_name', '')\n",
    "                last = creator.get('last_name', '')\n",
    "                if first and last:\n",
    "                    creator_names.append(f\"{first} {last}\")\n",
    "                elif last:\n",
    "                    creator_names.append(last)\n",
    "            return creator_names\n",
    "        elif creators_string:\n",
    "            # Fall back to creator string, split on common separators\n",
    "            return [name.strip() for name in re.split(r'[,;]', creators_string) if name.strip()]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def _extract_publishers(self, metadata):\n",
    "        \"\"\"Extract publisher info\"\"\"\n",
    "        publishers = metadata.get('publishers', [])\n",
    "        return publishers[0] if publishers else ''\n",
    "    \n",
    "    def _extract_year(self, date_string):\n",
    "        \"\"\"Extract year from date string\"\"\"\n",
    "        if date_string and len(date_string) >= 4:\n",
    "            return int(date_string[:4])\n",
    "        return None\n",
    "    \n",
    "    def _extract_primary_language(self, languages):\n",
    "        \"\"\"Extract primary language\"\"\"\n",
    "        return languages[0] if languages else 'eng'\n",
    "    \n",
    "    def _map_content_type(self, jstor_content_type):\n",
    "        \"\"\"Map JSTOR content type to Constellate doc type\"\"\"\n",
    "        mapping = {\n",
    "            'article': 'article',\n",
    "            'book': 'book', \n",
    "            'chapter': 'chapter',\n",
    "            'report': 'report'\n",
    "        }\n",
    "        return mapping.get(jstor_content_type, jstor_content_type or 'article')\n",
    "    \n",
    "    def convert_dataset(self, metadata_file_path, fulltext_file_path, output_file_path, limit=None):\n",
    "        \"\"\"\n",
    "        Convert JSTOR dataset to Constellate format\n",
    "        \n",
    "        Args:\n",
    "            metadata_file_path (str): Path to JSTOR metadata JSONL.gz file\n",
    "            fulltext_file_path (str): Path to JSTOR full-text dataset JSONL.gz file  \n",
    "            output_file_path (str): Path for output Constellate JSONL.gz file\n",
    "            limit (int, optional): Limit number of records to process\n",
    "        \"\"\"\n",
    "        # Load metadata first\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"STEP 1: Loading metadata cache\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        self.load_metadata(metadata_file_path)\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"STEP 2: Processing full-text dataset\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Reading full-text data from: {fulltext_file_path}\")\n",
    "        logger.info(f\"Writing converted data to: {output_file_path}\")\n",
    "        if limit:\n",
    "            logger.info(f\"Processing limit set to: {limit:,} records\")\n",
    "        else:\n",
    "            logger.info(\"No processing limit set - will process all records\")\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        Path(output_file_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Process full-text dataset\n",
    "        records_processed = 0\n",
    "        records_written = 0\n",
    "        records_skipped = 0\n",
    "        \n",
    "        # Determine if files are compressed\n",
    "        fulltext_is_compressed = fulltext_file_path.endswith('.gz')\n",
    "        output_is_compressed = output_file_path.endswith('.gz')\n",
    "        \n",
    "        logger.info(f\"Full-text file: {'compressed' if fulltext_is_compressed else 'uncompressed'}\")\n",
    "        logger.info(f\"Output file: {'compressed' if output_is_compressed else 'uncompressed'}\")\n",
    "        \n",
    "        fulltext_open_func = gzip.open if fulltext_is_compressed else open\n",
    "        fulltext_mode = 'rt' if fulltext_is_compressed else 'r'\n",
    "        \n",
    "        output_open_func = gzip.open if output_is_compressed else open\n",
    "        output_mode = 'wt' if output_is_compressed else 'w'\n",
    "        \n",
    "        logger.info(\"Starting full-text processing...\")\n",
    "        \n",
    "        with fulltext_open_func(fulltext_file_path, fulltext_mode, encoding='utf-8') as infile, \\\n",
    "             output_open_func(output_file_path, output_mode, encoding='utf-8') as outfile:\n",
    "            \n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                if limit and records_processed >= limit:\n",
    "                    print()  # Move to new line\n",
    "                    logger.info(f\"Reached processing limit of {limit:,} records\")\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    jstor_record = json.loads(line.strip())\n",
    "                    constellate_record = self.convert_jstor_to_constellate_record(jstor_record)\n",
    "                    \n",
    "                    if constellate_record:\n",
    "                        outfile.write(json.dumps(constellate_record) + '\\n')\n",
    "                        records_written += 1\n",
    "                    else:\n",
    "                        records_skipped += 1\n",
    "                        \n",
    "                    records_processed += 1\n",
    "                    \n",
    "                    # Update progress on same line every 1,000 records, log milestone every 100,000\n",
    "                    if records_processed % 1000 == 0:\n",
    "                        print(f\"\\rProgress: {records_processed:,} processed | {records_written:,} written | {records_skipped:,} skipped\", end=\"\", flush=True)\n",
    "                    if records_processed % 100000 == 0:\n",
    "                        print()  # Move to new line for milestone\n",
    "                        logger.info(f\"✓ Milestone: {records_processed:,} processed | {records_written:,} written | {records_skipped:,} skipped\")\n",
    "                        \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print()  # Move to new line for warning\n",
    "                    logger.warning(f\"Failed to parse full-text line {line_num}: {e}\")\n",
    "                    records_skipped += 1\n",
    "                except Exception as e:\n",
    "                    print()  # Move to new line for error\n",
    "                    logger.error(f\"Error processing line {line_num}: {e}\")\n",
    "                    records_skipped += 1\n",
    "                    \n",
    "        print()  # Move to new line when done\n",
    "                    \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"CONVERSION COMPLETE!\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"📊 Final Statistics:\")\n",
    "        logger.info(f\"   • Total records processed: {records_processed:,}\")\n",
    "        logger.info(f\"   • Records successfully written: {records_written:,}\")\n",
    "        logger.info(f\"   • Records skipped/failed: {records_skipped:,}\")\n",
    "        logger.info(f\"   • Success rate: {(records_written/records_processed*100):.1f}%\")\n",
    "        logger.info(f\"📁 Output file: {output_file_path}\")\n",
    "        logger.info(f\"✓ Ready for use with Constellate notebooks!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the conversion\n",
    "    \n",
    "    Update the file paths below to match your downloaded files:\n",
    "    \"\"\"\n",
    "    \n",
    "    # File paths - UPDATE THESE TO MATCH YOUR FILES\n",
    "    metadata_file = 'jstor_metadata_2025-06-12.jsonl.gz'  # change this to the path to your downloaded metadata file\n",
    "    fulltext_file = 'a6ce02a8-a2d6-48b1-af37-79c18934c66f.jsonl.gz' # change this to the path to your requested full-text dataset file\n",
    "    output_file = 'constellate_format_dataset.jsonl.gz'   # Output file in the Constellate format, ready to be processed by the existing Constellate notebooks; change the file name as you wish\n",
    "    \n",
    "    # Optional: limit number of records for testing\n",
    "    limit = None  # Set to a number like 1000 for testing\n",
    "    \n",
    "    # Run conversion\n",
    "    converter = JSTORToConstellateConverter()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"🔧 Initializing JSTOR to Constellate converter...\")\n",
    "        converter.convert_dataset(\n",
    "            metadata_file_path=metadata_file,\n",
    "            fulltext_file_path=fulltext_file, \n",
    "            output_file_path=output_file,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎉 CONVERSION SUCCESSFUL!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"📁 Your Constellate-format dataset is ready: {output_file}\")\n",
    "        print(f\"🚀 You can now use this file with your existing Constellate notebooks.\")\n",
    "        print(f\"💡 Tip: Use the same dataset_reader() function as in your topic modeling notebook!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(\"\\n\" + \"❌\" * 20)\n",
    "        print(f\"ERROR: Could not find file - {e}\")\n",
    "        print(\"Please check that the file paths in main() are correct.\")\n",
    "        print(\"Expected files:\")\n",
    "        print(f\"  • Metadata: {metadata_file}\")\n",
    "        print(f\"  • Full-text: {fulltext_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during conversion: {e}\")\n",
    "        logger.exception(\"Full error details:\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185935b-ac46-41a3-be13-e9b750700047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588a678-eb8f-4ed3-bce6-aef1d87de252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
