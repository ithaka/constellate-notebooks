{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033d0d6d",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"../All-sample-files/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92053a48",
   "metadata": {},
   "source": [
    "# Concordance and Collocation\n",
    "\n",
    "**Description:** This notebook describes how to create a concordance and collocation starting from text files and from a JSONL file.\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 45 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics I](../Python-basics/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** None\n",
    "\n",
    "**Data Format:** Text, JSON File\n",
    "\n",
    "**Libraries Used:** NLTK\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06fbf4",
   "metadata": {},
   "source": [
    "## Concordance\n",
    "\n",
    "The concordance has a long history in humanities study and Roberto Busa's concordance Index Thomisticus—started in 1946—is arguably the first digital humanities project. Before computers were common, they were printed in large volumes such as John Bartlett's 1982 reference book *A Complete Concordance to Shakespeare*—it was 1909 pages pages long! \n",
    "\n",
    "A concordance gives the context of a given word or phrase in a body of texts. For example, a literary scholar might ask: how often and in what context does Shakespeare use the phrase \"honest Iago\" in Othello? A historian might examine a particular politician's speeches, looking for examples of a particular [dog whistle](https://en.wikipedia.org/wiki/Dog_whistle_(politics)).\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "\n",
    "* Geoffrey Rockwell and Stéfan Sinclair. [Tremendous Mechanical Labor: Father Busa's Algorithm](http://www.digitalhumanities.org/dhq/vol/14/3/000456/000456.html) (2020)\n",
    "* Julianne Nyhan and Marco Passarotti, eds. [One Origin of Digital Humanities: Fr Roberto Busa in His Own Words](https://link.springer.com/book/10.1007/978-3-030-18313-4) (2019)\n",
    "* Julianne Nyhan and Melissa Terras. [Uncovering 'hidden contributions to the history of Digital Humanities: the Index Thomisticus' femal keypunch operators](https://discovery.ucl.ac.uk/id/eprint/10052279/9/Nyhan_DH2017.redacted.pdf) (2017)\n",
    "* Steven E. Jones [Roberto Busa, S.J., and the Emergence of Humanities Computing](https://www.routledge.com/Roberto-Busa-S-J-and-the-Emergence-of-Humanities-Computing-The-Priest/Jones/p/book/9781138587250) (2016)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27886db",
   "metadata": {},
   "source": [
    "We will be working with The Folger Shakespeare, made freely available by the [The Folger Shakespeare Library](https://shakespeare.folger.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10f54c-e5cf-41e1-940d-39e6d69e1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef16b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data for this lesson\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "download_urls = [\n",
    "    'https://shakespeare.folger.edu/downloads/txt/shakespeares-works_TXT_FolgerShakespeare.zip',\n",
    "]\n",
    "\n",
    "for url in download_urls:\n",
    "    urllib.request.urlretrieve(url, './data/' + url.rsplit('/', 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d1f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the text files\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./data/shakespeares-works_TXT_FolgerShakespeare.zip', 'r') as zf:\n",
    "    zf.extractall('./data/shakespeare/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b409073",
   "metadata": {},
   "source": [
    "Now that we have our data downloaded and extracted, we will load a single text for our analysis. Feel free to choose a different Shakespeare text to change the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6cf4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a text\n",
    "with open(\"./data/shakespeare/othello_TXT_FolgerShakespeare.txt\") as f:\n",
    "    file_contents = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298dbee",
   "metadata": {},
   "source": [
    "Next, we lowercase our text and use the Natural Language Toolkit (NLTK) to tokenize it. Tokenizing breaks up the the document into individual words. Finally, we use our tokens to create an NLTK Text object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ac4cf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize one of the files\n",
    "import nltk\n",
    "nltk.download('punkt_tab', download_dir='./data/nltk_data')\n",
    "file_contents = file_contents.lower()\n",
    "tokens = nltk.word_tokenize(file_contents)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8263f4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify that we have created an NLTK Text object\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee8615",
   "metadata": {},
   "source": [
    "Now, we can create a concordance with the `.concordance()` method. We pass in a string and the results are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593cb90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a concordance for the given word\n",
    "text.concordance('honest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab44e1",
   "metadata": {},
   "source": [
    "By default, the first 25 matches are printed along with 80 characters on each side of our string text. We can specify that more lines should be shown using a `lines` and `width` argument that accept integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf035c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a concordance for the given word\n",
    "# Increasing lines shown and number of characters\n",
    "text.concordance('honest', lines=50, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c66dfc",
   "metadata": {},
   "source": [
    "If we want to supply a bigram, trigram, or longer construction, they are supplied as individual strings within a Python list. (If you try to supply a string with a space in the middle, there will be no results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0696186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a concordance for a sequence of words\n",
    "text.concordance(['honest', 'iago'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82ed2a",
   "metadata": {},
   "source": [
    "This method works well for a quick preview of the lines, but if we want to save this concordance for later analysis we can use the `.concordance_list()` method. The `.concordance_list()` method outputs a list, but the elements of that list *are not* simple strings. They are ConcordanceLine objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45525f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output the concordance data\n",
    "output_list = text.concordance_list(['honest', 'iago'], width=200, lines=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e3813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(output_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acc5938",
   "metadata": {},
   "source": [
    "We can view individual lines by using a Python list index followed by `.line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7859087a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_list[0].line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2894a",
   "metadata": {},
   "source": [
    "If we want to save our concordance, we can write to a file line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b2169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Writing the concordance to a text file\n",
    "\n",
    "with open('./data/concordance.txt', 'w') as f:\n",
    "    for i in range(len(output_list)):\n",
    "        f.write(output_list[i].line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a555aa",
   "metadata": {},
   "source": [
    "Lastly, NLTK can create a dispersion plot that helps visualize where tokens occur in the document. This can reveal the way words are used in the document over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8955e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.dispersion_plot(['honest', 'fool', 'noble', 'virtue', 'doubt', 'love', 'hate', 'villain'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701219f",
   "metadata": {},
   "source": [
    "## Concordance with JSONL file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ac4c6-db31-48e1-b584-def22fba32af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import your dataset\n",
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the compressed Constellate JSONL file containing metadata, ngrams and full texts to the current working directory.&lt; / &gt; </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38654715",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine the documents in the dataset\n",
    "# function that reads a jsonl file into a generator\n",
    "def dataset_reader(file_path):\n",
    "    \"\"\"\n",
    "    Helper to read in gzip files and yield Python dictionary\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)\n",
    "\n",
    "## Import your dataset\n",
    "dataset_file = '' # put the path to the jsonl.gz file here\n",
    "\n",
    "\n",
    "for document in dataset_reader(dataset_file):\n",
    "    print('\\n')\n",
    "    document_id = document.get('id')\n",
    "    document_title = document.get('title')\n",
    "    document_author = document.get('creator')\n",
    "    print(f'ID: {document_id}')\n",
    "    print(f'Title: {document_title}')\n",
    "    print(f'Author: {document_author}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bec8ba",
   "metadata": {},
   "source": [
    "There are only 63 documents in this dataset, so we can easily search through to pick out one of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70488",
   "metadata": {},
   "source": [
    "Let's use Frederick Douglass's:\n",
    "\n",
    ">NARRATIVE OF THE LIFE OF FREDERICK DOUGLASS,  AN AMERICAN SLAVE. Written by Himself:  Electronic Edition.\n",
    "\n",
    "The corresponding ID is: `http://docsouth.unc.edu/neh/douglass/menu.html`\n",
    "\n",
    "Note that this is also a URL which takes us to the Docsouth project where the document is described.\n",
    "\n",
    "We can select the document by matching on the document `'id'` key and then copy the full-text, which is stored as the value for the key `'fullText'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389913d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the full-text of the document selected\n",
    "# Here we select \n",
    "## Import your dataset\n",
    "dataset_file = ''\n",
    "text = ''\n",
    "for document in dataset_reader(dataset_file):\n",
    "    if document['id'] == 'http://docsouth.unc.edu/neh/douglass/menu.html':\n",
    "        text = document.get('fullText')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f372c1",
   "metadata": {},
   "source": [
    "We will out the contents to a text file, so we have a local copy of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013f500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write the text to a file\n",
    "with open('./data/document.txt', 'w') as f:\n",
    "    for line in text:\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e07bd",
   "metadata": {},
   "source": [
    "And then read the document back in. (Writing the document out to a file and reading it back also removes all the Python escape characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879efb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the text from the file\n",
    "with open(\"./data/document.txt\") as f:\n",
    "    file_contents = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e4c88",
   "metadata": {},
   "source": [
    "Next, we tokenize the document using the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39030920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the file\n",
    "import nltk\n",
    "file_contents = file_contents.lower()\n",
    "tokens = nltk.word_tokenize(file_contents)\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f17c65",
   "metadata": {},
   "source": [
    "And we are ready to create a concordance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead29a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.concordance('freedom', lines=50, width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee1f910",
   "metadata": {},
   "source": [
    "We can write our concordance out to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f00002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output the concordance data\n",
    "output_list = text.concordance_list('freedom', width=200, lines=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ddf402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Writing the concordance to a text file\n",
    "\n",
    "with open('./data/concordance.txt', 'w') as f:\n",
    "    for i in range(len(output_list)):\n",
    "        f.write(output_list[i].line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d79d0d",
   "metadata": {},
   "source": [
    "And visualize the distribution of words across the text in a lexical dispersion plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea121ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.dispersion_plot(['free', 'knowledge', 'read', 'desire'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e872240b",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Collocations are concepts in a text that cannot be expressed in a single word. English speakers can usually recognize collocations when we see them.\n",
    "\n",
    "Consider the concepts mentioned in this sentence:\n",
    "> The machine learning algorithm helped solve a major problem in social science research.\n",
    "\n",
    "There are bigrams and trigrams which are collocations, cohesive units that describe a larger concept:\n",
    "\n",
    "* 'machine learning algorithm' (a three-word, trigram)\n",
    "* 'major problem' (a two-word, bigram)\n",
    "* 'social science research'(a three-word, trigram)\n",
    "\n",
    "Yet other bigrams and trigrams in the sentence are not collocations:\n",
    "\n",
    "* 'The machine'\n",
    "* 'learning algorithm helped'\n",
    "* 'helped solve a'\n",
    "* 'problem in social'\n",
    "\n",
    "Identifying a collocation is more complicated than simply finding words that commonly co-occur. The phrase 'of the' occurs in many texts, but it is not a collocation since it does not relate to a larger, defined concept.\n",
    "\n",
    "Linguists and Natural Language Processing researchers have sought to create a mathematical method for discovering collocations. Here are some of the effective methods:\n",
    "\n",
    "* Finding frequent bigrams and trigrams (while removing spaces, stop words, articles, prepositions, and pronouns)\n",
    "* Pointwise Mutual Information (measuring how likely a word is to appear alone vs. paired with another)\n",
    "* [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test) (while removing spaces, stop words, articles, prepositions, and pronouns)\n",
    "* [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "\n",
    "All of these methods can be effective. We will demonstrate Pointwise Mutual Information (PMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f0d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open a text\n",
    "with open(\"./data/document.txt\") as f:\n",
    "    file_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecff44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize one of the files\n",
    "import nltk\n",
    "file_contents = file_contents.lower()\n",
    "tokens = nltk.word_tokenize(file_contents)\n",
    "removed_punctuation = [token for token in tokens if token.isalnum()] # We remove all punctuation with a list comprehension\n",
    "text = nltk.Text(removed_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8da775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbe45b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finder = BigramCollocationFinder.from_words(text)\n",
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd5d72",
   "metadata": {},
   "source": [
    "Pointwise Mutual Information is very sensitive to words that are \"highly-connected\" but also occur very infrequently. To get good results, we need to filter out bigrams that rarely occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d5db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out Bigrams that occur less than three times\n",
    "finder.apply_freq_filter(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03082d1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "finder.nbest(bigram_measures.pmi, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a6b77",
   "metadata": {},
   "source": [
    "We can also discover common trigram collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974df9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finder = TrigramCollocationFinder.from_words(text)\n",
    "finder.apply_freq_filter(3)\n",
    "#finder.apply_word_filter(lambda w: w in ('I', 'me')) Remove particular words with a lambda function\n",
    "finder.nbest(trigram_measures.pmi, 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
