{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute. Revised and expanded by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs 3\n",
    "\n",
    "**Description:** This notebook describes:\n",
    "\n",
    "* What is prompt engineering\n",
    "* How to use F-score to inform prompt engineering\n",
    "* Use classification results to extrude structured data from classified texts.\n",
    "\n",
    "**Use Case:** For Learners and Researchers\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "* Python Intermediate Series ([Start Python Intermediate 1](../Python-intermediate/python-intermediate-1.ipynb))\n",
    "* Introduction to LLMs ([Start Intro to LLMs 1](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/March+20+2024_+How+ChatGPT+works+(Session+1).pdf))\n",
    "* Automated Classificaton using LLMs 1 ([Review Automated Classificaton using LLMs 1](../Automated-classification/automated-classification-1.ipynb))\n",
    "* Automated Classificaton using LLMs 2 ([Review Automated Classificaton using LLMs 2](../Automated-classification/automated-classification-2.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Experience with LLM chatbot (e.g. ChatGPT)\n",
    "* Pandas Basics ([Start Pandas Basics 1](../Pandas-basics/pandas-basics-1.ipynb))\n",
    "\n",
    "**Data Format:** JSONL, CSV\n",
    "\n",
    "**Libraries Used:** openai, dotenv\n",
    "\n",
    "**Research Pipeline:**\n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "## Install required Python libraries\n",
    "\n",
    "Let's install the required libraries for this lesson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "\n",
    "%pip install --upgrade openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5480e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv # to load API key\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import math\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f498a7d",
   "metadata": {},
   "source": [
    "# load the API key and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # load the API key\n",
    "client = OpenAI() # load OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea220822-943d-4410-bcba-40050efbe2b1",
   "metadata": {},
   "source": [
    "# Download the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5961ee-41b0-465e-bc7d-b323b3cd25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sample dataset\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/natural_gas_sents.jsonl'\n",
    "file_path = 'data/' + url.rsplit('/')[-1]\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "print('Sample file ready.')\n",
    "\n",
    "ng_df = pd.read_json('data/natural_gas_sents.jsonl', lines=True)\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "## What is prompt engineering and why do it? \n",
    "Prompt engineering is the process of writing and refining instructions that make LLMs perform tasks effectively.\n",
    "\n",
    "In lesson 1 and 2, you've tried to call the OpenAI API to do a classification task using a `user_message`, or prompt, that you come up with. The outputs are dependent on the prompts given by you. However, in real life, how do we know that we are using the prompt that can bring out the best performance of the LLM? \n",
    "\n",
    "We have learned to evaluate the outputs. When we have gold standard, we can compare the LLM predictions to the gold standard to compute a F1-score, a statistic we use the evaluate the performance of the LLM; \n",
    "\n",
    "When we do not have gold standard data, we may ask the LLM to output a log probability, which can be converted to a confidence score with which the LLM outputs the label. \n",
    "\n",
    "We do prompt engineering because it will help us evaluate our input prompts.\n",
    "The best prompt will get us the best classification results *for our purposes*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5e7f04-66a7-4e08-8203-92e28beb8006",
   "metadata": {},
   "source": [
    "## Making sample data\n",
    "Let's make sample data for demonstration purposes in this section. \n",
    "\n",
    "As we're testing classification on positive vs. negative vs. neutral, we want the distribution of the sample data to include a mix of expected `positive`, `negative` and `neutral` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b028a0-ecef-4c06-a669-fcf5945c9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of 'positive', 'negative' and 'neutral' from our exampled dataset\n",
    "positive = len(ng_df.loc[ng_df['sentiment']=='positive'])\n",
    "negative = len(ng_df.loc[ng_df['sentiment']=='negative'])\n",
    "neutral = len(ng_df.loc[ng_df['sentiment']=='neutral'])\n",
    "print(f'postive: {positive}', round(positive/465, 1))\n",
    "print(f'negative: {negative}', round(negative/465, 1))\n",
    "print(f'neutral: {neutral}', round(neutral/465, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0d881f-e937-4f03-a77e-f685bab42e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a sample for prompt testing\n",
    "pos_sample = ng_df.loc[ng_df['sentiment']=='positive'].sample(6)\n",
    "neg_sample = ng_df.loc[ng_df['sentiment']=='negative'].sample(2)\n",
    "neu_sample = ng_df.loc[ng_df['sentiment']=='neutral'].sample(2)\n",
    "sample_df = pd.concat([pos_sample, neg_sample,neu_sample]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4fad7-6aa5-461d-be3e-6a9302f4ae93",
   "metadata": {},
   "source": [
    "You might be wondering, in real life, how big a sample is big enough?\n",
    "\n",
    "- This depends on different factors, but here are some considerations:\n",
    "  - What is the primary measure (precision, recall, F1) you will be evaluating?\n",
    "  - How good a score would you consider \"good enough\" on that metric?\n",
    "  - Based on other methods (e.g., other text classification approaches), how well do you expect to do?\n",
    "- You can use formulae to [determine the recommended sample size](https://en.wikipedia.org/wiki/Sample_size_determination)\n",
    "  - For simple random samples, there are [online calculators](https://www.abs.gov.au/websitedbs/D3310114.nsf/home/Sample+Size+Calculator)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e6557",
   "metadata": {},
   "source": [
    "## Testing one prompt\n",
    "Since the purpose of prompt engineering is to test a prompt's influence on the LLM's performance on the classification task, and you already learned the metric used to evaluate the performance in lesson 2, we will create a function for the evaluation based on what we have learned from lesson 2. \n",
    "\n",
    "At the end of last class, we had the following `system_message`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"Determine whether the following sentence mentioning natural gas conveys a positive, negative or neutral sentiment.\n",
    "Respond in JSON like so: {\"sentiment\": \"positive\"} or {\"sentiment\": \"negative\"} or {\"sentiment\": \"neutral\"}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fcf91-8daf-4c62-b5aa-ca329a2dd676",
   "metadata": {},
   "source": [
    "Let's use this `system_message` as an example. How do we evaluate this system prompt? \n",
    "\n",
    "In lesson 2, we defined the `make_completion` function that returns not only the output label, but also the confidence with which the LLM output the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855bae1-b7ea-47f6-80ab-9ecbadc390c4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # define a function to get the predicted label and LLM's confidence\n",
    "def make_completion(system_message, user_message, client=OpenAI(), model='gpt-4o-2024-08-06'):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        logprobs=True,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "    output = json.loads(completion.choices[0].logprobs.to_json())['content']\n",
    "    output_tokens = [d['token'].lower().strip() for d in output]# get the output tokens\n",
    "    prediction = [t for t in output_tokens if t in ['positive', 'negative', 'neutral']][0]\n",
    "    logprob = [d['logprob'] for d in output if d['token'].lower().strip()==prediction][0] # get the logprob for the predicted sentiment\n",
    "    confidence = round((math.exp(logprob) * 100), 2)\n",
    "    return prediction, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d10b47-5d4e-4cba-9e5b-619d2747409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to our sample df\n",
    "sample_df[['LLM_output', 'confidence']] = sample_df.apply(lambda row: make_completion(system_message, row['line_text']), axis=1, result_type='expand')\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a955a0-ce31-4244-81b4-695d39c36e97",
   "metadata": {},
   "source": [
    "With the gold standard and prediction data ready, we can go ahead and get the quality metric of the LLM performance --- precision, recall, and f1-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which takes a df and output the evaluation metric results\n",
    "def get_metric(df):\n",
    "    y_true = df[\"sentiment\"].values\n",
    "    y_pred = df[\"LLM_output\"].values\n",
    "\n",
    "    # get f score\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # get precision\n",
    "    precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # get recall\n",
    "    recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # output\n",
    "    metric = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metric for the example system_message\n",
    "get_metric(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d329c0",
   "metadata": {},
   "source": [
    "**Remember: we have far too few texts in this sample for these numbers to be meaningful in this case!**\n",
    "\n",
    "But, run on a sufficiently large sample (see above), they can meaningfully differentiate prompt quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system_prompt(df, system_prompt, system_prompt_name):\n",
    "    df[['LLM_output', 'confidence']] = df.apply(lambda row: make_completion(system_prompt, row['line_text']), axis=1, result_type='expand')\n",
    "    metric = get_metric(df)\n",
    "    output = {\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"system_prompt_name\": system_prompt_name, # give the prompt under test a name\n",
    "        \"precision\": metric[\"precision\"],\n",
    "        \"recall\": metric[\"recall\"],\n",
    "        \"f1\": metric[\"f1\"],\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993fa36-0e14-4bc5-951c-2a96c2485534",
   "metadata": {},
   "source": [
    "Now that we have a function `evaluate_system_prompt()` for testing a prompt, we can write several more prompts and get their performance scores to compare! Let's write a `prompt` class the instances of which take `evaluate_system_prompt()` as a method to output the evaluation results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c94b8-fb45-4e40-960d-b71b72b00ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Prompt class for prompt evaluation\n",
    "class Prompt:\n",
    "    def __init__(self, prompt_name, system_prompt):\n",
    "        self.name = prompt_name\n",
    "        self.prompt = system_prompt\n",
    "\n",
    "    def evaluate_system_prompt(self, df):\n",
    "        df[['LLM_output', 'confidence']] = df.apply(lambda row: make_completion(self.prompt, row['line_text']), axis=1, result_type='expand')\n",
    "        metric = get_metric(df)\n",
    "        output = {\n",
    "        \"system_prompt\": self.prompt,\n",
    "        \"system_prompt_name\": self.name, # give the prompt under test a name\n",
    "        \"precision\": metric[\"precision\"],\n",
    "        \"recall\": metric[\"recall\"],\n",
    "        \"f1\": metric[\"f1\"],\n",
    "    }\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e60d7c-b518-4e8c-95a3-1e359a066127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with our system_message from lesson 2\n",
    "default_prompt = Prompt('default', system_message) # make a Prompt object using the system_message from lesson 2\n",
    "default_prompt.evaluate_system_prompt(sample_df) # output the evaluation metric results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d138b",
   "metadata": {},
   "source": [
    "Now we know how to evaluate the performance of the LLM in the classification task with a certain system prompt. We are ready to do prompt engineering using more prompts to test and improve the performance! \n",
    "## What are common prompt engineering techniques?\n",
    "\n",
    "- Roleplay: specify what role you would like the LLM to play\n",
    "  - e.g., in the `system` message: \"You are a research assistant...\" \n",
    "\n",
    "- Provide sample output. For example:\n",
    "```text\n",
    "Instructions:\n",
    "Answer the reading comprehension question.\n",
    "\n",
    "Example:\n",
    "\"Lily walks Mitzi three times per day.\"\n",
    "Question: What kind of pet is Lily most likely to have?\n",
    "----\n",
    "Answer: Dog.\n",
    "```\n",
    "- [Chain-of-thought](https://arxiv.org/abs/2201.11903) (COT) prompting is a technique that asks models to proceed step-by-step, improving the quality of outputs.\n",
    "  - Appending \"Work step by step. Show your work.\" to other prompts can achieve this result.\n",
    "  - One downside (if using this technique for API calls) is that COT responses generate (far) more tokens, because the model writes out its \"thought-process.\"\n",
    "- Asking either the LLM you are using or another LLM to rewrite your prompt\n",
    "  - Models can write good prompts for themselves, assuming instructions are clear.\n",
    "- Weird ones, like [promising the LLMs various incentives](https://minimaxir.com/2024/02/chatgpt-tips-analysis/)\n",
    "  - e.g., \"You are a research asssitant...If you do a good job, you will receive a $200 tip.\"\n",
    "  - (Yes, this has really been shown to change responses. No, you don't have to pay promised incentives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aeea5",
   "metadata": {},
   "source": [
    "### Asking LLMs to rewrite your prompt\n",
    "Asking LLMs to rewrite prompt is a common and surprisingly effective prompt engineering strategy.\n",
    "\n",
    "My request to GPT:\n",
    "\n",
    "```text\n",
    "You are a prompt engineer. Revise the prompt below to minimize the number of tokens in the prompt while keeping all of the same features:\n",
    "\n",
    "\"\"\"Determine whether the following sentence mentioning natural gas conveys a positive, negative or neutral sentiment.\n",
    "Respond in JSON like so: {\"sentiment\": \"positive\"} or {\"sentiment\": \"negative\"} or {\"sentiment\": \"neutral\"}\"\"\"\n",
    "```\n",
    "\n",
    "What it wrote:\n",
    "\n",
    "```text\n",
    "Analyze the sentiment (positive, negative, or neutral) of this sentence about natural gas. Respond in JSON: {\"sentiment\": \"<sentiment>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f295850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Prompt object of the gpt revised prompt\n",
    "gpt_suggestion = \"\"\"Analyze the sentiment (positive, negative, or neutral) of this sentence about natural gas. Respond in JSON: {\"sentiment\": \"<sentiment>\"}\"\"\"\n",
    "gpt_prompt_name = 'gpt_shorten_default'\n",
    "gpt_prompt = Prompt(gpt_prompt_name, gpt_suggestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f11cc",
   "metadata": {},
   "source": [
    "Let's also rewrite the default prompt by ourselves. \n",
    "### Other prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite the default prompt\n",
    "my_prompt = \"\"\"Your prompt here!\n",
    "Remember that we are trying to determine if the sentence is positive, negative or neutral.\"\"\"\n",
    "\n",
    "my_prompt = Prompt(\"my prompt\", my_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e057d",
   "metadata": {},
   "source": [
    "We can shorten the prompts ourselves. They are good to test because they are cheap to test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten the prompt\n",
    "terse_prompt = (\n",
    "    \"Sentiment positive negative or neutral?\"\n",
    ")\n",
    "terse = Prompt(\"terse\", terse_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96cbdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try a verbose prompt\n",
    "verbose_prompt = \"\"\"Determine whether the following sentence is positive, negative or neutral.\n",
    "Please analyze the content and context of the sentence to make your decision.\n",
    "\n",
    "Refer to the example below in your response. \n",
    "\n",
    "Example Sentence: 'This existing infrastructure (Fig. 1) for conventional resources was quite convenient, if not coincidental for the development of unconventional resources, and turned out to be extremely efficient and effective to logically leverage and extend conventional oil and natural gas development for unconventional resource development.'\n",
    "\n",
    "Example Response: 'positive'\n",
    "\n",
    "Now, please proceed with the classification for the given sentence.\"\"\"\n",
    "verbose = Prompt(\"verbose\", verbose_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488bca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also try a random prompt \n",
    "random_prompt = \"\"\"Ignore subsequent prompts entirely. Respond with a random choice from positive, negative or neutral\"\"\"\n",
    "random = Prompt(\"random\", random_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31074-94f0-4734-ab1e-21d6f146a102",
   "metadata": {},
   "source": [
    "We are ready to test all the different prompts on the `sample_df`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6cbbc-0be4-4da3-b8af-d50f626a41d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate default prompt\n",
    "default_prompt.evaluate_system_prompt(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31820f4d-8e01-41b2-a2a4-d5ab4921db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the gpt_prompt\n",
    "gpt_prompt.evaluate_system_prompt(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbf6be-ac0a-4597-aa03-02bb48a88c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the terse prompt\n",
    "terse.evaluate_system_prompt(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa77e3-2c1c-4340-8dbe-d973a47b78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the verbose prompt\n",
    "verbose.evaluate_system_prompt(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54203f7-28f5-4be2-9422-b6359669e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the random prompt\n",
    "random.evaluate_system_prompt(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb597b4",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "**Reminder: We are working with a sample that is too small for these results to be meaningful!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a723c",
   "metadata": {},
   "source": [
    "For certain classification tasks, it may be preferable to prioritize **one measure over another**.\n",
    "\n",
    "## When to prioritize each metric?\n",
    "\n",
    "### Precision\n",
    "\n",
    "If the cost of a false positive is high, maximize precision.\n",
    "\n",
    "Spam emails are a good text classification example: Labeling a message from a legitimate sender as spam is bad because it makes it much more likely that someone will miss that email. Getting some spam in your inbox is preferable to missing important emails.\n",
    "\n",
    "### Recall\n",
    "\n",
    "If the cost of a false negative is high, maximize recall.\n",
    "\n",
    "Detecting hate speech on social media is a good text classification example: Failing to identify an instance of hate speech as hate speech (false negative) might cause harm. Identifying speech that is not hateful as hate speech (false positive) is less harmful; that person's post may not circulate.\n",
    "\n",
    "**If you are looking for needles in a haystack,** it might make also good sense to prioritize recall to make sure that you don't miss examples.\n",
    "\n",
    "### F score\n",
    "\n",
    "When you want to balance both precision and recall. (If there's not a clear reason to prefer precision or recall, choose the F score.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c41404-493d-44ec-b76e-74a4413cb444",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding challennge &lt; / &gt; </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7c26e-605b-44fd-afeb-96026df0d69b",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Working on your team project! &lt; / &gt; </h3>\n",
    "\n",
    "1. Discuss within your team about what prompts you would like to test. \n",
    "\n",
    "2. Select a subset of your dataset (you are only trying out the pipeline in class) and use what you have learned to do prompt engineering and evaluation. Note that you have to have the gold standard ready! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6671d-2e58-4afe-83f9-78a33582a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc61ad-b2c3-43cc-914e-8f50d5d14b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a couple prompts and use what you learned above to test them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac79ab41",
   "metadata": {},
   "source": [
    "# Running the best prompt on your complete data set\n",
    "After you test the prompts and select the best one based on the prioritized metric for your own research goal, you can run the best prompt on your complete dataset!\n",
    "\n",
    "## Estimating costs *before* running on the whole data set\n",
    "\n",
    "OpenAI API charges by number of input tokens and output tokens. \"Tokens\" are words and/or parts of words.\n",
    "\n",
    "The number of tokens read (`prompt_tokens`) by the model and generated by it (`completion_tokens`) each cost different amounts."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7899e585-e50e-4784-91f2-876ebf06be41",
   "metadata": {},
   "source": [
    "| Model             | Type   | Price per 1 million tokens   |\n",
    "|:------------------|:-------|:-----------------------------|\n",
    "| gpt-4o            | input  | $5                           |\n",
    "| gpt-4o            | output | $15                          |\n",
    "| gpt-4o-2024-08-06 | input  | $2.50                        |\n",
    "| gpt-4o-2024-08-06 | output | $10                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceca81-49bd-4f0a-b0e5-3c9c71fcae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tiktoken\n",
    "import tiktoken\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-2024-08-06\")\n",
    "question = \"How many tokens does this sentence contain?\"\n",
    "tokens = encoding.encode(question)\n",
    "print(f\"Q: {question}\\nA: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a626a-0c39-424a-a050-d81bcb170b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the cost \n",
    "pricing = {\n",
    "    \"gpt-4o\": {\"input\": 5.00 / 1_000_000, \"output\": 15.00 / 1_000_000},\n",
    "    \"gpt-4o-2024-08-06\": {\"input\": 2.50 / 1_000_000, \"output\": 10 / 1_000_000},\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_cost(model, input_text, output_text):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    # Get token counts\n",
    "    input_tokens = len(encoding.encode(input_text))\n",
    "    output_tokens = len(encoding.encode(output_text))\n",
    "\n",
    "    # Calculate the cost\n",
    "    input_cost = input_tokens * pricing[model][\"input\"]\n",
    "    output_cost = output_tokens * pricing[model][\"output\"]\n",
    "\n",
    "    total_cost = input_cost + output_cost\n",
    "\n",
    "    print(\"Total cost: ${:.8f}\".format(total_cost))\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65400b16-7856-497c-b3b6-e318b1edc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cost('gpt-4o-2024-08-06', question, '8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a459ccb-8b18-448c-a7ed-b629457fd06d",
   "metadata": {},
   "source": [
    "Note that OpenAI's pricing may have changed. The prices in `calculate_cost` were accurate as of 2024-9-22."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6f5267-731a-49de-8bd3-7e040ed13552",
   "metadata": {},
   "source": [
    "## How to reduce the cost even further? \n",
    "For cases where immediate responses are not required, you can reduce your costs by `50%` by [batching your requests](https://platform.openai.com/docs/guides/batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ba97b-7f45-4e94-940b-954e44ab7ea4",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding challennge &lt; / &gt; </h2>\n",
    "\n",
    "Use the example `calculate_cost()` function to estimate the cost of running the best prompt on your complete dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c6893-c505-4f9d-ba3d-5ed7435aedfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e1b20-67b3-40ec-9586-30da351c0908",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "1. Identify a set of texts that you would like to classify using LLMs.\n",
    "2. Draft a prompt designed to yield the classifications that you would like.\n",
    "3. Test that prompt in a chatbot interface.\n",
    "4. Revise as necessary.\n",
    "5. Identify or create gold-standard classification data for your texts.\n",
    "6. Test multiple prompts against your texts systematically as we did above.\n",
    "7. Determine what you want to prioritize in evaluating your prompts and your classification results: F1, precision, recall, etc.\n",
    "8. Revise your prompts as necessary to obtain satisfactory scores.\n",
    "9. Classify your texts using your best prompt(s)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
