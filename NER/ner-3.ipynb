{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"../All-sample-files/CC_BY.png\"><br />\n",
    "\n",
    "This notebook was created by [William Mattingly](https://datascience.si.edu/people/dr-william-mattingly) for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/) and [Zoe LeBlanc](https://ischool.illinois.edu/people/zoe-leblanc) for the 2021 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Virginia Libraries](https://library.virginia.edu).\n",
    "\n",
    "This notebook is adapted by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Multilingual NER 3\n",
    "\n",
    "This is lesson 3 in the educational series on named entity recognition. \n",
    "\n",
    "**Description:** This notebook describes:\n",
    "* how to understand word embeddings as a concept\n",
    "* how to understand Machine Learning as a concept\n",
    "* how to understand supervised learning\n",
    "* how to do NER ML in spaCy 3\n",
    "\n",
    "**Use case:** Explanation\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "\n",
    "* Python basics ([start learning Python basics](../Python-basics/python-basics-1.ipynb))\n",
    "* [Python intermediate 4](../Python-intermediate/python-intermediate-4.ipynb) (OOP, classes, instances, inheritance)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* Basic file operations ([start learning file operations](../Python-intermediate/python-intermediate-2.ipynb))\n",
    "* Data cleaning with `Pandas` ([start learning Pandas](../Pandas-basics/pandas-basics-1.ipynb))\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26823dcc",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8daaae1-b83e-4790-980b-c39270d4c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Install your specific packages\n",
    "packages = [\n",
    "    'beautifulsoup4==4.12.2',\n",
    "    'click==8.1.3',\n",
    "    'gensim==4.3.1',\n",
    "    'ipympl==0.9.3',\n",
    "    'jupyter-ai==2.19.1',\n",
    "    'jupyter-ai-magics==2.19.0',\n",
    "    'jupyterlab-git==0.50.0',\n",
    "    'matplotlib==3.8.4',\n",
    "    'numpy>=1.16',\n",
    "    'nltk==3.9.1',\n",
    "    'openai==1.51.0',\n",
    "    'pandas>=2.0.3',\n",
    "    'pillow==10.3.0',\n",
    "    'pyarrow==14.0.1',\n",
    "    'pyldavis==3.4.1',\n",
    "    'pytesseract==0.3.10',\n",
    "    'regex==2023.6.3',\n",
    "    'requests==2.32.3',\n",
    "    'scikit-learn==1.5.1',\n",
    "    'scipy==1.11.1',\n",
    "    'seaborn==0.12.2',\n",
    "    'spacy==3.5.4',\n",
    "    'urllib3==2.2.2',\n",
    "    'vadersentiment==3.3.2',\n",
    "    'wordcloud==1.9.2',\n",
    "    'zipp==3.19.2'\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    install(package)\n",
    "\n",
    "# Additional setup for specific packages that need extra data/models\n",
    "print(\"Setting up additional package data...\")\n",
    "\n",
    "# NLTK data downloads\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    print(\"✓ NLTK data downloaded\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ NLTK setup issue: {e}\")\n",
    "\n",
    "print(\"Package installation and setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm # for English NER\n",
    "!python3 -m spacy download en_core_web_md # for showing the word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac6c55",
   "metadata": {},
   "source": [
    "# Introduction to word embeddings\n",
    "\n",
    "How do we represent word meanings in NLP? One way we can represent word meanings is to use word vectors. **Word embeddings** are vector representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a2de3",
   "metadata": {},
   "source": [
    "## Distributional hypothesis\n",
    "\n",
    "Word embeddings is inspired by the **distributional hypothesis** proposed by Harris ([1954](https://link.springer.com/chapter/10.1007/978-94-009-8467-7_1). This theory could be summarized as: words that have similar context will have similar meanings.\n",
    "\n",
    "What does \"context\" mean in word embeddings? Basically, \"context\" means the neighboring words of a target word. \n",
    "\n",
    "Consider the following example. If we choose \"village\" as the target word and choose a fixed size context window of 2, the two words before \"village\" and the two words after \"village\" will constitute the context of the target word.\n",
    "\n",
    "Treblinka is **a small** **<span style=\"color: blue;\">village</span>** **in Poland.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc39b123",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Google’s pre-trained word2vec model includes word vectors for a vocabulary of 3 million words and phrases that they trained on roughly 100 billion words from a Google News dataset. The vector length is 300 features, which means each of the 3 million words in the vocabulary is represented by a vector with 300 floating numbers. Word2Vec is one of the most popular techniques to learn word embeddings.\n",
    "\n",
    "The training samples are the (target, context) pairs from the text data. For example, suppose your source text is the sentence \"The quick brown fox jumps over the lazy dog\". If you choose \"quick\" as your target word and have set a context window of size 2, you will get three training samples for it, i.e. (quick, the), (quick, brown) and (quick fox).   \n",
    "\n",
    "**McCormick, C**. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://mccormickml.com/\n",
    "\n",
    "The word2vec model is trained to accomplish the following task: given the input word $w_{1}$, for each word $w_{2}$ in our vocab, how likely $w_{2}$ is a context word of $w_{1}$.\n",
    "\n",
    "The network is going to learn the statistics from the number of times each (target, context) shows up. So, for example, if you have a text about kings, queens and kingdoms, the network is probably going to get many more training samples of (\"King\", \"Queen\") than (\"King\", \"kangaroo\"). Therefore, if you give your trained model the word \"King\" as input, then it will output a much higher probability for \"Queen\" than it will for \"kangaroo\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae805",
   "metadata": {},
   "source": [
    "## Word vectors in SpaCy\n",
    "\n",
    "We have used the small English model from spaCy in the previous two notebooks. Actually, there are medium size and large size English models from spaCy as well. Both are trained using the word2vec family of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663703eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the medium size English model from spaCy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Get the word vector for the word \"King\"\n",
    "nlp(\"King\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the vector\n",
    "nlp(\"King\").vector.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa846c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"Queen\"\n",
    "nlp(\"King\").similarity(nlp(\"Queen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2dfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the similarity between the two words \"King\" and \"kangaroo\"\n",
    "nlp(\"King\").similarity(nlp(\"kangaroo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99377411-f756-4003-994d-6a202c51cf6e",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning \n",
    "\n",
    "How is word2vector model trained? The model is trained using a machine learning technique. \n",
    "\n",
    "Machine learning is a branch of artificial intelligence. Traditionally the human writes the rules in a computer system to perform a specific task. In machine learning, we use statistics to write the rules for us.\n",
    "\n",
    "## The machine learning pipeline\n",
    "<center><img src='../All-sample-files/NER_ML_pipeline.png' width=700></center>\n",
    "\n",
    "Let's use a simple example to understand the ML pipeline. Suppose you are interested in the relationship between the size and the price of a house in your neighborhood. Specifically, you would like to use the size of a house to predict its price. You go to Redfin/Zillow and find the information about the recently sold houses in your neighborhood. You note down their size and sale price. You draw a scatter plot like the following to examine the data. \n",
    "\n",
    "<center><img src='../All-sample-files/NER_housebuying_scatter.png' width=300></center>\n",
    "\n",
    "What you have in this scatter plot is your data. Now, you would like to derive a relationship between the house size and house price. Let's use linear regression in this case. Essentially, you fit a line to the data points. \n",
    "\n",
    "<center><img src='../All-sample-files/NER_housebuying.png' width=300></center>\n",
    "\n",
    "The function for this line is y = ax + b (where y is the price and x is the # of sqft). Of course, you would not just fit any line to your data points. You would want to fit a line so that the difference between the actual house prices and the predicted house prices is the smallest. Our task, then, reduces to the calculation of the value of a and b in the function y = ax + b so that the difference between the actual house prices and the predicted house prices is the smallest.\n",
    "\n",
    "## ML in Word2Vec\n",
    "\n",
    "<center><img src='../All-sample-files/NER_ML_pipeline.png' width=700></center>\n",
    "\n",
    "The ML method used in word2vec is a shallow neural network with one hidden layer of neurons and one output layer of neurons. Chris McCormick has a very detailed explanation of this model in his blog post http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/. Let's go take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7bf26-6a6c-4cfb-9987-04ae0342da31",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41c009-2809-4dc1-8939-dbf3951665df",
   "metadata": {},
   "source": [
    "**Supervised learning** is the process by which a system learns from a set of inputs that have known labels. To train a model, you first need training data – text examples, and the gold standard – labels you want the model to predict. This means that your training data need to be annotated.\n",
    "\n",
    "### Training and evaluation\n",
    "\n",
    "\"When training a model, we don’t just want it to memorize our examples – we want it to come up with a theory that can be generalized across unseen data. After all, we don’t just want the model to learn that this one instance of “Amazon” right here is a company – we want it to learn that “Amazon”, in contexts like this, is most likely a company. That’s why the training data should always be representative of the data we want to process. A model trained on Wikipedia, where sentences in the first person are extremely rare, will likely perform badly on Twitter. Similarly, a model trained on romantic novels will likely perform badly on legal text.\n",
    "\n",
    "This also means that in order to know how the model is performing, and whether it’s learning the right things, you don’t only need training data – you’ll also need evaluation data.\"\n",
    "\n",
    "https://spacy.io/usage/training\n",
    "\n",
    "**Honnibal, M., & Montani, I.** (2017). spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing.\n",
    "\n",
    "The training data is used to hone a statistical model via predetermined algorithms. It does this by making guesses about what the proper labels are. It then checks its accuracy against the correct labels, i.e., the annotated labels, and makes adjustments accordingly. Once it is finished viewing and guessing across all the training data, the first **epoch**, or **iteration** over the data, is finished. At this stage, the model then tests its accuracy against the evaluation data. The training data is then randomized and given back to the system for x number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68af31-4703-4750-865a-818142566dd5",
   "metadata": {},
   "source": [
    "# NER with EntityRuler vs. ML NER\n",
    "\n",
    "In this section, we are going to make two models to do the same NER task, one doing NER with an EntityRuler and the other doing NER using word vectors.\n",
    "\n",
    "First, let's download the two data files needed for this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8ba8af",
   "metadata": {},
   "source": [
    "The first file stores the information about the spells in Harry Potter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18218bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hp_spells = '../All-sample-files/NER_HarryPotter_Spells.csv'\n",
    "spells_df = pd.read_csv(hp_spells, sep=\";\")\n",
    "spells_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148d7d2",
   "metadata": {},
   "source": [
    "In the second file, we find the characters speaking and their speech. Notice that there is a column storing the spells found in the sentence if there is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a00ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_film_spells = '../All-sample-files/NER_HarryPotter_FilmSpells.csv'\n",
    "film_spells = pd.read_csv(hp_film_spells)\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948edc5",
   "metadata": {},
   "source": [
    "Suppose we would like to create a model that can identify spells in a sentence and give it the label 'SPELL'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51643760",
   "metadata": {},
   "source": [
    "## Create an NLP model with an EntityRuler to identify the spells\n",
    "\n",
    "In the following, we will first create a NLP model with an entity ruler that identifies spells. This section can be seen as a review of what we have learned about EntityRuler in Wednesday's lesson.\n",
    "Before we create a new EntityRuler, we will do some preprocessing of the data to get the patterns that we will add to the EntityRuler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b30d9",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f555ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN cells with an empty string\n",
    "spells_df['Incantation'] = spells_df['Incantation'].fillna(\"\")\n",
    "\n",
    "# Get all spells\n",
    "spells = spells_df['Incantation'].unique().tolist() # Put all strs in the 'Incantation' column in a list\n",
    "spells = [spell for spell in spells if spell != ''] # Get all non-empty strs from the list, i.e. all the spells\n",
    "\n",
    "# Take a look at the spells\n",
    "spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067334d",
   "metadata": {},
   "source": [
    "### Creating the patterns to be added to the EntityRuler\n",
    "\n",
    "Recall from Wednesday's lesson that the patterns we add to an EntityRuler look like the following.\n",
    "\n",
    "`patterns = [{\"label\": \"GPE\", \"pattern\": \"Aars\"}]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95345f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the pattern to be added to the ruler\n",
    "patterns = [{\"label\":\"SPELL\", \"pattern\":spell} for spell in spells]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865668ad",
   "metadata": {},
   "source": [
    "Now that we have the patterns ready, we can add them to an EntityRuler and add the ruler as a new pipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087cb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EntityRuler and add the patterns to the ruler\n",
    "entruler_nlp = spacy.blank('en') # Create a blank English model\n",
    "ruler = entruler_nlp.add_pipe(\"entity_ruler\") \n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e938c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"Ron Weasley: Wingardium Leviosa! Hermione Granger: You're saying it wrong. \n",
    "It's Wing-gar-dium Levi-o-sa, make the 'gar' nice and long. \n",
    "Ron Weasley: You do it, then, if you're so clever\"\"\"\n",
    "doc = entruler_nlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print('EntRulerModel', ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60d0bf",
   "metadata": {},
   "source": [
    "In this model, we have basically hard written all spell strings in the EntityRuler. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c69b3",
   "metadata": {},
   "source": [
    "## Train a NLP model using ML to identify the spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eb6c0b",
   "metadata": {},
   "source": [
    "The format of the training data will look like the following. It is a list of tuples. In each tuple, the first element is the text string containing spells and the second element is a dictionary. The key of the dictionary is 'entities'. The value is a list of lists. In each list, we find the starting index, ending index and the label of the spell(s) found in the text string. \n",
    "\n",
    "`[\n",
    "('Oculus Reparo', {'entities': [[0, 13, 'SPELL']]}),\n",
    "('Alohomora', {'entities': [[0, 9, 'SPELL']]})\n",
    "]`\n",
    "\n",
    "The text strings we use for the training are from the 'Sentence' column of the film_spells dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the film_spells df\n",
    "film_spells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c362e54",
   "metadata": {},
   "source": [
    "Since we have hard written all spell strings in the EntityRuler and give them the label 'SPELL', we could just use this model to generate labeled data as our training data and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "# for sentence tokenization\n",
    "nltk.download('punkt_tab')\n",
    "def generate_labeled_data(ls_sents): # the input will be a list of strings\n",
    "    text = ' '.join(ls_sents)\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    labeled_data = []\n",
    "    for sent in sents:\n",
    "        doc = entruler_nlp(sent) # create a doc object\n",
    "        if doc.ents != (): # if there is at least one entity identified\n",
    "            labeled_data.append((sent, {\"entities\":[[ent.start_char, ent.end_char, ent.label_] for ent in doc.ents]}))\n",
    "    return labeled_data       \n",
    "\n",
    "# Assign the result from the function to a new variable\n",
    "training_validation_data = generate_labeled_data(film_spells['Sentence'].tolist())\n",
    "\n",
    "# Take a look at the labeled data\n",
    "training_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a8751",
   "metadata": {},
   "source": [
    "spaCy 3 requires that our data be stored in the proprietary `.spacy` format. To do that we need to use the `DocBin` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin \n",
    "\n",
    "db = DocBin() \n",
    "\n",
    "for text, annot in training_validation_data[:19*2]: # Get the first 38 tuples as the training data\n",
    "    doc = entruler_nlp(text) # create a doc object\n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./train_spells.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, annot in training_validation_data[19*2:]: # Get the rest tuples as the validation data\n",
    "    doc = entruler_nlp(text) \n",
    "    doc.ents = [doc.char_span(ent[0], ent[1], label=ent[2]) for ent in annot['entities']]\n",
    "    db.add(doc)\n",
    "db.to_disk(f\"./valid_spells.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa73251",
   "metadata": {},
   "source": [
    "Now we can finally start training our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy init config --lang en --pipeline ner config.cfg --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy train config.cfg --output ./output/spells-model/ --paths.train ./train_spells.spacy --paths.dev ./valid_spells.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0e1b6",
   "metadata": {},
   "source": [
    "Now let's finally run our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44239291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_best = spacy.load('./output/spells-model/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926983c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try our model on this long text string\n",
    "test_text = \"\"\"53. Imperio - Makes target obey every command But only for really, really funny pranks. 52. Piertotum Locomotor - Animates statues On one hand, this is awesome. On the other, someone would use this to scare me.\n",
    "\n",
    "51. Aparecium - Make invisible ink appear\n",
    "\n",
    "Your notes will be so much cooler.\n",
    "\n",
    "50. Defodio - Carves through stone and steel\n",
    "\n",
    "Sometimes you need to get the eff out of there.\n",
    "\n",
    "49. Descendo - Moves objects downward\n",
    "\n",
    "You'll never have to get a chair to reach for stuff again.\n",
    "\n",
    "48. Specialis Revelio - Reveals hidden magical properties in an object\n",
    "\n",
    "I want to know what I'm eating and if it's magical.\n",
    "\n",
    "47. Meteolojinx Recanto - Ends effects of weather spells\n",
    "\n",
    "Otherwise, someone could make it sleet in your bedroom forever.\n",
    "\n",
    "46. Cave Inimicum/Protego Totalum - Strengthens an area's defenses\n",
    "\n",
    "Helpful, but why are people trying to break into your campsite?\n",
    "\n",
    "45. Impedimenta - Freezes someone advancing toward you\n",
    "\n",
    "\"Stop running at me! But also, why are you running at me?\"\n",
    "\n",
    "44. Obscuro - Blindfolds target\n",
    "\n",
    "Finally, we don't have to rely on \"No peeking.\"\n",
    "\n",
    "43. Reducto - Explodes object\n",
    "\n",
    "The \"raddest\" of all spells.\n",
    "\n",
    "42. Anapneo - Clears someone's airway\n",
    "\n",
    "This could save a life, but hopefully you won't need it.\n",
    "\n",
    "41. Locomotor Mortis - Leg-lock curse\n",
    "\n",
    "Good for footraces and Southwest Airlines flights.\n",
    "\n",
    "40. Geminio - Creates temporary, worthless duplicate of any object\n",
    "\n",
    "You could finally live your dream of lying on a bed of marshmallows, and you'd only need one to start.\n",
    "\n",
    "39. Aguamenti - Shoot water from wand\n",
    "\n",
    "No need to replace that fire extinguisher you never bought.\n",
    "\n",
    "38. Avada Kedavra - The Killing Curse\n",
    "\n",
    "One word: bugs.\n",
    "\n",
    "37. Repelo Muggletum - Repels Muggles\n",
    "\n",
    "Sounds elitist, but seriously, Muggles ruin everything. Take it from me, a Muggle.\n",
    "\n",
    "36. Stupefy - Stuns target\n",
    "\n",
    "Since this is every other word of the \"Deathly Hallows\" script, I think it's pretty useful.\"\"\"\n",
    "\n",
    "# Create a doc object out of the text string using the trained model\n",
    "doc = model_best(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e2888",
   "metadata": {},
   "source": [
    "Let's also try the model we created with an EntityRuler with all spell names hard written in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e12b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc object out of the text string using the EntityRuler model\n",
    "doc = entruler_nlp(test_text)\n",
    "\n",
    "# Find out the entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e49ed",
   "metadata": {},
   "source": [
    "It seems in this example our EntityRuler model performs better than our trained model. Why do we think that is?\n",
    "\n",
    "Part of the reason we aren't getting better results is something that Ines Montani describes in this Stack Overflow answer https://stackoverflow.com/questions/50580262/how-to-use-spacy-to-create-a-new-entity-and-learn-only-from-keyword-list/50603247#50603247\n",
    "\n",
    "\"The advantage of training the named entity recognizer to detect SPECIES in your text is that the model won't only be able to recognise your examples, but also generalise and recognise other species in context. If you only want to find a fixed set of terms and not more, a simpler, rule-based approach might work better for you.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01473f",
   "metadata": {},
   "source": [
    "# References\n",
    "McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
