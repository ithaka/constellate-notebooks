{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec6647b",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Hannah Jacobs](http://hannahlangstonjacobs.com/) for the [2021 Text Analysis Pedagogy Institute](https://nkelber.github.io/tapi2021/book/intro.html).\n",
    "\n",
    "Adapted by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d765b",
   "metadata": {},
   "source": [
    "# Creating an OCR Workflow (Post-Processing)\n",
    "\n",
    "These [notebooks](https://constellate.org/docs/key-terms/#jupyter-notebook) describe how to turn images and/or pdf documents into plain text using Tesseract [optical character recognition](https://constellate.org/docs/key-terms/#ocr). The goal of this notebook is to help users design a workflow for a research project.\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "* [Optical Character Recognition Basics](./ocr-basics.ipynb)\n",
    "* [Creating an OCR Workflow (Pre-Processing)](./ocr-workflow-1.ipynb)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "**Data Format:** \n",
    "* image files (.jpg, .png)\n",
    "* document files (.pdf)\n",
    "* plain text (.txt)\n",
    "\n",
    "**Libraries Used:**\n",
    "* [Tesseract](https://tesseract-ocr.github.io/) for performing [optical character recognition](https://constellate.org/docs/key-terms/#ocr).\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "1. Run OCR on a large batch of prepared images\n",
    "2. Assess the degree of accuracy achieved in performing OCR\n",
    "3. Identify post-processing strategies for improving OCR accuracy\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Digitize documents\n",
    "2. **Optical Character Recognition**\n",
    "3. Tokenize your texts\n",
    "4. Perform analysis\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b368de",
   "metadata": {},
   "source": [
    "## \"Cleaning\" OCR (Post-Processing) Overview\n",
    "\n",
    "**This part of the process is often best performed with a combination of manual (human) and automated (computer) steps.** This is where you may be addressing not only errors in the OCR itself but also issues with the original printing, as we describe below with regard to hyphenated words at the end of lines. As with pre-processing, how complex you make iterations in this phase depends on your corpus and your resources:\n",
    "\n",
    "1. **Review the OCR output.** Take an initial look at the OCR text files. Sometimes even just a glance will give you a sense of how well the process has gone. If you see a lot of errors, return to the pre-processing questions and consider which steps you might take to improve the OCR output.\n",
    "\n",
    "\n",
    "2. **Run a spellchecker & calculate the quality of the OCR output.** Use a spellchecker to get a sense of just how accurate the OCR process may have been. Note that spellchecking here, as with spellchecking in software such as Word, is really looking for known and unknown words.\n",
    "\n",
    "\n",
    "3. **Use Python to check for and correct possible recurring & unique spelling errors.** These are errors that appear frequently and may be caused by the typescript, hyphenation at the end of lines, or other patterns that Tesseract repeatedly misinterprets. This step should focus on common words and avoid proper nouns (unless you have a full list of proper nouns to draw from). As with any automated step, it's possible that new errors will be introduced here. If there is a known and small quantity of proper nouns used in individual texts or across the corpus, and these are consistently \"read\" incorrectly by Tesseract, it may be possible to use Python to correct these.\n",
    "\n",
    "\n",
    "4. If your corpus is small enough and/or you have a team that can help you, **read through the corpus to manually check for and correct unique errors**. This may be a moment to correct proper nouns. If you have a team, it may be advisable to have texts read and corrected by multiple team members. It will be important that these team members have access to both inputs and outputs, and perhaps even lists of proper nouns, to be able to compare the original scans with the computer-readable versions. You may even want to set up a process whereby reviewers can flag words they are not sure about so that another reviewer can provide their opinion so that you and/or another project manager making a final decision on uncertain words.\n",
    "\n",
    "\n",
    "The above process could be broken down further to address smaller issues incrementally and iteratively. It may also be useful to break your corpus into units of analysis before or during this process to assist with cleaning. Let's download a sample, OCR it, and investigate the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37ff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample PDF from On the Books\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# Sample file to download\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sample_01.pdf'\n",
    "\n",
    "# Check if a folder exists to hold pdfs. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "path_url = Path(url)\n",
    "urllib.request.urlretrieve(url, f'{data_folder.as_posix()}/{path_url.name}')\n",
    "    \n",
    "## Success message\n",
    "print('PDF downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09769ce5",
   "metadata": {},
   "source": [
    "Now let's break the PDF down into individual images using the same method from our last lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert a single PDF into a series of image files ###\n",
    "\n",
    "# Import pdf2image's convert_from_path module.\n",
    "from pdf2image import convert_from_path\n",
    "# Import pathlib's Path module.\n",
    "from pathlib import Path\n",
    "\n",
    "# Define where the images will be saved\n",
    "# Check if a folder exists to hold pdfs. If not, create it.\n",
    "input_folder = Path('./data/pdf_images/')\n",
    "input_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Get the PDF and convert to a group of PIL (Pillow) objects\n",
    "# This does NOT save the images as files.\n",
    "document_path = Path('./data/sample_01.pdf')\n",
    "PIL_objects = convert_from_path(document_path)\n",
    "\n",
    "# For each PIL image object:\n",
    "for page, image in enumerate(PIL_objects):\n",
    "\n",
    "    # Create a file name that includes the original file name, and\n",
    "    # a file number, as well as the file extension.\n",
    "    fileName = f'{input_folder.as_posix()}/image_{str(page)}.jpg'\n",
    "\n",
    "    # Save each PIL image object using the file name created above\n",
    "    # and declare the image's file format. (Try also PNG or TIFF.)\n",
    "    image.save(fileName, 'JPEG')\n",
    "\n",
    "# Success message\n",
    "print('PDF converted successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295713d",
   "metadata": {},
   "source": [
    "And finally, let's batch OCR all the pages, creating a single text file for each image file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd07d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert all the image files into text files ###\n",
    "import pytesseract\n",
    "\n",
    "# Import pdf2image's convert_from_path module.\n",
    "from pdf2image import convert_from_path\n",
    "# Import pathlib's Path module.\n",
    "from pathlib import Path\n",
    "#Import PIL's Image module.\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# For each .jpg file in the input folder, do the following:\n",
    "for img in input_folder.rglob('*.jpg'):\n",
    "    # Open the input file and complete OCR\n",
    "    with open(f'{img}', 'rb') as f_image:\n",
    "        file = Image.open(f_image)\n",
    "        ocrText = pytesseract.image_to_string(file)\n",
    "    \n",
    "    # Create (or overwrite!) the output file and append the text\n",
    "    with open(f'{input_folder}/{img.stem}.txt', 'w') as f_text:\n",
    "        f_text.write(ocrText)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9e2ce",
   "metadata": {},
   "source": [
    "# Post-Processing Step-by-Step\n",
    "\n",
    "## Review the OCR output.\n",
    "\n",
    "Open your output text files and begin your review. Make sure to compare them with the original page images. What do you notice?\n",
    "\n",
    "## Check for misspellings & quality.\n",
    "\n",
    "Although it appears that this page has been entirely correctly OCR'ed, there are two issues that show up in this text file that we want to address in all of our OCR'ed files:\n",
    "\n",
    "1. The original printers **broke words at the end of some lines**. For example, `Dis-trict` might be broken up across two lines. How do we deal with this without removing words that *should* be hyphenated?\n",
    "2. **How would we know how accurate this simple script might be when applied to the entire volume, or to the entire corpus?** \n",
    "\n",
    "In addition to being hyphenated, `Dis-trict` may be misspelled as `Dis-triet` or `Dis-trism` in our output—is this just one instance, or does this error recur? If it's recurring, we can use Python to fix it across the corpus. This could be more efficient than having to read the entire OCR'ed corpus. A good starting point is to get a sense of just how accurate the OCR process has been, that is **check its readability**, before we start trying to identify and fix spelling errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ef531",
   "metadata": {},
   "source": [
    "**In the following scripts, we'll look at how to correct misspelling and check for OCR accuracy by generating a readability score.** During this process, we'll remove the hyphens at the end of lines to help us with spellchecking, but we may find that we introduce new issues for the spellcheck.\n",
    "\n",
    "To begin, there are a number of modules and libraries we need to import (or reimport) to extend Python's functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1529a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install PySpellChecker ###\n",
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a0f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the word_tokenize module from the nltk (\"Natural Language Processing Kit\") library.\n",
    "# NLTK is a powerful toolset we can use to manipulate and analyze text data.\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt', download_dir='./data/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTesseract and PIL, an image processing library used by PyTesseract, to complete the OCR.\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Import os, a module for file management.\n",
    "import os\n",
    "\n",
    "# Import re, a module that we can use to search text.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Import the SpellChecker module, which we'll use to look for likely misspelled words.\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# We'll also need the pandas library, which is a powerful toolset for managing data.\n",
    "# We'll learn more about pandas in the exploratory analysis modules.\n",
    "import pandas as pd\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Modules & libraries imported. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d80b8",
   "metadata": {},
   "source": [
    "Now we'll set up variables that we'll use to give Python information and structure information that Python returns. These include the location of the original image files and the place we want to store our OCR'ed text, as well as a [spellcheck dictionary](https://pypi.org/project/pyspellchecker/), which we'll extend to include North Carolina placenames, and a dataframe (essentially, an empty table) we'll use to structure readability information along with the OCR'ed text.\n",
    "\n",
    "*Note: The [spellchecker library](https://pypi.org/project/pyspellchecker/) we are using supports a limited number of Western languages. English is the default.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we loop through each page, we'll augment our spellchecker \n",
    "# dictionary to include place names specific to North Carolina. \n",
    "# Our script for gathering these place names is available here: \n",
    "# https://github.com/UNC-Libraries-data/OnTheBooks/blob/master/examples/adjustment_recommendation/geonames.py\n",
    "\n",
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "# Add the place name words from the \"geonames.txt\" file to the \n",
    "# spellchecker dictionary.\n",
    "# Sample file to download\n",
    "url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/geonames.txt'\n",
    "\n",
    "# Download the file\n",
    "geonames_path = Path('./data/geonames.txt')\n",
    "urllib.request.urlretrieve(url, geonames_path)\n",
    "spell.word_frequency.load_text_file(geonames_path.as_posix())\n",
    "\n",
    "# This statement confirms that the above code was run without issue.\n",
    "print(\"Variables created. Ready for the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e5aaf",
   "metadata": {},
   "source": [
    "Here is what each column will hold:\n",
    "\n",
    "- **file_name**: The name for the corresponding image file. For now, this is the only information in the table that identifies where the rest of the information in each row comes from (which page).\n",
    "- **token_count**: The total number of tokens (words) found in each page.\n",
    "- **unknown_count**: The number of unknown (\"misspelled\") words found in each page.\n",
    "- **readability**: Think of this as the percentage of the page that was readable.\n",
    "- **unknown_words**: A list of tokens (words or in some cases characters) that were not listed in the spellchecker.\n",
    "- **text**: The OCR'ed text output from each page. The output here includes all <a href=\"https://en.wikipedia.org/wiki/Escape_character#JavaScript\" target=\"blank\">escape characters</a>, so it may look as if a lot of erronenous characters have been added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb9db3",
   "metadata": {},
   "source": [
    "Now we'll remove hyphens from the text, run the spellcheck script, and produce a dataframe (table) of information that will give us a sense of the accuracy of our OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionary Test a Folder of .txt Files ###\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "\n",
    "df = pd.DataFrame(columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# Set the folder for the input images\n",
    "texts_folder = Path('./data/pdf_images/')\n",
    "\n",
    "for txt_file in texts_folder.rglob('*.txt'):\n",
    "    \n",
    "    # Open each text file and read text into `ocrText`\n",
    "    with open(txt_file, 'r') as inputFile:\n",
    "        ocrText = inputFile.read()\n",
    "        \n",
    "    # Join hyphenated words that are split between lines by \n",
    "    # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "    # \"\\n\" is an \"escape character\" and represents the \n",
    "    # \"newline,\" a character that is usually invisible \n",
    "    # to human readers but that computers use to mark the \n",
    "    # end/beginning of a line. Each time you press the \n",
    "    # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "    # is created to mark the beginning of a new line.\n",
    "    ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "    \n",
    "    # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "    tokens = word_tokenize(ocrText)\n",
    "    \n",
    "    # Lowercase all tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Now we can get all of the words that don't match the \n",
    "    # spellchecker dictionary or our list of place names--\n",
    "    # these are the potential spelling errors.\n",
    "    unknown = spell.unknown(tokens)\n",
    "    \n",
    "    # Let's use a little math to find out how many potential \n",
    "    # spelling errors were identified. As part of this process, \n",
    "    # we'll create a \"readability\" score that will give us a \n",
    "    # percentage of how readable each file is--how much of the \n",
    "    # OCR'ed is \"correct.\"\n",
    "        \n",
    "    # If the list of unknown tokens (words) is greater than 0 \n",
    "    # (i.e. if the list is not empty):\n",
    "    if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "        readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "    else:\n",
    "        readability = 100\n",
    "    \n",
    "    # Let's create a record of the readability information \n",
    "    # for this page that we'll add to the dataframe. \n",
    "    # The following is a Python dictionary, another way of \n",
    "    # storing data. Each word or phrase to the left of the : is a\n",
    "    # \"key\" -- think of it as a column header. Each piece of \n",
    "    # information to the right is a \"value\" -- information \n",
    "    # written in a single cell below each header. \n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "            \"file_name\" : txt_file.as_posix(),\n",
    "            \"token_count\" : len(tokens),\n",
    "            \"unknown_count\" : len(unknown),\n",
    "            \"readability\" : readability,\n",
    "            \"unknown_words\" : [unknown],\n",
    "            \"text\" : ocrText\n",
    "            })\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "    # This statement lets us know if a page has been successfully \n",
    "    # checked for readability.\n",
    "    print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv(f'{texts_folder}/spellcheck_data.csv', header=True, index=False, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df\n",
    "\n",
    "# Delete the df variable in case we wish to run this script again\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d7ebd",
   "metadata": {},
   "source": [
    "# Correcting Errors\n",
    "\n",
    "Broadly speaking, we can break down errors into two categories: **unique** or **recurring**. We can use Python to address both types to an extent, but it's likely that some manual review will still need to be done to ensure the highest quality OCR. Whether and how much manual review can be done will depend on the project's resources.\n",
    "\n",
    "## Unique Errors\n",
    "\n",
    "There are at least **two ways to address unique computer-identified errors:**\n",
    "\n",
    "1. Since we produced a list of unknown words in our readability test, we could simply open each file in a text editor and use find-and-replace functionalities (Command + F or Control + F) to locate and replace instances of unique errors.\n",
    "\n",
    "2. We could use a little Python to find and replace these errors across the corpus. \n",
    "\n",
    "*Caveat: There may be instances where variant spellings are identified as \"unknown\" (misspelled) but are true representations of the word as it was originally printed. It may be necessary to check these misspellings against the scanned pages and decide whether or not to correct the text in the OCR output.*\n",
    "\n",
    "The following script runs through the entire sample output (and could be applied to an entire corpus) and checks for and replaces instances of a unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4044de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "filePath = glob.glob(\"./data/pdf_images/*.txt\")\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in filePath:\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    text = text.read()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word.\n",
    "    \n",
    "    unknown_word = \"shali\"\n",
    "    \n",
    "    known_word = \"shall\"\n",
    "    \n",
    "    word_correction = text.replace(unknown_word, known_word)\n",
    "    \n",
    "    # Close the file.\n",
    "    #file.close()\n",
    "    \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(file, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()\n",
    "\n",
    "print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd96d3",
   "metadata": {},
   "source": [
    "Check the output files for the unknown word to see if the word is still present. \n",
    "\n",
    "We've done this for one word at a time, but we could use the list of unknown words generated to create a script that runs through the list and corrects each instance all at once--rather than running the above script for each correction individually.\n",
    "\n",
    "## Recurring Errors & Changes\n",
    "\n",
    "There are several kinds of recurring errors:\n",
    "\n",
    "- Specific Words & Phrases (if a unique mispelling above is present consistently across the corpus, for example).\n",
    "- Word, Phrase, or Character Patterns (for example, a hyphen used to break up a word at the end of a line).\n",
    "\n",
    "We looked earlier at how to remove hyphens at the end of lines. To do this we replaced `-\\n` with nothing (\"\"). We saved that change to spellcheck csv, but we could have written that to the original text files. We could use the above script to make that change directly in the original output files, though it may be advisable to *keep the original text output files separate from the corrected versions in case you need to refer back.*\n",
    "\n",
    "We could also use the below script in combination with [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to correct issues that we know are recurring.\n",
    "\n",
    "**Be careful when attempting changes with regular expressions**--these always come with the risk of introducing new errors. To avoid as many as possible, make your regular expression as specific as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad379fc9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the regular expressions module (re), \n",
    "# which helps us use regex in Python.\n",
    "import re\n",
    "\n",
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "\n",
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "filePath = glob.glob(\"./data/pdf_images/*.txt\")\n",
    "\n",
    "# Save the pattern for a chapter header (even pages) that we \n",
    "# want to search each page for. We've added \"^\" to our regular \n",
    "# expressions to be extra sure that Python searches only at the \n",
    "# beginning of each file.\n",
    "regex_search = re.compile(\"\\n\\nThe General Assembly.*?t:\\n\\n\")\n",
    "\n",
    "# Save the text that we want to use to correct the OCR output.\n",
    "replacement = \"\\n\\nThe General Assembly of North Carolina do enact:\\n\\n\"\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in filePath:\n",
    "    \n",
    "    # Create a file name for a new output file.\n",
    "    \n",
    "    # First, get the existing file name \n",
    "    # (e.g. \"sessionlawsresol1955nort_0066.txt\")\n",
    "    # & create a new name for the output file\n",
    "    outFileName = file.replace(\".txt\", \"_corrected.txt\")\n",
    "\n",
    "    # Create and open a new \"outFile\" to save our results to.\n",
    "    # \"w\" tells Python that we plan to write to this file.\n",
    "    outFile = open(outFileName, \"w\")\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    inFile = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    inFile = inFile.read()\n",
    "    \n",
    "    # Search inFile for the the regular expression.\n",
    "    if re.search(regex_search, inFile):\n",
    "        \n",
    "        # If the regex search is found,\n",
    "        # print a statement to let us know that there is a match.\n",
    "        print(outFileName, \"Match found.\")\n",
    "        \n",
    "        # Substitute the regex_search for the replacement phrase\n",
    "        # and write the updated contents of inFile to outFile.\n",
    "        outFile.write(re.sub(regex_search, replacement, inFile))\n",
    "    \n",
    "    # If neither the regex search is not found,\n",
    "    else:\n",
    "        \n",
    "        # print a statement to let us know that no matches were found.\n",
    "        print(outFileName, \"No match found.\")\n",
    "        \n",
    "        # And write all of the contents from the inFile to the outFile.\n",
    "        outFile.write(inFile)\n",
    "    \n",
    "    # Close the current outFile and move to the next file.\n",
    "    outFile.close()\n",
    "    \n",
    "# The loop will finish when Python has gone through all files in \n",
    "# the sample_output folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bbe58",
   "metadata": {},
   "source": [
    "# Concatenate all the text files into one\n",
    "If we are happy with our outputs, then we can stitch all the text files into a single text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec84ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the folder for the input texts\n",
    "texts_folder = Path('./data/pdf_images/')\n",
    "\n",
    "# Set output filename and create file\n",
    "full_text = Path('./data/full.txt')\n",
    "full_text.touch()\n",
    "\n",
    "for txt in texts_folder.rglob('*corrected.txt'):\n",
    "    with open(txt, 'r') as f_in:\n",
    "        fileText = f_in.read()\n",
    "        with open(full_text, 'a') as f_out:\n",
    "            f_out.write(fileText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908b427",
   "metadata": {},
   "source": [
    "# Try it out!\n",
    "\n",
    "Here's [the first 50 pages of an edition of Moby Dick from 1922](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/moby_dick.pdf). Can you OCR all the pages and then generate a list of errors based on dictionary analysis? How about replacing some of the text with errors?\n",
    "\n",
    "`https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/moby_dick.pdf`\n",
    "\n",
    "You'll need to start by either using `urllib.request` to download the materials to the Constellate environment or by downloading the document to your local machine and uploading it to Constellate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
