{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8955af",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "**This notebook does not contain code but links to notebooks with code.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbca32",
   "metadata": {},
   "source": [
    "# Why Learn Text Analysis?: A Guide for Absolute Beginners\n",
    "\n",
    "\n",
    "**Description:** This lesson introduces key concepts in text analysis for a general academic audience. If you are completely new to text analysis, this is the place to start.\n",
    "\n",
    "**Use Case:** For learners who wonder if they should learn text analysis\n",
    "\n",
    "**Difficulty:** Beginner with no coding experience\n",
    "\n",
    "**Completion time:** 45 minutes\n",
    "\n",
    "**Knowledge Required:** None\n",
    "\n",
    "**Knowledge Recommended:** None\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** None\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0310db",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is a high-level overview of common text analysis methods used in academic research. In particular, this notebook describes:\n",
    "\n",
    "* Common text analysis methods useful for academic research\n",
    "* The kind of research questions each method can answer\n",
    "* How difficult each method is to learn\n",
    "* Constellate events and open educational resources\n",
    "* Additional learning resources\n",
    "\n",
    "Text analysis has emerged from many research areas (computer science, linguistics, information science, social sciences, and computational humanities, to name a few) to help address the over-abundance of data. When a single scholar—or even a team of dozens of scholars—is confronted with millions of documents, it becomes impossible to read all the available evidence. This is true whether the documents are full-length monographs or tweets. The challenge of informational synthesis becomes more difficult as the research record grows larger with each passing day.\n",
    "\n",
    "Text analysis is part of a generational shift in the toolkit of 21st-century researchers. Textual research methods play an important role in helping address our post-modern data deluge. No matter the field, all academics are increasingly confronted by data overload. From students to academic leaders, the \"data problem\" has taken center stage: ChatGPT, Large Language Models, Machine Learning, Big Data, Neural Networks. Whether academics use these technologies for their research or not, they increasingly underpin the systems we use for knowledge-making. \n",
    "\n",
    "Thriving in this universe of information requires learning digital literacies, ways of seeing the size, scope, and shape of human thought. Text analysis then, is not just a field or set of methods, but a growing body of textual literacies that help us connect, discover, and interpret humanity's knowledge at a superhuman scale. Like a researcher operating a telescope, knowing the right methods can make the difference between a shot in the dark and a glimpse into the richness of newfound galaxies and constellations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61534746",
   "metadata": {},
   "source": [
    "## Text Analysis Data\n",
    "\n",
    "Text analysis creates insights from a large number of texts (also known as a corpus). Some texts may be ready for analysis and others may require thousands of hours for preparation. All research has time and funding restraints, so understanding the difference is important for determining whether a project is feasible.\n",
    "\n",
    "Research projects that start with printed documents can be prohibitively expensive in time and labor. Assuming the printed documents can be scanned accurately into a suitable image (`.jpg`, `.tiff`, etc.) or portable document format (`.pdf`), the researcher will then need to convert the file using a process known as Optical Character Recognition (OCR). The process can be semi-automated, but high-quality, accurate results at scale almost always require significant human labor.\n",
    "\n",
    "Since it is easier to start with \"born-digital\" materials, many researchers gather data from:\n",
    "* Pre-existing datasets\n",
    "* Scraping from websites\n",
    "* Application Programming Interfaces (APIs)\n",
    "If the data has a consistent structure, the relevant text can probably be extracted at-scale with some Python literacy. \n",
    "\n",
    "In most cases, the ideal scenario is having texts in an easily readable format, such as a plaintext file, e.g. a `.txt` file extension. Document formats such as Extensible Markup Langauge `.xml` or JavaScript Object Notation `.json` can be even richer sources of data, especially if they contain relevant metadata such as titles, authors, publishers, dates, etc.\n",
    "\n",
    "Finally, since text analysis methods rely on statistical models, it is generally true that having more texts will improve the outcomes of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d20117",
   "metadata": {},
   "source": [
    "### What about copyright law?\n",
    "\n",
    "Text analysis on copyrighted materials poses an additional challenge. It is not possible to secure the copyrights for millions of documents. There are two common approaches to this problem:\n",
    "\n",
    "1. Analyze the data within a **secure computing environment**\n",
    "2. Use a **non-consumptive dataset**\n",
    "\n",
    "For example, the HathiTrust Research Center (HTRC) offers two approaches. Researchers who need full-text access, can use a secure computing environment known as an HTRC [data capsule](https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+Capsule+Environment). The primary benefit is that researchers can work with full-text data directly in the environment. The capsule monitors data that is exported, ensuring that copyright is observed.\n",
    "\n",
    "There are some significant downsides to this secure computing approach. Most notably, researchers must bring their tools into the environment, and they are limited by the technical specifications of the secure compute environment.\n",
    "\n",
    "To address these issues, HTRC also offers a non-consumptive dataset called [\"Extracted Features\"](https://analytics.hathitrust.org/datasets). Users can download the non-consumptive dataset to their machine (~4 TB in size). The dataset does not contain full-text (ensuring copyright is observed), but it does contain n-gram counts which enables a significant amount of useful text analysis.\n",
    "\n",
    "The downside to the non-consumptive dataset approach is that some text analysis methods, such as Named Entity Recognition (NER), require full-text data for the most accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e42201",
   "metadata": {},
   "source": [
    "### Constellate Data\n",
    "\n",
    "Constellate's dataset builder offers full-text when legally permissible and non-consumptive data for copyrighted materials. In the cases where Constellate cannot supply full-text due to copyright laws (JSTOR and Portico content), the datasets include three n-gram counts for each document:\n",
    "\n",
    "* Unigrams- A single-word construction, for example: \"vegetable\".\n",
    "* Bigrams- An two-word construction, for example: \"vegetable stock\".\n",
    "* Trigrams- A three-word construction, for example: \"homemade vegetable stock\".\n",
    "\n",
    "While having the full texts for the documents in your corpus is ideal, a great deal can be still be discovered through the use of unigrams. Even when researchers have access to the full-texts of a corpus, it is common for them to create a list of n-gram counts for analysis. \n",
    "\n",
    "As a non-profit, Constellate gives the maximum access (as allowed by law) to its textual data without charge. We offer additional services for a modest fee in the form of educational classes, community events, and a specialized lab designed for teaching, learning, and research. We do not resell content to libraries that have purchased licenses for those materials. In fact, we offer access to all of our available materials for text analysis—whether or not the institution has licensed them. \n",
    "\n",
    "Constellate does not offer a secure compute environment for working with full-text documents. Researchers tell us they have little interest in being forced to conduct research within a secure compute environment. Instead, we offer full-text datasets for researchers willing to sign a legal agreement through [Data For Research](https://jstor.libwizard.com/f/dfr-request). \n",
    "\n",
    "___\n",
    "<font color=\"red\">Read more</font>\n",
    "* Constellate Dataset Builder: full-text and n-gram content\n",
    "* Bring your own data into Constellate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cea84c",
   "metadata": {},
   "source": [
    "### I have my own data. What will it take to get it ready?\n",
    "\n",
    "For a major text analysis project, such as UNC Chapel Hill's [On the Books: Jim Crow and Algorithms of Resistance](https://onthebooks.lib.unc.edu/), about 90% of the labor is creating the corpus. One of the most significant benefits of using Constellate to teach, learn, and do research is that the dataset builder takes out *the vast majority* of effort in doing text analysis. Of course, we recognize researchers' scholarly interests may require building their own dataset, and we support researchers by offering educational classes and events on building a dataset. Of course, we also offer the Constellate Lab for conducting and sharing research.\n",
    "\n",
    "If you have your own data, you will need to assess what it will take to make it ready for analysis. Constellate offers classes on preparing your own dataset based on our open educational resources. We can help you answer the following questions:\n",
    "\n",
    "* How can I convert my data into plain text? \n",
    "    * <font color=\"red\">Start learning</font> [Optical Character Recognition Basics](../OCR/ocr-basics.ipynb)\n",
    "* How can I tokenize my texts (separate the individual words)? \n",
    "    * <font color=\"red\">Start learning</font> [Tokenize Text Files](../Tokenization/tokenizing-text-files.ipynb)\n",
    "    * <font color=\"red\">Start learning</font> [Tokenize Text Files with NLTK](../Tokenization/tokenize-text-files-with-nltk.ipynb)\n",
    "    \n",
    "Consider the data's current form as well as the size and skill of your project staff. The corpus creation process could take anywhere from a few hours to many years of labor. If there is a significant amount of labor, you may need to write a grant proposal to hire help. *If writing a grant, contact your library early in the process since funding agencies will require a data preservation plan in the grant application, which will likely include committing your dataset to your institutional repository.*\n",
    "\n",
    "In addition to the cleaned-up texts for your corpus, you will also need a strategy for dealing with textual metadata, information such as author, year, etc. Some of this is discussed in [Tokenize Text Files with NLTK](../Tokenization/tokenize-text-files-with-nltk.ipynb), but it would also help to have some experience with working with data at scale. The Python [Pandas](https://pandas.pydata.org/) Library is one of the best ways to work with data at scale. (Compared to Excel, Pandas is faster, more flexible, works at larger scales, and can be automated.) Constellate offers classes for those interested in learning Pandas.\n",
    "___\n",
    "\n",
    "<font color=\"red\">Start learning</font>\n",
    "* [Pandas basics 1](../Pandas-basics/pandas-basics-1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22d50f",
   "metadata": {},
   "source": [
    "## Common Research Questions\n",
    "\n",
    "Here are a few common research questions that text analysis can help answer:\n",
    "\n",
    "1. What are these texts about?\n",
    "2. What emotions are expressed?\n",
    "3. What key names can I find?\n",
    "4. Which texts are similar?\n",
    "\n",
    "Let's consider the methods to answer each of these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f472c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### What are these texts about?\n",
    "\n",
    "The most common problem for researchers is trying to sort through a large pile of data: \"What are these texts about?\" There are a variety of approaches for answering this question, varying from basic word frequency accounts to machine learning methods such as topic modeling and topic classification. The following approaches try to answer:\n",
    "\n",
    "**What are the words, topics, concepts, and significant terms in these documents?**\n",
    "___\n",
    "\n",
    "**Word Frequency** (Beginner Friendly)\n",
    "\n",
    "Researchers often begin to explore a corpus by counting the frequency of each word in each document. Almost all text analysis tools feature word frequency analysis, often including visualizations such as a word cloud. When exploring a corpus using word frequencies, it is often helpful to make a distinction between content words(generally nouns and verbs) and function words (grammatical word constructions like \"the\", \"of\", and \"or\"). Researchers may refine a stop words list to improve their data output.\n",
    "\n",
    "<font color=\"red\">Start learning</font> \n",
    "* [Word Frequency Analysis](../Exploring-word-frequencies/exploring-word-frequencies.ipynb) Create a word cloud\n",
    "* [Creating a Stop Words List](../Stopwords/creating-stopwords-list.ipynb) Improve your research results\n",
    "___\n",
    "\n",
    "**Significant Terms** (Beginner Friendly)\n",
    "\n",
    "Search engines use significant terms analysis to match a user query with a list of appropriate documents. This method could be useful if you want to search your corpus for the most significant texts based on a word (or set of words). It can also be useful in reverse. For a given document, you could create a list of the ten most significant terms. This can be useful for summarizing the content of a document. \n",
    "\n",
    "<font color=\"red\">Start learning</font> \n",
    "* [Significant Terms Analysis](../Significant-terms/finding-significant-terms.ipynb) Create a simple search engine\n",
    "___\n",
    "**Topic Analysis** or Topic Modeling (Intermediate)\n",
    "\n",
    "While significant terms analysis reveals terms commonly found in a given document, a topic analysis can tell us what words tend to cluster together across a corpus. For example, if we were to study newspapers, we would expect that certain words would cluster together into topics that match the sections of the newspaper. We might see something like:\n",
    "\n",
    "* Topic 1: baseball, ball, player, trade, score, win, defeat\n",
    "* Topic 2: market, dow, bull, trade, run, fund, stock\n",
    "* Topic 3: campaign, democratic, polls, red, vote, defeat, state\n",
    "\n",
    "We can recognize that these words tend to cluster together within newspaper sections such as \"Sports\", \"Finance\", and \"Politics\". If we have never read a set of documents, we might use a topic analysis to get a sense of what topics are in a given corpus. Given that Topic Analysis is an exploratory technique, it may require some expertise to fine-tune and get good results for a given corpus. However, if the topics can be discovered then they could potentially be used to train a model using machine learning to categorize the topics in a given document automatically using Topic Classification.\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* Keli Du's \"A Survey on LDA Topic Modeling in Digital Humanities\"\n",
    "\n",
    "<font color=\"red\">Start learning</font> \n",
    "* [Topic Analysis](../Significant-terms/finding-significant-terms.ipynb) Create an interactive topic visualization\n",
    "___\n",
    "**Concordance** (Beginner Friendly)\n",
    "\n",
    "The concordance has a long history in the computational humanities and Roberto Busa's concordance *Index Thomisticus*—started in 1946—is arguably the first digital humanities project. Before computers were common, they were printed in large volumes such as John Bartlett's 1982 reference book *A Complete Concordance to Shakespeare*—it was 1909 pages pages long! A concordance gives the context of a given word or phrase in a body of texts. For example, a literary scholar might ask: how often and in what context does Shakespeare use the phrase \"honest Iago\" in Othello? A historian or sociologist may examine social media data to discover the contextual use of dog whistles, while a medical researcher may use concordance to identify journal articles that mention \"remdesivir\" in the context of \"Covid-19\" for a systematic review.\n",
    "\n",
    "<font color=\"red\">Start learning</font> \n",
    "* [Concordance](../Concordance-and-collocation/concordance.ipynb) View context windows with journal data and visualize terms with a lexical dispersion plot\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "\n",
    "* Steven E. Jones [Roberto Busa, S.J., and the Emergence of Humanities Computing](https://www.routledge.com/Roberto-Busa-S-J-and-the-Emergence-of-Humanities-Computing-The-Priest/Jones/p/book/9781138587250) (2016)\n",
    "* Julianne Nyhan and Marco Passarotti, eds. [One Origin of Digital Humanities: Fr Roberto Busa in His Own Words](https://link.springer.com/book/10.1007/978-3-030-18313-4) (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f8e76",
   "metadata": {},
   "source": [
    "### What emotions are expressed?\n",
    "\n",
    "**Sentiment Analysis** (Intermediate)\n",
    "\n",
    "Sentiment analysis can help determine the emotions expressed in a given text. This can be determined using grammar-based algorithms, Machine Learning, or both. This is important for many business use-cases such as market research, consumer sentiment, and recommender systems.\n",
    "\n",
    "<font color=\"red\">Start learning</font> \n",
    "* [Sentiment Analysis with VADER](../Significant-terms/finding-significant-terms.ipynb) Classify positive and negative product reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cd4dd",
   "metadata": {},
   "source": [
    "### What key names can I find?\n",
    "\n",
    "**Named Entity Recognition** or NER (Intermediate)\n",
    "\n",
    "Named Entity Recognition (NER) automatically identifies entities within a text and can helpful for extracting certain kinds of entities such as proper nouns. For example, NER could identify names of organizations, people, and places. It might also help identify things like dates, times, or dollar amounts. Like sentiment analysis, NER relies on grammar rules and/or machine learning.\n",
    "\n",
    "NER is very prominent in molecular biology and bioinformatics, particularly for identifying genes and gene products. NER can help summarize and/or extract data by identifying key phrases in relevant documents. Some examples include tweets, resumes, novels, journal articles, oral histories, and reviews.\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* Miguel Won, Patricia Murrieta-Flores, and Bruno Martins [Ensemble Named Entity Recognition (NER): Evaluating NER Tools in the Identification of Place Names in Historical Corpora](https://www.frontiersin.org/articles/10.3389/fdigh.2018.00002/full) (2018)\n",
    "\n",
    "<font color=\"red\">Start learning</font>\n",
    "* [Named Entity Recognition](../NER/ner-1.ipynb)\n",
    "* [Multilingual NER](https://github.com/wjbmattingly/tap-2022-multilingual-ner) by William Mattingly (2022 TAP Institute)\n",
    "* [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html) by Melanie Walsh (2021 TAP Institute)\n",
    "* [Named Entity Recognition](https://nkelber.github.io/tapi2021/book/courses/ner.html) by Zoe LeBlanc (2021 TAP Institute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c255102",
   "metadata": {},
   "source": [
    "### Which texts are similar?\n",
    "\n",
    "**Stylometrics and Authorship Attribution** (Intermediate to Advanced)\n",
    "\n",
    "The digital humanities, and its precursor \"humanities computing,\" have a long history in the analysis of literature, particularly for analyzing genre and authorship. For example, the New Oxford Shakespeare surprised many scholars by assigning significant authorship of Shakespeare's \"Henry VI,\" Parts 1, 2, and 3. It also lists as co-authors many Shakespeare contemporaries such as Thomas Nashe, George, Peele, Thomas Heywood, Ben Jonson, George Wilkins, Thomas Middleton, and John Fletcher.\n",
    "\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* Patrick Juola [How a Computer Program Helped Show J.K. Rowling Wrote A Cuckoo's Calling](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/) (2013)\n",
    "* Ros Barber [Big data or not enough? Zeta test reliability and the attribution of Henry VI](https://academic.oup.com/dsh/article-abstract/36/3/542/5918973?redirectedFrom=fulltext)\n",
    "\n",
    "___\n",
    "\n",
    "**Supervised Machine Learning** (Intermediate to Advanced)\n",
    "\n",
    "Supervised machine learning is an excellent choice for classification problems, identifying whether a text belongs to one group or another. These methods \"train\" computers to identify and classify similar items based on data that has been labeled or tagged by experts. For example, **On the Books: Jim Crow and Algorithms of Resistance** was able to use machine learning to identify 1939 North Carolina Jim Crow laws enacted between Reconstruction and the Civil Rights Movement. \n",
    "\n",
    "<font color=\"red\">Start learning</font>\n",
    "* [Introduction to Machine Learning](https://github.com/wjbmattingly/intro-to-ml) by William Mattingly (2022 TAP Institute)\n",
    "* [Introduction to Machine Learning](https://github.com/Grantglass/intro_to_ml) by Grant Glass (2022 TAP Institute)\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* [Project Outcomes for On the Books](https://onthebooks.lib.unc.edu/otb-research/project-outcomes/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913762",
   "metadata": {},
   "source": [
    "## More learning materials\n",
    "\n",
    "### Digital Humanities Resources\n",
    "\n",
    "* [TAP Institute Open Course Materials](https://labs.jstor.org/projects/text-analysis-pedagogy-institute-2/)\n",
    "* [Introduction to Python for Humanists](https://www.routledge.com/Introduction-to-Python-for-Humanists/Mattingly/p/book/9781032378374) by [William Mattingly](https://wjbmattingly.com/)\n",
    "* [Programming Historian](https://programminghistorian.org/en/lessons/) by various authors\n",
    "* [The Carpentries](https://carpentries.org/workshops-curricula/) by various authors\n",
    "* [Computational Humanities Research](https://discourse.computational-humanities-research.org/) <br />\n",
    "* [YaleDHLab Lab Workshops](https://github.com/YaleDHLab/lab-workshops) <br />\n",
    "* [Jupyter notebooks for digital humanities](https://github.com/quinnanya/dh-jupyter/blob/master/README.md) curated by [Quinn Dombrowski](https://quinndombrowski.com/)\n",
    "* [Data Sitter's Club](https://datasittersclub.github.io/site/) by various authors\n",
    "* [HathiTrust Digital Library Collections and Tools](https://www.hathitrust.org/htrc_collections_tools)\n",
    "* [Documenting the Now](https://github.com/DocNow)\n",
    "  \n",
    "**Books on Python, Text Analysis, and DH**\n",
    "* *Automate the Boring Stuff with Python: Practical Programming for Total Beginners* (2019) by Al Sweigart\n",
    "* *Python Crash Course: A Handson, project-based introduction to programming* (2019) by Eric Matthes\n",
    "* *Machine Learning with Python Cookbook* (2018) by Chris Albon\n",
    "* *Natural Langauge Processing in Action* (2019)by Hobson Lane, Cole Howard, and Hannes Max Hapke\n",
    "* [*Humanities Data Analysis: Case Studies with Python*](https://www.humanitiesdataanalysis.org/) by Folgert Karsdorp, Mike Kestemont, and Allen Riddell\n",
    "* [Technical Textbooks List](https://cmu-lib.github.io/dhlg/global-resources/educational-resources/textbooks/) by [Scott B. Weingart](https://scottbot.github.io/)\n",
    "\n",
    "**Books on Data Ethics**\n",
    "* *Algorithms of Oppression* (2018) by Safiya Noble\n",
    "* *Race After Technology* (2019) by Ruha Benjamin\n",
    "* *Data Feminism* (2020) by Catherine D'Ignazio and Lauren F. Klein\n",
    "\n",
    "**Instructional Video**\n",
    "* [DH, Coding, and Book History](https://www.youtube.com/user/pvierth/videos)\n",
    "* [Python Tutorials for Digital Humanities](https://www.youtube.com/@python-programming)\n",
    "\n",
    "**Course Examples**\n",
    "* [Humanities Analytics](https://humanitiesanalytics.com/) by [Matt Lavin](https://matthew-lavin.com/)\n",
    "* [Introduction to Cultural Analytics and Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) by [Melanie Walsh](https://melaniewalsh.org/)\n",
    "* [CodeLab](https://github.com/ZoeLeBlanc/CodeLab) by [Shane Lin](https://www.library.virginia.edu/staff/ssl2ab), [Zoe LeBlanc](https://zoeleblanc.com/), and [Brandon Walsh](https://scholarslab.lib.virginia.edu/people/brandon-walsh/)\n",
    "* [Computational and Inferential Thinking: The Foundations of Data Science](https://inferentialthinking.com/chapters/intro.html) by Ani Adhikari, John DeNero, David Wagner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
