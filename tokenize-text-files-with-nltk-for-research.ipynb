{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____\n",
    "# Tokenize Text Files with NLTK\n",
    "\n",
    "**Description:**\n",
    "This notebook takes as input:\n",
    "\n",
    "* Plain text files (.txt) in a zipped folder called 'texts' in the data folder\n",
    "* Metadata CSV file called 'metadata.csv' in the data folder (optional)\n",
    "\n",
    "and outputs a single JSON-L file containing the unigrams, bigrams, trigrams, full-text, and metadata. \n",
    "\n",
    "**Use Case:** For Researchers (Mostly code without explanation, not ideal for learners)\n",
    "\n",
    "**Difficulty:** Advanced\n",
    "\n",
    "**Completion time:** 10-15 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* [Working with Dataset Files](./working-with-dataset-files.ipynb)\n",
    "\n",
    "**Data Format:** .txt, .csv, .jsonl\n",
    "\n",
    "**Libraries Used:**\n",
    "* os\n",
    "* json\n",
    "* NLTK\n",
    "* gzip\n",
    "* nltk.corpus\n",
    "* collections\n",
    "* pandas\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Scan documents\n",
    "2. OCR files\n",
    "3. Clean up texts\n",
    "4. **Tokenize text files** (this notebook)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os, nltk, json, gzip, pandas as pd\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Special Functions for this Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Various functions written for this notebook ###\n",
    "\n",
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into bigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    \"\"\"Converts NLTK tuples into trigram strings\"\"\"\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    \"\"\"Converts a Counter of n-grams into a dictionary\"\"\"\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams\n",
    "\n",
    "def update_metadata_from_csv():\n",
    "    \"\"\"Uses pandas to grab additional metadata fields from a CSV file then adds them to the JSON-L file.\n",
    "    Unused fields can be commented out.\"\"\"\n",
    "    title = df.loc[identifier, 'title']\n",
    "    isPartOf = df.loc[identifier, 'isPartOf']\n",
    "    publicationYear = str(df.loc[identifier, 'publicationYear'])\n",
    "    doi = df.loc[identifier, 'doi']\n",
    "    docType = df.loc[identifier, 'docType']\n",
    "    provider = df.loc[identifier, 'provider']\n",
    "    datePublished = df.loc[identifier, 'datePublished']\n",
    "    issueNumber = str(df.loc[identifier, 'issueNumber'])\n",
    "    volumeNumber = str(df.loc[identifier, 'volumeNumber'])\n",
    "    url = df.loc[identifier, 'url']\n",
    "    creator = df.loc[identifier, 'creator']\n",
    "    publisher = df.loc[identifier, 'publisher']\n",
    "    language = df.loc[identifier, 'language']\n",
    "    pageStart = df.loc[identifier, 'pageStart']\n",
    "    pageEnd = df.loc[identifier, 'pageEnd']\n",
    "    placeOfPublication = df.loc[identifier, 'placeOfPublication']\n",
    "    pageCount = str(df.loc[identifier, 'pageCount'])\n",
    "\n",
    "    data.update([   \n",
    "        ('title', title),\n",
    "        ('isPartOf', isPartOf),\n",
    "        ('publicationYear', publicationYear),\n",
    "        ('doi', doi),\n",
    "        ('docType', docType),\n",
    "        ('provider', provider),\n",
    "        ('datePublished', datePublished),\n",
    "        ('issueNumber', issueNumber),\n",
    "        ('volumeNumber', volumeNumber),\n",
    "        ('url', url),\n",
    "        ('creator', creator),\n",
    "        ('publisher', publisher),\n",
    "        ('language', language),\n",
    "        ('pageStart', pageStart),\n",
    "        ('pageEnd', pageEnd),\n",
    "        ('placeOfPublication', placeOfPublication),\n",
    "        ('pageCount', pageCount),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip Texts Folder (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract Zip File of Texts ###\n",
    "# The text file should extract into a folder\n",
    "# called 'texts'\n",
    "\n",
    "filename = './data/texts.zip'\n",
    "\n",
    "try:\n",
    "    corpus_zip = zipfile. ZipFile(filename)\n",
    "    corpus_zip.extractall('./data/')\n",
    "    corpus_zip.close()\n",
    "    print('Zip file extracted successfully.')\n",
    "except:\n",
    "    print('No zip file detected. Upload your zip file to the data folder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Metadata CSV (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for a metadata CSV file ###\n",
    "\n",
    "csv_filename = 'metadata.csv'\n",
    "\n",
    "if os.path.exists(f'./data/{csv_filename}'):\n",
    "    csv_exists = True\n",
    "    print('Metadata CSV found.')\n",
    "else: \n",
    "    csv_exists = False\n",
    "    print('No metadata CSV found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Text Files into NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Establish root folder holding all text files ###\n",
    "# Create corpus using all text files in corpus_root\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = './data/texts'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print all File IDs in corpus based on text file names ###\n",
    "text_list = corpus.fileids()\n",
    "print(f'Corpus created from: {text_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Output Data to JSON-L File\n",
    "**If an old json-l file already exists, this process will overwrite it**\n",
    "\n",
    "For each text, this code will:\n",
    "1. Gather unigrams, bigrams, trigrams, and full text\n",
    "2. Compute word counts\n",
    "3. Check for additional metadata in a CSV file\n",
    "4. Write any data to JSON-L file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Create the JSON-L file and gzip it ###\n",
    "\n",
    "# For every text: \n",
    "# 1. Compute unigrams, bigrams, trigrams, and wordCount\n",
    "# 2. Append the data to a JSON-L file\n",
    "# After all data is written, compress the dataset using gzip\n",
    "## **If the JSONL file exists, it will be overwritten**\n",
    "\n",
    "# Define the file output name\n",
    "output_filename = 'my_data.jsonl'\n",
    "\n",
    "# Delete output files if they already exist\n",
    "if os.path.exists(f'./data/{output_filename}'):\n",
    "    os.remove(f'./data/{output_filename}')\n",
    "    print(f'Overwriting old version of {output_filename}')\n",
    "\n",
    "if os.path.exists(f'./data/{output_filename}.gz'):\n",
    "    os.remove(f'./data/{output_filename}.gz')\n",
    "    print(f'Overwriting old version of {output_filename}.gz\\n')\n",
    "                  \n",
    "\n",
    "for text in text_list:\n",
    "    \n",
    "    # Create identifier from filename\n",
    "    identifier = text[:-4]\n",
    "    \n",
    "    # Compute unigrams\n",
    "    unigrams = corpus.words(text)\n",
    "    unigramCount = convert_strings_to_counts(unigrams)\n",
    "    \n",
    "    # Compute bigrams\n",
    "    tuple_bigrams = list(nltk.bigrams(unigrams))\n",
    "    string_bigrams = convert_tuple_bigrams(tuple_bigrams)\n",
    "    bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "    \n",
    "    # Compute trigrams\n",
    "    tuple_trigrams = list(nltk.trigrams(unigrams))\n",
    "    string_trigrams = convert_tuple_trigrams(tuple_trigrams)\n",
    "    trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "    \n",
    "    # Compute fulltext\n",
    "    with open(f'./data/texts/{text}', 'r') as file:\n",
    "        fullText = file.read()\n",
    "    \n",
    "    # Calculate wordCount\n",
    "    wordCount = 0\n",
    "    for counts in unigramCount.values():\n",
    "        wordCount = wordCount + counts\n",
    "  \n",
    "    # Create a dictionary `data` to hold each document's data\n",
    "    # Including id, wordCount, outputFormat, unigramCount,\n",
    "    # bigramCount, trigramCount, fullText, etc.\n",
    "    data = {}\n",
    "    \n",
    "    data.update([\n",
    "        ('id', identifier),\n",
    "        ('outputFormat', ['unigram', 'bigram', 'trigram', 'fullText']),\n",
    "        ('wordCount', wordCount),\n",
    "        ('fullText', fullText),\n",
    "        ('unigramCount', unigramCount), \n",
    "        ('bigramCount', bigramCount), \n",
    "        ('trigramCount', trigramCount)\n",
    "    ])\n",
    "    \n",
    "    # Add additional metadata if there is a metadata.csv available\n",
    "    if csv_exists == True:\n",
    "        # Read in the CSV file and set the index\n",
    "        df = pd.read_csv(f'./data/{csv_filename}')\n",
    "        df.set_index('id', inplace=True)\n",
    "        # Update Metadata\n",
    "        update_metadata_from_csv()\n",
    "        \n",
    "    \n",
    "    # Write the document to the json file  \n",
    "    with open(f'./data/{output_filename}', 'a') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "        print(f'Text {text} written to json-l file.')\n",
    "\n",
    "print('\\n' + str(len(text_list)) + f' texts written to {output_filename}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gzip the JSON-L file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GZip dataset\n",
    "\n",
    "f_in = open(f'./data/{output_filename}', 'rb')\n",
    "f_out = gzip.open(f'./data/{output_filename}.gz', 'wb')\n",
    "f_out.writelines(f_in)\n",
    "f_out.close()\n",
    "f_in.close()\n",
    "print(f'Compression complete. \\n{output_filename}.gz has been created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
