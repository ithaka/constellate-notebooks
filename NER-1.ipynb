{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "This notebook was created by William Mattingly for the 2022 Text Analysis Pedagogy Institute, with support from the [National Endowment for the Humanities](https://neh.gov), [JSTOR Labs](https://labs.jstor.org/), and [University of Arizona Libraries](https://new.library.arizona.edu/). \n",
    "\n",
    "This notebook is adapted by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# MultiLingual NER\n",
    "\n",
    "This is lesson 1 of 3 in the educational series on **Named Entity Recognition**. This notebook is intended to show the basic problems one faces in multilingual texts. \n",
    "\n",
    "**Audience:** Teachers / Learners / Researchers\n",
    "\n",
    "**Use case:** Tutorial / How-To / Reference / Explanation\n",
    "\n",
    "**Difficulty:** Intermediate / Advanced\n",
    "\n",
    "**Completion time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "\n",
    "* [Python Basics](./python-basics-1.ipynb)\n",
    "* [Python intermediate 2](./python-intermediate-2.ipynb)\n",
    "* [Python intermediate 4](./python-intermediate-4.ipynb)\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* Natural Language Processing\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "\n",
    "* Understand Named Entity Recognition (NER) as a concept\n",
    "* Understand text encoding\n",
    "* Understand how to solve encoding-issues\n",
    "* Understand the complexities of multilingual corpora\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a44149-e47f-44b9-a885-b4544a4dabc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3c4ab",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "**Entities** are words in a text that correspond to a specific type of data. For example, we may find the following types of entities in a text.\n",
    "* numerical, such as cardinal numbers; \n",
    "* temporal, such as dates; \n",
    "* nominal, such as names of people and places;\n",
    "* political, such as geopolitical entities (GPE). \n",
    "\n",
    "Named entity recognition, or NER, is the process by which a system takes an input of a text and outputs the identification of entities.\n",
    "\n",
    "### A simple example\n",
    "Let's use the following sentence as an example.\n",
    "\n",
    "*Martha, a senior, moved to Spain where she will be playing basketball until 05 June 2022 or until she can't play any longer.*\n",
    "\n",
    "First, there is \"Martha\", a person's name. Different NER models may give the lable of PERSON or PER to it.\n",
    "\n",
    "Second, there is \"Spain\", a country name. It is a GPE, or Geopolitical Entity.\n",
    "\n",
    "Finally, there is \"05 June 2022\", a date. It is a DATE entity. \n",
    "\n",
    "Of course, you may have more categories of entities from a text that you would like to label. For example, if you are interested in extracting sports from a text, “basketball” could be extracted and given the label SPORT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63c21-198c-4858-96e2-478fb41fb350",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We have been talking about \"tokens\", but what does it mean? To understand what \"tokens\" means, let's get a general idea of natural language processing first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9777476-480d-40a9-aeef-89144fc79499",
   "metadata": {},
   "source": [
    "Natual Language Processing (NLP) is the process by which a researcher uses a computer system to parse human language and extract important metadata from texts.\n",
    "\n",
    "How do we extract information from texts? We do it through a series of pipelines that perform some operations on the data at hand.\n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_NLP_Pipeline.png' width=700></center>\n",
    "\n",
    "Named entity recognition is a branch of natural language processing. From the graph, you may notice that named entity recognition comes later in a pipeline. This is because it needs to receive a tokenized text and, in some languages, it needs to understand a word's part-of-speech (POS) to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354529f1",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "The job of a tokenizer is to break a text into individual tokens. Tokens are items in a text that have some linguistic meaning. They can be words, such as \"Martha\", but they can also be punctuation marks, such as \",\" in the relative clause \", a senior,\". Likewise, \"'nt\" in the contraction \"can't\" would also be recognized as a token since \"'nt\" in English corresponds to the word \"not\".\n",
    "\n",
    "### POS tagger\n",
    "A common pipeline after a tokenizer is a POS tagger whose job is to identify the parts-of-speech, or POS, in the text. Let us consider an example sentence:\n",
    "\n",
    "The boy took the ball to the store.\n",
    "\n",
    "The nominative (subject), \"boy\", comes first in the sentence, followed by the verb, \"took\", then followed by the accusative (object), \"ball\", and finally the dative (indirect object), \"store\". The words \"the\" and \"to\" also contain vital information. \"The\" occurs twice and tells the reader that it's not just any ball, it's the ball; likewise, it's not just a store, but the store. The period too tells us something important. This is a statement, not a question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803f59c-d7f8-48ff-af5a-b10aa0b8fbdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Encoding\n",
    "\n",
    "Before we tokenize a text, we have to know what encoding it uses. \n",
    "\n",
    "When we say \"plain text\", we are actually being really sloppy. There is no such thing as \"plain text\"! It does not make sense to have a text without knowing what encoding it uses. \n",
    "\n",
    "Have you ever received an email in which you find the text is unintelligible because there are random question marks in it? Did you wonder why?\n",
    "\n",
    "## A little bit history about text encoding\n",
    "\n",
    "### ASCII (American Standard Code for Information Interchange)\n",
    "There was an old time when the only characters that mattered were good old unaccented English letters. ASCII is a code which was able to represent every character using a number between 32 and 127 (Codes below 32 were called unprintable and for control characters, e.g. the backspace). For example, the ASCII code of the letter A is 65. As you know, computers use a binary system and therefore the number 65 is actually encoded as a 8-bit number.\n",
    "\n",
    "A $\\rightarrow$ 0100 0001\n",
    "\n",
    "8-bit allows up to $2^{8}=256$ characters and we have only had 128 (with numbers 0-127). That means we can use the numbers 128 to 255 to represent other characters! English, of course, is not the only language that matters. Therefore, people speaking different languages chose to use the numbers 128 to 255 for the characters in their own language. This means that two different characters from two different languages may be represented by the same number in their respective encoding standard. This is no good, because when Americans would send their résumés to Israel they would arrive as rגsumגs. \n",
    "\n",
    "\n",
    "### UTF-8\n",
    "Can't we have a single character set that included every reasonable writing system on the planet? Yes we can! \n",
    "Here comes the brilliant idea of UTF-8. UTF stands for Unicode Tranformation Format. 8 means 8-bit.\n",
    "\n",
    "Every letter in every alphabet is assigned a number written like this: U+0041. It is called a *code point*. The U+ means \"Unicode\" and the numbers are hexadecimal. In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal form. The English letter A is assigned the number U+0041.\n",
    "\n",
    "UTF-8 was designed for backward compatibility with ASCII: the first 128 characters of Unicode, which correspond one-to-one with ASCII, are encoded using a single byte with the same binary value as ASCII.\n",
    "\n",
    "## ord( ) and chr( )\n",
    "\n",
    "`ord()` and `chr()` are built-in functions in Python.\n",
    "\n",
    "The `ord()` function takes a single Unicode character and returns its integer Unicode code point value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aec386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ord() to convert a Unicode character string to its integer code point value\n",
    "\n",
    "ord(\"ø\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea73bb",
   "metadata": {},
   "source": [
    "The `chr()` function does the opposite. It takes an integer and returns the corresponding Unicode character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a350edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use chr() to convert a number to a character\n",
    "\n",
    "chr(248)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec3733",
   "metadata": {},
   "source": [
    "The brief historical review gives us an idea of how messy it was with regard to text encoding before UTF-8. There were and actually still are so many different encoding methods out there. Right now I am viewing a webpage in Safari. I can take a peek at how many different encoding methods there are by just choosing View > Text Encoding. For you, depending on your browser, you may find the text encoding in a different place. \n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_DifferentEncodings.png' width=500></center>\n",
    "\n",
    "## Exercise \n",
    "Go to a website of your choice. Change the text encoding of the page to a different one. What happens after you change the encoding? Keep changing the text encoding to different encoding methods and see what happens. Last, change the text encoding to utf8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ad92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a4ed9f6",
   "metadata": {},
   "source": [
    "When you are viewing a webpage, the website is sending a sequence of integers to your web browser, with a recommendation about how they are supposed to be translated into characters. So, if your character encoding does not match the one intended by the web page, you will see garbled text. For example, let's go to the news page of Google Hong Kong https://news.google.com/home?hl=zh-HK&gl=HK&ceid=HK:zh-Hant. On this page, you see some traditional Chinese characters and they are displayed correctly. \n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_WebsiteChineseCharacters.png' width=900></center>\n",
    "\n",
    "\n",
    "As soon as I change the text encoding to ISO 8859-8, which is used to encode Hebrew characters, you see that the text becomes garbage.\n",
    "\n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/NER_ChineseCharacterParsedByHebrewEncoding.png' width=900></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766ebd7-066f-460e-b9ab-fae64ba585de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## An example\n",
    "\n",
    "Let's use the text file polish-lat2.txt in NLTK as an example. As the name suggests, this file is in Polish and is encoded as Latin2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f32471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk and download the sample files\n",
    "import nltk\n",
    "nltk.download('unicode_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6bb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the file\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc9ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python3 to read the file\n",
    "with open(path, 'r', encoding='latin2') as f:\n",
    "    pol_data=f.read()\n",
    "print(pol_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc4087",
   "metadata": {},
   "source": [
    "Everything looks fine. This is because we have specified the encoding method of the original file correctly. What will happen if we read the file using the wrong encoding method? For example, if we open the file and specify the encoding as utf8 instead of latin2, what will happen? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wrong encoding utf8 to read the file\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    pol_data=f.read()\n",
    "print(pol_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd3b53",
   "metadata": {},
   "source": [
    "We get an error! This is because in different encodings, the inventory of characters are different and the valid bytes the characters are encoded into are different as well. In our example, 0xf1 is a valid byte in latin2 but is not a valid byte in utf8. \n",
    "\n",
    "In real life, instead of an error message, we often see a question mark � or a box  􏿾 in place of the character that cannot be correctly rendered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b2995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that when error occurs, fill in replacement character\n",
    "with open(path, 'r', encoding='utf8', errors='replace') as f:\n",
    "    pol_data_replace=f.read()\n",
    "print(pol_data_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cfc47",
   "metadata": {},
   "source": [
    "Let's encode the Polish data into bytes in latin2. The letter b prefixed to the string indicates that this is a byte string. You may wonder why some characters stay the same but others are turned into hex bytes. This is because only non-ASCII characters are converted to hex decimal representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the Polish data into latin2 bytes\n",
    "pol_bytes = pol_data.encode(encoding='latin2')\n",
    "pol_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5981b2",
   "metadata": {},
   "source": [
    "## What does all this mean to Named Entity Recognition?\n",
    "Now, we understand why 'plain text' is just a mystery. Computers ultimately only get a sequence of numbers and what characters those numbers translate to depends on the encoding. \n",
    "\n",
    "For those who work with multilingual corpora, especially those who work with texts that were created before the modern day, you will encounter at some point corpora that contain multiple encodings. We can use Python, however, to read a different encoding, standardize it into utf-8, and then continue to open that file as a utf-8 file consistently in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38613ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python3 to read the file\n",
    "with open(path, 'r', encoding='latin2') as f:\n",
    "    pol_data=f.read()\n",
    "print(pol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d154cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data with the encoding utf8 to another file\n",
    "with open('pol_lat2_to_utf8.txt', 'w', encoding='utf8') as f:\n",
    "    f.write(pol_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faacec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the new file using utf8\n",
    "with open('pol_lat2_to_utf8.txt', 'r', encoding='utf8') as f:\n",
    "    pol_utf8=f.read()\n",
    "print(pol_utf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2aca83",
   "metadata": {},
   "source": [
    "Recall the non-ASCII character ń that trips us over? We can see how it looks like at the byte level in utf8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the Polish data into utf8 bytes\n",
    "pol_utf8_bytes = pol_utf8.encode('utf8')\n",
    "pol_utf8_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d7e72",
   "metadata": {},
   "source": [
    "When we print out the Polish data from the file encoded in latin2 and from the one encoded in utf8, we get two strings on the screen that look exactly the same. However, when we get the byte strings from the two files, we see that even if two characters look the same to our naked eye, e.g. ń, they are different at the byte level. This means the computer will see them as two different characters.\n",
    "\n",
    "Again, for those who work with multilingual corpora, you will encounter at some point corpora that contain multiple encodings. So, always convert your data to utf8 before proceeding to tokenization and NER. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0bbe26-b237-4e04-8d38-46924e346f4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problems within UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d67dc-4b5d-414f-b2e2-356f3baa7f9a",
   "metadata": {},
   "source": [
    "Our problems with encodings, unfortunately, do not end with UTF-8. Once we have encoded our texts into UTF-8, we can still have issues with characters that look the same but being encoded differently. This is particularly true with accented characters.\n",
    "\n",
    "Here we have two characters that look exactly the same and are also deemed the same by the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cd950-aa3d-4386-9d03-473f536c9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two characters that look exactly the same are deemed the same\n",
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f9a29",
   "metadata": {},
   "source": [
    "Here, we also have two characters that look exactly the same but this time they are deemed as two different characters by the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7096345-bb1f-4903-9889-147fef0a6b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two characters that look exactly the same are not deemed the same\n",
    "\"Ç\" == \"Ç\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26c8ae",
   "metadata": {},
   "source": [
    "The two characters are regarded as different by the computer because at the byte level, they are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34560c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the unicode strings for the two characters\n",
    "print(\"\\u00C7\", \"\\u0043\\u0327\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85274a3",
   "metadata": {},
   "source": [
    "One of them is seen as a single character, the accented C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046a64a-deba-415e-a2fc-3ca9733fbce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latin capital letter C with cedilla\n",
    "accent_c = \"\\u00C7\"\n",
    "print (accent_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac1d46c",
   "metadata": {},
   "source": [
    "The other is seen as a compound character, consisting of two characters, one is the Latin letter C and the other is the 'combining cedilla' character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27755e-2210-4d2c-b07d-8a3fe2da95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Latin capital letter C' and 'combining cedilla' characters together\n",
    "compound_c = \"\\u0043\\u0327\"\n",
    "print (compound_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f1212",
   "metadata": {},
   "source": [
    "The 'combining cedilla' character can be combined with other letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4098ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example of compound character\n",
    "\"J\\u0327\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the two parts of the compound C\n",
    "\"\\u0043 \\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd59b5",
   "metadata": {},
   "source": [
    "Because the compound C and the accented C are different at the byte level, they are not considered the same by the computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two different byte strings for the two characters\n",
    "\"\\u00C7\" == \"\\u0043\\u0327\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b9002",
   "metadata": {},
   "source": [
    "### Unicode normalization\n",
    "\n",
    "In NER though, we will not want our NER model to interpret the two characters as two different characters. Therefore, we will need to first normalize them to make them the same at the byte level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8a37-22a7-47d2-a23a-06ab2f256f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60460cb-e888-4b1b-812e-e024b0572596",
   "metadata": {},
   "source": [
    "| Name | Abbreviation | Description | Example |\n",
    "| --- | --- | --- | --- |\n",
    "| Form D | NFD | *Canonical* decomposition | `Ç` → `C ̧` |\n",
    "| Form C | NFC | *Canoncial* decomposition followed by *canonical* composition | `Ç` → `C ̧` → `Ç` |\n",
    "\n",
    "```\n",
    "Source: James Briggs - https://towardsdatascience.com/what-on-earth-is-unicode-normalization-56c005c55ad0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compound C and accented C\n",
    "print(compound_c, accent_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\u00C7\", \"\\u0043\\u0327\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901a3f4-1219-4581-aa9a-0fba005aaf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the accented character using Normal Form D\n",
    "nfd_accent = unicodedata.normalize('NFD', accent_c)\n",
    "print (compound_c == nfd_accent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9448ea24-7271-4443-a28a-ff02c01c001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose and then compose using Normal Form C\n",
    "nfc_compound = unicodedata.normalize('NFC', compound_c)\n",
    "print (accent_c == nfc_compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d1f74",
   "metadata": {},
   "source": [
    "## Resources\n",
    "Tim Scott from Computerphile explains UTF-8\n",
    "\n",
    "[![utf8](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/video/NER_utf8.png)](https://www.youtube.com/watch?v=MijmeoH9LT4)\n",
    "\n",
    "James Briggs Explains Unicode Normalization\n",
    "\n",
    "[![unicodenormalization](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/video/NER_unicodenormalization.png)](https://www.youtube.com/embed/9Od9-DV9kd8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e61e3",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c003f69",
   "metadata": {},
   "source": [
    "Bird, S., Klein, E., & Loper, E. (2009). *Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit*. O'Reilly Media, Inc.\n",
    "\n",
    "Spolsky, J. (2003, October 8). The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!). Joel on Software. https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ec7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
