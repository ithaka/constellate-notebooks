{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZtnwJbt6oJT"
   },
   "source": [
    "<img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\" align=left alt=\"CC BY license logo\" /><br /><br />\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "**For questions/comments/improvements, email nathan.kelber@ithaka.org.**<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Metadata and Pre-Processing\n",
    "\n",
    "**Description of methods in this notebook:**\n",
    "This [notebook](https://constellate.org/docs/key-terms/#jupyter-notebook) shows how to explore and pre-process the [metadata](https://constellate.org/docs/key-terms/#metadata) of a [dataset](https://constellate.org/docs/key-terms/#dataset) using [Pandas](https://constellate.org/docs/key-terms/#pandas). \n",
    "\n",
    "The following processes are described:\n",
    "\n",
    "* Importing a [CSV file](https://constellate.org/docs/key-terms/#csv-file) containing the [metadata](https://constellate.org/docs/key-terms/#metadata) for a given dataset ID\n",
    "* Creating a [Pandas](https://constellate.org/docs/key-terms/#pandas) dataframe to view the [metadata](https://constellate.org/docs/key-terms/#metadata)\n",
    "* Pre-processing your [dataset](https://constellate.org/docs/key-terms/#dataset) by filtering out unwanted texts\n",
    "* Importing the full [JSON Lines (.jsonl)](https://constellate.org/docs/key-terms/#jsonl) dataset and filtering it\n",
    "* Visualizing the metadata of your pre-processed [dataset](https://constellate.org/docs/key-terms/#dataset) by the number of documents/year and pages/year\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 60 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "* Pandas Series ([Start Pandas 1](../Pandas-basics/pandas-basics-1.ipynb))\n",
    "\n",
    "**Data Format:** \n",
    "* [CSV file](https://constellate.org/docs/key-terms/#csv-file)\n",
    "* [JSON Lines (.jsonl)](https://constellate.org/docs/key-terms/#jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* [constellate](https://constellate.org/docs/key-terms/#constellate-client) client to retrieve the [metadata](https://constellate.org/docs/key-terms/#metadata) in a [CSV file](https://constellate.org/docs/key-terms/#csv-file)\n",
    "* [pandas](https://constellate.org/docs/key-terms/#pandas) to manipulate and visualize the metadata\n",
    "\n",
    "**Research Pipeline:** None\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import modules and libraries ###\n",
    "\n",
    "import constellate\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "\n",
    "# For displaying plots inline with Jupyter Notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset metadata file (.csv)\n",
    "\n",
    "The next code cell tries to import your dataset metadata with each of the following methods:\n",
    "\n",
    "1. Search for a custom dataset metadata file in the data folder \n",
    "2. Download a full dataset metadata file that has been requested\n",
    "3. Download a sampled dataset metadata file (1500 items) that builds automatically when a dataset is created\n",
    "\n",
    "If you are using a [dataset ID](https://constellate.org/docs/key-terms/#dataset-ID), replace the default dataset ID in the next code cell.\n",
    "\n",
    "If you don't have a dataset ID, you can:\n",
    "* Use the sample dataset ID already in the code cell\n",
    "* [Create a new dataset](https://constellate.org/builder)\n",
    "* [Use a dataset ID from other pre-built sample datasets](https://constellate.org/dashboard/datasets)\n",
    "\n",
    "The Constellate client will download dataset metadata automatically using either the `.download()` or `.get_metadata()` method.\n",
    "* Full dataset metadata files are downloaded using the `.download()` method. They must be requested first in the builder environment. See the [Constellate client documentation](https://constellate.org/docs/constellate-client).\n",
    "\n",
    "* Sampled dataset metadata files (1500 items) are downloaded using the `.get_metadata()` method. They are built automatically when a dataset is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:03:13.423058Z",
     "iopub.status.busy": "2024-12-16T15:03:13.422773Z",
     "iopub.status.idle": "2024-12-16T15:03:13.428440Z",
     "shell.execute_reply": "2024-12-16T15:03:13.427831Z",
     "shell.execute_reply.started": "2024-12-16T15:03:13.423039Z"
    }
   },
   "source": [
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the metadata csv file to the current working directory.&lt; / &gt; </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_metadata = '' # copy and paste the path to your metadata csv file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to import pandas for our analysis and create a dataframe. We will use the `read_csv()` method to create our dataframe from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create our dataframe using Pandas\n",
    "df = pd.read_csv(dataset_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the size of our dataset using the `len()` function on our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_document_count = len(df)\n",
    "print(f'Total original documents: {original_document_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the data in our dataframe `df`. We will set pandas to show all columns using `set_option()` then get a preview using `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the pandas option to show all columns\n",
    "# Setting None gives us all columns\n",
    "# To show less columns replace None with an integer\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "# Set maximumum number of rows to 50\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "# Show the first five rows of our dataframe\n",
    "# To show a different number of preview rows\n",
    "# Pass an integer into the .head()\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are descriptions for the metadata types found in each column:\n",
    "\n",
    "|Column Name|Description|\n",
    "|---|---|\n",
    "|id|a unique item ID (In JSTOR, this is a stable URL)|\n",
    "|title|the title for the item|\n",
    "|isPartOf|the larger work that holds this title (for example, a journal title)|\n",
    "|publicationYear|the year of publication|\n",
    "|doi|the digital object identifier for an item|\n",
    "|docType|the type of document (for example, article or book)|\n",
    "|provider|the source or provider of the dataset|\n",
    "|datePublished|the publication date in yyyy-mm-dd format|\n",
    "|issueNumber|the issue number for a journal publication|\n",
    "|volumeNumber|the volume number for a journal publication|\n",
    "|url|a URL for the item and/or the item's metadata|\n",
    "|creator|the author or authors of the item|\n",
    "|publisher|the publisher for the item|\n",
    "|language|the language or languages of the item (eng is the ISO 639 code for English)|\n",
    "|pageStart|the first page number of the print version|\n",
    "|pageEnd|the last page number of the print version|\n",
    "|placeOfPublication|the city of the publisher|\n",
    "|wordCount|the number of words in the item|\n",
    "|pageCount|the number of print pages in the item|\n",
    "|outputFormat|what data is available ([unigrams](https://constellate.org/docs/key-terms/#unigram), [bigrams](https://constellate.org/docs/key-terms/#bigram), [trigrams](https://constellate.org/docs/key-terms/#trigram), and/or full-text)|\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out columns using Pandas\n",
    "\n",
    "If there are any columns you would like to drop from your analysis, you can drop them with:\n",
    "\n",
    "`df = df.drop(['column_name1', 'column_name2', ...], axis=1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop each of these named columns\n",
    "# axis=1 specifies we are dropping columns\n",
    "# axis=0 would specify to drop rows\n",
    "df = df.drop(['outputFormat', 'pageEnd', 'pageStart', 'datePublished'], axis=1)\n",
    "\n",
    "# Show the first five rows of our updated dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out rows with Pandas\n",
    "\n",
    "Now that we have filtered out unwanted metadata columns, we can begin filtering out any texts that may not match our research interests. Let's examine the first and last ten rows of the dataframe to see if we can identify texts that we would like to remove. We are looking for patterns in the metadata that could help us remove many texts at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview the first ten items in the dataframe\n",
    "# Can you identify patterns to select rows to remove?\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview the last ten items in the dataframe\n",
    "# Can you identify patterns to select rows to remove?\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create a function to report how many documents were removed.\n",
    "\n",
    "def texts_report(pre_count):\n",
    "    \"\"\"Prints out a report of:\n",
    "    1. How many documents were removed\n",
    "    2. The total original number of documents\n",
    "    3. The total current number of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    removed_count = pre_count - len(df)\n",
    "    print(f'{removed_count} texts were removed.')\n",
    "    print(f'Total original documents: {original_document_count}')\n",
    "    print('Total current documents: ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all rows without data for a particular column\n",
    "\n",
    "For example, we may wish to remove any texts that do not have authors. (In the case of journals, this may be helpful for removing paratextual sections such as the table of contents, indices, etc.) The column of interest in this case is `creator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all texts without an author\n",
    "\n",
    "print('Removing texts without authors...')\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['creator']) #drop each row that has no value under 'creators'\n",
    "\n",
    "# Report the number of texts removed\n",
    "texts_report(initial_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove row based on the content of a particular column\n",
    "\n",
    "We can also remove texts, depending on whether we do (or do not) want a particular value in a column. Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all items with a particular title\n",
    "# Change title to desired column\n",
    "# Change `Review Article` to your undesired title\n",
    "title_to_remove = 'Review Article'\n",
    "\n",
    "# Removing texts\n",
    "print(f'Removing texts with title \"{title_to_remove}\"...')\n",
    "initial_count = len(df)\n",
    "\n",
    "# Create a filter that returns all titles that do not match `title_to_remove`\n",
    "# Apply the filter to the DataFrame\n",
    "title_filter = df['title'] != title_to_remove\n",
    "df = df[title_filter]\n",
    "\n",
    "# Report the number of texts removed\n",
    "texts_report(initial_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep only items with a particular language\n",
    "# Change language to desired column\n",
    "# Change 'eng' to your desired language\n",
    "language = 'eng' # Change to another language code for other languages\n",
    "\n",
    "# Removing texts\n",
    "print(f'Removing texts not in \"{language}\" language...')\n",
    "initial_count = len(df)\n",
    "\n",
    "# Create a filter that returns all languages matching `language` variable\n",
    "language_filter = df['language'] == language\n",
    "df = df[language_filter] # Apply filter to the DataFrame\n",
    "\n",
    "# Report the number of texts removed\n",
    "texts_report(initial_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove all items with less than 1500 words\n",
    "# Change `min_word_count to your desired minimum number of words\n",
    "min_word_count = 1500\n",
    "\n",
    "# Removing texts\n",
    "print(f'Removing texts with fewer than {min_word_count} words...')\n",
    "initial_count = len(df)\n",
    "\n",
    "# Create a filter that \n",
    "word_count_filter = df['wordCount'] > min_word_count\n",
    "df = df[word_count_filter]\n",
    "\n",
    "# Report the number of texts removed\n",
    "texts_report(initial_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a final look at your dataframe to make sure the current texts fit your research goals. In the next step, we will save the IDs of your pre-processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview the first 50 rows of your dataset\n",
    "# If all the items look good, move to the next step.\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Full JSON-L Dataset\n",
    "\n",
    "We created our filtered list of IDs from the metadata CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Python list of IDs we want from the Pandas dataframe\n",
    "filtered_id_list = df[\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will download the dataset (JSON-L file) and then filter out any unwanted documents. If you used a sampled metadata (.csv) file, then you should use the sampled dataset (.jsonl). If you used the full metadata (.csv) file, then you should use the full dataset (.jsonl) file. \n",
    "\n",
    "See more:\n",
    "* [Constellate dataset types](https://constellate.org/docs/what-format-are-jstor-portico-datasets)\n",
    "* [Dataset options](https://constellate.org/docs/dataset-options)\n",
    "* [Constellate client](https://constellate.org/docs/constellate-client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the JSONL file containing metadata, ngrams and full texts to the current working directory.&lt; / &gt; </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the jsonl file in the current directory\n",
    "file_path = '' # copy and paste the path to the JSONL file \n",
    "\n",
    "# function that reads a jsonl file into a generator\n",
    "def dataset_reader(file_path):\n",
    "    \"\"\"\n",
    "    Helper to read in gzip files and yield Python dictionary\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an empty JSON-L file to filter our results into. We will also check if we want to overwrite the file, if it already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now filter through each document in the original JSON file. All documents that match our filtered ID list will be added to the new JSON file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Append all documents with ids in `filtered_id_list` to a new JSONL file\n",
    "\n",
    "# Define the filtered file output name\n",
    "new_file_path = Path.cwd() / 'my_filtered_data.jsonl' # You may change the name of the file here\n",
    "\n",
    "for document in dataset_reader(file_path): \n",
    "    document_id = document['id']\n",
    "    # Append any documents in the filtered list\n",
    "    if document_id in filtered_id_list:\n",
    "        with new_file_path.open('a') as outfile:\n",
    "            json.dump(document, outfile)\n",
    "            outfile.write('\\n')\n",
    "print(f'{file_path} created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compress the dataset file using `gzip`. The compression may take some time, but it will make it easier to transfer the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compress the file using gzip\n",
    "# This may take several minutes to complete\n",
    "f_in = new_file_path.open('rb')\n",
    "f_out = gzip.open(f'{new_file_path}.gz', 'wb')\n",
    "f_out.writelines(f_in)\n",
    "f_out.close()\n",
    "f_in.close()\n",
    "print(f'Compression complete. \\n{new_file_path}.gz has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Note! &lt; / &gt; </h3>\n",
    "\n",
    "It is a good idea to keep a copy of your dataset in your lab and on your local machine as backup. The Constellate lab has a storage limit of 30 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualizing the Pre-Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upxLwnZr6oJs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the data by publication year and the aggregated number of ids into a bar chart\n",
    "df.groupby(['publicationYear'])['id'].agg('count').plot.bar(title='Documents by year', figsize=(20, 5), fontsize=12); \n",
    "\n",
    "# Read more about Pandas dataframe plotting here: \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNp2wN8I6oJt"
   },
   "source": [
    "And now let's look at the total page numbers by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uifT8Ocy6oJu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the data by publication year and aggregated sum of the page counts into a bar chart\n",
    "\n",
    "df.groupby(['publicationYear'])['pageCount'].agg('sum').plot.bar(title='Pages by decade', figsize=(20, 5), fontsize=12);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2-metadata.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
