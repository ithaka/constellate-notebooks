{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e809f75",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Hannah Jacobs](http://hannahlangstonjacobs.com/) for the [2021 Text Analysis Pedagogy Institute](https://nkelber.github.io/tapi2021/book/intro.html).\n",
    "\n",
    "Adapted by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3839d0",
   "metadata": {},
   "source": [
    "# Optical Character Recognition Basics\n",
    "\n",
    "These [notebooks](https://docs.constellate.org/key-terms/#jupyter-notebook) describe how to turn images and/or pdf documents into plain text using Tesseract [optical character recognition](https://docs.constellate.org/key-terms/#ocr).\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "\n",
    "**Data Format:** \n",
    "* image files (.jpg, .png)\n",
    "* document files (.pdf)\n",
    "* plain text (.txt)\n",
    "\n",
    "**Libraries Used:**\n",
    "* [Tesseract](https://tesseract-ocr.github.io/) for performing [optical character recognition](https://docs.constellate.org/key-terms/#ocr).\n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this lessons, students will be able to\n",
    "1. Define \"OCR\"\n",
    "2. Explain the importance of OCR for computer-aided reading and analysis\n",
    "3. Perform basic OCR operations using Python, Tesseract, and Jupyter Notebooks\n",
    "\n",
    "**Research Pipeline:**\n",
    "\n",
    "1. Digitize documents\n",
    "2. **Optical Character Recognition**\n",
    "3. Tokenize your texts\n",
    "4. Perform analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9d285",
   "metadata": {},
   "source": [
    "## What is OCR? Why is it important?\n",
    "\n",
    "In order to do text analysis (or [natural language processing](https://docs.constellate.org/key-terms/#nlp), we need to have our text in a machine-readable format such as plaintext. In practice, this usually means converting an image file (e.g. a file ending in .png or jpg) into a plaintext file (.txt). Text is machine-readable if you are able to select, copy, and paste it's individual characters.\n",
    "\n",
    "The difference can be illustrated by a digital image (.png) of the print edition of Dr. Faust.\n",
    "\n",
    "![An image of the German print edition of Dr. Faust](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/faust.png)\n",
    "\n",
    "and the [text version found on Project Gutenberg](https://www.gutenberg.org/files/2229/2229-0.txt). While a human can read the text of the digital image, a computer is not able to manipulate the individual characters of the text. The digital text cannot be easily copied and pasted for manipulation in other applications. \n",
    "\n",
    "![Image of the text \"Blackwell's\" showing the pixels.](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/blackwell-pixels.jpeg)\n",
    "\n",
    "While we might see this as the word `Blackwell's`, the computer understands the above as a series of squares, **pixels**, containing information about which color the pixel should be--*not* which character to display.  If we want the computer to be able to work this text *as* text, we need to convert the image above into this:\n",
    "\n",
    "`01000010 01101100 01100001 01100011 01101011 01110111 01100101 01101100 01101100 00100111 01110011`\n",
    "\n",
    "...which the computer will then display for human readers as `Blackwell's`. We can then use our computers to search for instances of this word, analyze its freqency, patterns in occurrence, collocation, and so on. We can also ask the computer to read this and any other words in the page aloud if we need to hear them instead of viewing them on a screen.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd733883",
   "metadata": {},
   "source": [
    "## What is OCR?\n",
    "\n",
    "OCR, or \"Optical Character Recognition,\" is **a computational process that converts digital images of text into computer-readable text**. OCR is both a noun and a verb.\n",
    "\n",
    "More specifically:\n",
    "\n",
    ">**OCR software attempts to replicate the combined functions of the human eye and brain, which is why it is referred to as artificial intelligence software.** A human can quickly and easily recognise text of varying fonts and of various print qualities on a newspaper page, and will apply their language and cognitive abilities to correctly translate this text into meaningful words. Humans can recognise, translate and interpret the text on a newspaper page very rapidly, even text on an old poor quality newspaper page from the 1800s. We can quickly scan layout, sections and headings, and read the text of articles in the right order (which is much more difficult than reading the page of a book). **OCR software can now do all these things too, but not to the same level of perfection as a human can.** - ([Holley, \"How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs\"](http://www.dlib.org/dlib/march09/holley/03holley.html)).\n",
    "\n",
    "    \n",
    ">\"Optical character recognition (OCR) software is **a type of artificial intelligence software designed to mimic the functions of the human eye and brain and discern which marks within an image represent letterforms or other markers of written language.** OCR scans an image for semantically-meaningful material and transcribes what language it finds into text data.\" - [Cordell, \"Why You (A Humanist) Should Care About Optical Character Recognition\"](https://ryancordell.org/research/why-ocr/).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d73f2",
   "metadata": {},
   "source": [
    "## OCR Tools\n",
    "\n",
    "If you have [Adobe Acrobat](https://acrobat.adobe.com/us/en/) on your computer, then you have probably already been using software that contains OCR functionality. Acrobat's OCR is designed to help users [edit scanned PDFs or PDFs created by others](https://helpx.adobe.com/acrobat/using/edit-scanned-pdfs.html). It can also be used to export editable text versions (e.g. Microsoft Word documents), or to ask the computer to read aloud the text contained in the PDF. However, *at scale* and working with *older printed documents, perhaps with irregular printing patterns*, Acrobat may not give you the best results.\n",
    "\n",
    "### Questions when considering an OCR tool\n",
    "\n",
    "* [Proprietary or open source?](#proprietary-or-open)\n",
    "* [GUI (graphical user interface) or script-based?](#gui-or-script)\n",
    "* [File types supported?](#file-types)\n",
    "* [Languages supported?](#languages)\n",
    "* [Which printed scripts can it read?](#print-scripts)\n",
    "* [Preprocessing features?](#preprocessing)\n",
    "* [Accuracy and error assessment?](#accuracy)\n",
    "\n",
    "There may be other questions you'll need to add to this list, but it will get you started. Likewise, you may wish to reorder these questions based on your project's priorities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc8ead",
   "metadata": {},
   "source": [
    "#### Proprietary or open source? <a id=\"proprietary-or-open\"></a>\n",
    "\n",
    "Proprietary, meaning do you need to purchase a license? Knowing the resources you have or need to start your OCR project is key to how you make your decision. You may wish to work with a program such as [ABBYY FineReader](https://pdf.abbyy.com/pricing/), which includes a number of graphical features for preprocessing that you'd like to use. But you'll need to be prepared to pay $200-300 for it. If you don't have those funds, you may wish to work with a free tool. \n",
    "\n",
    "Although *free* software is not necessarily the same as *open source* software, [open source](https://en.wikipedia.org/wiki/Open-source_model) software is free. **Open source, in the software world, refers to software whose creators have made the underlying code available for others to edit and build upon.** You may opt to choose an open source OCR tool so that you have more access to the codebase, and therefore better understanding of the computation that goes into performing OCR on your corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fefde4",
   "metadata": {},
   "source": [
    "#### GUI (graphical user interface) or script-based?<a id=\"gui-or-script\"></a>\n",
    "\n",
    "If you are working on a project alone with no coding experience, you may be thinking that a GUI that provides the ease of clicking a button is the best way to go--and it may be if you have a small set of documents with modern typefaces. \n",
    "\n",
    "On the other hand, you may wish to learn some coding if there are a significant number of documents and/or those documents contain unusual features (typefaces, language, text layouts, etc.). If so, learning how to run OCR with Python is a great opportunity. Even if you're collaborating with a programmer who will write most of your OCR code, you may want to learn some of the concepts and basic steps behind the OCR to ensure you have a good understanding of this project phase and to aid communications with your collaborator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3087f",
   "metadata": {},
   "source": [
    "#### File types supported?<a id=\"file-types\"></a>\n",
    "\n",
    "Does the OCR tool work only with PDFs, or can it also read image files? Which file type(s) are you working with? This may seem a small point, but if you have image files, and you purchase a license for OCR software that works only with PDFs, you may be a bit surprised. There are tools out there that can help you convert images to PDFs, but you may risk degrading the scanned text with these conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305264d2",
   "metadata": {},
   "source": [
    "#### Languages supported?<a id=\"languages\"></a>\n",
    "\n",
    "If you are working with texts that are not in English, it's a good idea to check. At this point, most OCR tools work with multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41f2b0",
   "metadata": {},
   "source": [
    "#### Which printed scripts can it read?<a id=\"print-scripts\"></a>\n",
    "\n",
    "If you're working with a language written in a script no longer commonly in use, you may need to seek out some specific tools to assist you. Even if you're working with [late-nineteenth- and early-twentieth-century American non-English newspapers](https://chroniclingamerica.loc.gov/lccn/sn93060356/1917-01-18/ed-1/seq-1/#date1=1880&index=11&date2=1917&searchType=advanced&language=&sequence=0&words=son+sonille&proxdistance=5&state=Missouri&rows=20&ortext=son&proxtext=&phrasetext=&andtext=&dateFilterType=yearRange&page=1), you may need to find out which tools handle specific scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0e42c",
   "metadata": {},
   "source": [
    "#### Preprocessing features?<a id=\"preprocessing\"></a>\n",
    "\n",
    "**Preprocessing is a set of steps that we can use to try to minimize issues such as a skewed page, faded text, or smudges on a page *before* performing OCR.** Some OCR tools offer some preprocessing tools. Others don't. Even if a tool can run preprocessing, though, you may find you have a specific need that must be met with another tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1cf1f2",
   "metadata": {},
   "source": [
    "#### Accuracy and error assessment?<a id=\"accuracy\"></a>\n",
    "\n",
    "Can the tool help you evaluate how well the process has gone and where there may be errors to correct? Are there tools to support both automated and manual error correction? How will you know if the OCRed corpus you've produced is of a high enough quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8480251",
   "metadata": {},
   "source": [
    "### Popular OCR Tools\n",
    "\n",
    "#### [ABBYY Fine Reader](https://pdf.abbyy.com/)\n",
    "Perhaps at the opposite end of the OCR spectrum from Pytesseract, ABBYY is another powerful OCR tool. It has a GUI (graphical user interface) in which users can make adjustments (preprocessing), and it also has an SDK (software developer toolkit) that programmers can use to run ABBYY tools in their own programs. ABBYY even has a cloud service. Like Tesseract, ABBYY supports many languages and a number of file formats. ABBYY is, however, proprietary--you'll need to be prepared to pay a minimum of $200 if your institution does not provide a license.\n",
    "\n",
    "#### [Adobe Acrobat](https://acrobat.adobe.com/us/en/acrobat.html)\n",
    "A common PDF reader, Acrobat can do a lot of things including OCR. It comes in DC and Pro DC versions, and both are paid. DC includes OCR functionality in the \"Enhance PDF\" menu.\n",
    "\n",
    "#### [Amazon Textract](https://aws.amazon.com/textract/resources/?blog-posts-cards.sort-by=item.additionalFields.createdDate&blog-posts-cards.sort-order=desc)\n",
    "Like Pytesseract, this tool from Amazon runs in Python. Like ABBYY Fine Reader, it's proprietary code, which means we don't know what's happening in Textract itself when we use it--it's a black box. There is a free tier to get started if you're working with fewer than 1,000 pages, and you can run your Textract code in Amazon's cloud environment. The cost to use it, if you are planning to learn a little programming or are working with a programmer, is significantly lower than the cost of an ABBYY license.\n",
    "\n",
    "#### [Google Cloud Vision](https://cloud.google.com/vision/docs)\n",
    "A competitor of Amazon's, Google's Cloud Vision API (application programming interface) is likewise proprietary after a certain number of uses, requires programming knowledge, and can be used in the cloud. This same tool can be used to perform computer vision tasks such as facial recognition. Because we don't know what's happening in Cloud Vision's code when we use it, we might not be able to explain unexpected results--it's another [black box](01-AlgorithmsOfResistance-WhatIsAnAlgorithm.ipynb#algorithms).\n",
    "\n",
    "#### [Tesseract](https://tesseract-ocr.github.io/tessdoc/Home.html)\n",
    "An OCR engine (basically, a collection of algorithms and training data) originally developed by Hewlett Packard and maintained by Google. Tesseract is open source and supports many languages and scripts. It also offers possibilities to customize OCR outputs in ways that may or may not be possible with proprietary software. The ability to add your own training data is also a big feature, though a resource-intensive process. Programmers have taken advantage of Tesseract being open source and have created [a number of tools based on Tesseract](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty) (some with GUIs).\n",
    "\n",
    "#### [Pytesseract](https://pypi.org/project/pytesseract/)\n",
    "Pytesseract (or Python-tesseract) is a powerful OCR tool made for the programming language Python using the Tesseract OCR Engine. It can work with many file formats and (human) languages, and, like [Tesseract](https://github.com/tesseract-ocr/tesseract), is open source. Since Pytesseract is used in a larger programming ecosystem, it can be combined with a variety of other Python packages to perform many different tasks. Furthermore, Python is both highly used and a popular computer language for beginning programmers, making it possible for users to move quickly from the basics of Python into working with Pytesseract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58c39df",
   "metadata": {},
   "source": [
    "## Introduction to Tesseract\n",
    "\n",
    "[Tesseract](https://github.com/tesseract-ocr/tesseract) was initially developed by Hewlett-Packward between 1985-1994. HP made it open source in 2005. [Google developed it](https://opensource.google/projects/tesseract) between 2006-2018. It is still open source and maintained Zdenko Podobny. There is an [active user forum](https://groups.google.com/g/tesseract-ocr).\n",
    "\n",
    "Tesseract supports over [100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) and can be [run in the command line](https://github.com/tesseract-ocr/tesseract#running-tesseract) on Windows, MacOS, and Linux. Its outputs can be stored in several interoperable file formats. There are a number of [third party GUIs available](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html).\n",
    "\n",
    "The latest versions (4x) of Tesseract incorporate [LSTM (Long Short-Term Memory)](https://en.wikipedia.org/wiki/Long_short-term_memory), an artificial Recurrent Neural Network. LSTM is a set of algorithms that computers can run to process lots of data, \"remember\" that data, and apply what it \"learns\" from that data to other data as it's processing.\n",
    "\n",
    "Because Tesseract is free and open source, it's [possible to retrain Tesseract in order to OCR a specific corpus](https://tesseract-ocr.github.io/tessdoc/tess4/TrainingTesseract-4.00.html). This requires a large and specific dataset, some expertise, and some time. But it's a key feature that you won't get from proprietary or closed-source software.\n",
    "\n",
    "[PyTesseract](https://pypi.org/project/pytesseract/) is a \"wrapper\" -- basically it makes Tesseract legible to Python so that it can be incorporated into various Python environments and functionalities. This means that if you're already working in Python, you don't need to leave your environment to build a dataset. You could also build PyTesseract into a Python application and/or into a code base that you plan to reuse. It was [developed and maintained](https://github.com/madmaze/pytesseract) beginning in 2014 by a group of programmers led by Mattias Lee."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a3925",
   "metadata": {},
   "source": [
    "### Input Files\n",
    "\n",
    "In order to perform OCR on a text corpus, we need the following:\n",
    "\n",
    "- A **single file folder** containing all of the corpus files. If the corpus is small enough (e.g. 1 book), this could be simply a single file (e.g. a .pdf).\n",
    "- All corpus files should be of the **same file format**.\n",
    "- The chosen file format should be **interoperable** (usable by many software and operating systems) and stable (changes rarely if ever).\n",
    "\n",
    "- For our work with Python and Tesseract, the files should be **images**, which means that each file will correspond to 1 single-sided page (recto or verso, assuming a book format).\n",
    "\n",
    "![First page of the 1955 North Carolina Session Laws](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sessionlaw-example.jpeg)\n",
    "\n",
    "**To keep image files organized,** it is helpful to create a file structure where every book is within a unique folder. Each book's folder then contains a series of numbered images for each page.\n",
    "\n",
    "![Screenshot of a file structure for image files to be OCR'ed.](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/folder-structure.jpeg)\n",
    "\n",
    "Note that the file naming structure identifies *both* which volume the images are part of *and* which scanned page they correspond to, which helps us maintain the order of the volume. These numbers *may not* correspond to page numbers because bookscanning usually includes the outer and inner covers, title pages, and other book pages that are not usually numbered.\n",
    "\n",
    "Note that we are working with .jpg files here. The process we'll be using, though, can also be run with .png, .tiff, .jp2, and other common interoperable image formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa80b5f",
   "metadata": {},
   "source": [
    "### Output Files\n",
    "\n",
    "For each folder of files (whether .jpg, .png, or .pdf), we will create a single plaintext file (.txt) that contains the full-text.  The plain text file format is interoperable, stable, and fully computer readable, meaning it will be ready for performing computational analysis and for storing in repositories and databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1069bb05",
   "metadata": {},
   "source": [
    "## PyTesseract Basics\n",
    "\n",
    "Here we will describe the basic process of OCRing using PyTesseract. The first step is to install Tesseract on your machine using the command line. The following code cell installs Tesseract-OCR on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112360e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt install tesseract-ocr\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f296e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTesseract, the Python wrapper for Tesseract\n",
    "# An exclamation point runs the command on the command line\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683132a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Tesseract training data in the Constellate Analytics Lab.\n",
    "# The exclamation runs the command as a terminal command.\n",
    "\n",
    "!wget https://github.com/tesseract-ocr/tessdata/raw/main/eng.traineddata\n",
    "!mv eng.traineddata /usr/share/tesseract-ocr/4.00/tessdata/eng.traineddata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2b40d",
   "metadata": {},
   "source": [
    "We will convert a [sample .jpg image](./data/ocr_sample.jpeg) to text. The sample comes from the Session Laws of the State of North Carolina. The material was OCRed for the [NEH-funded](https://www.neh.gov/), [Collections as Data](https://collectionsasdata.github.io/) project [On the Books: Jim Crow and Algorithms of Resistance](https://onthebooks.lib.unc.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Image module from the Pillow Library, which will help us access the image.\n",
    "from PIL import Image\n",
    "\n",
    "# Import the pytesseract library, which will run the OCR process.\n",
    "import pytesseract\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"./data/ocr_sample.jpeg\"), lang=\"eng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc480a",
   "metadata": {},
   "source": [
    "Let's break down the above code, from the inside out:\n",
    "\n",
    "1. `Image.open(\"./data/ocr_sample.jpeg\"), lang=\"eng\")` - Open the image file `ocr_sample.jpeg` in the `/data` folder. Set the language to English.\n",
    "\n",
    "2. `pytesseract.image_to_string()` - Using PyTesseract's `image_to_string` function, detect alphanumeric characters in the image and convert them into computer-readable text.\n",
    "\n",
    "\n",
    "3. `print()` - Display the computer-readable text output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e486d",
   "metadata": {},
   "source": [
    "## Tesseract Options \n",
    "\n",
    "Tesseract offers a number of different modes, or settings, that we can use to customize output. There are two types of modes: OEMs (OCR Engine Modes), which specify which OCR tools are available to Tesseract to use, and PSMs (Page Segmentations Modes), which specify how the OCR tools should read the image files--how to separate and order sections of text in the image file.\n",
    "\n",
    "### OCR Engine Modes (OEMs)\n",
    "\n",
    "Run the following command to view the list of OEMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca42e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the Tesseract OCR Engine Modes (OEMs)\n",
    "# Run a terminal command using an exclamation point\n",
    "!tesseract --help-oem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a9fa1",
   "metadata": {},
   "source": [
    "Here's more of an explanation of OCR Engine Modes (OEMs):\n",
    "\n",
    "- *0 - Original Tesseract only.* - This mode runs only the main Tesseract mode.\n",
    "  \n",
    "- *1 - Cube only.* - This mode runs only Cube, [according to Google](https://code.google.com/archive/p/tesseract-ocr-extradocs/wikis/Cube.wiki), \"an alternative recognition mode for Tesseract. It is slower than the original recognition engine, but often produces better results.\" [A Nanonets tutorial explains](https://nanonets.com/blog/ocr-with-tesseract/) that this is the LSTM mode. There is not much documentation out about this.\n",
    "  \n",
    "- *2 - Tesseract + Cube.* - Both Tesseract (Nanonets refers to this as \"Legacy\") and Cube (LSTM) modes are used.\n",
    "\n",
    "- *3 - Default, based on what is available.* - Tesseract will choose an OEM based on the configurations (language, PSM) we give it. Even if we don't include the configuration information, Tesseract will run in OEM 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51451a4b",
   "metadata": {},
   "source": [
    "___\n",
    "<h3 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h3>\n",
    "\n",
    "Run the following script, trying each of the different OEMs in turn replace the number in the first line to change the OEM.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeca4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the OEM number below to try\n",
    "# running another OCR mode.\n",
    "# 3 is the default setting.\n",
    "custom_oem_config = r'--oem 3'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"./data/ocr_sample.jpeg\"), lang=\"eng\", config=custom_oem_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080f8d1",
   "metadata": {},
   "source": [
    "### Page Segmentation Modes (PSM)\n",
    "\n",
    "Run the following command to view all of the PSMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be07ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tesseract --help-psm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1e687",
   "metadata": {},
   "source": [
    "This time, our configuration looks like\n",
    "\n",
    "`custom_oem_config = r'--psm 3'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b95821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the PSM number below to try\n",
    "# running another page segmentation mode.\n",
    "# 3 is the default setting.\n",
    "custom_psm_config = r'--psm 3'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(pytesseract.image_to_string(Image.open(\"./data/ocr_sample.jpeg\"), lang=\"eng\", config=custom_psm_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41489cd",
   "metadata": {},
   "source": [
    "Many of the PSMs are meant for images that have little text in them -- such as images that include road or store signs. [See Tesseract's documentation on improving OCR quality.](https://tesseract-ocr.github.io/tessdoc/ImproveQuality)\n",
    "\n",
    "**Most of the time, the default OEM and PSM is best.** There may be times when you are working with materials for which experimenting with these options may be useful.\n",
    "\n",
    "Note that it's possible to customize the `oem` and `psm` together. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fd0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the numbers below to try\n",
    "# running other modes together.\n",
    "custom_oem_psm_config = r'--oem 3 --psm 4'\n",
    "\n",
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR)\n",
    "# following the language and mode configuration we specify,\n",
    "# and then print the results for us to see here.\n",
    "print(\n",
    "    pytesseract.image_to_string(\n",
    "        Image.open(\"./data/ocr_sample.jpeg\"),\n",
    "        lang=\"eng\",\n",
    "        config=custom_oem_psm_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64a88b",
   "metadata": {},
   "source": [
    "### File Formats\n",
    "\n",
    "In addition to .txt, Tesseract can convert OCR'ed images into [hOCR (HTML)](https://en.wikipedia.org/wiki/HOCR), searchable PDF, and TSV.\n",
    "\n",
    "___\n",
    "<h3 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h3>\n",
    "\n",
    "The scripts below output various file formats. Try each and then click the file link below each script to view the output. You'll also find the files by clicking on the Jupyter icon at the top of this window.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6186fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to text file (.txt)\n",
    "\n",
    "# File location\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"./data/ocr_sample.jpeg\"\n",
    "\n",
    "# Open the file named above. \n",
    "# While it's open, do several things:\n",
    "with open(file, 'rb') as inputFile:\n",
    "        \n",
    "    # Read the file using PIL's Image module.\n",
    "    img = Image.open(inputFile)\n",
    "    \n",
    "    # Run OCR on the open file.\n",
    "    ocrText = pytesseract.image_to_string(img)\n",
    "        \n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "    \n",
    "fileName = file.rsplit('/', 1)[-1]\n",
    "fileName = fileName.rsplit('.', 1)[0]\n",
    "\n",
    "# The image file above will be closed before moving on to this line.\n",
    "# The OCR'ed text has been pulled from the image and stored in\n",
    "# a Python variable for us to continue to use.\n",
    "\n",
    "# Create and open a new text file, name it to match its input file,\n",
    "# declare its encoding to be UTF-8 so that it correctly outputs\n",
    "# non-ASCII characters.\n",
    "with open(fileName + \".txt\", \"w\", encoding=\"utf-8\") as outFile:\n",
    "        \n",
    "    # and write the OCR'ed text to the file.\n",
    "    outFile.write(ocrText)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"text file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5059587",
   "metadata": {},
   "source": [
    "To open this file, use the file menu to select: **File > Open**. You may then choose to view, edit, modify, or download the file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946aede1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to pdf file (.pdf)\n",
    "\n",
    "# File location\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"./data/ocr_sample.jpeg\"\n",
    "\n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "fileName = file.rsplit('/', 1)[-1]\n",
    "fileName = fileName.rsplit('.', 1)[0]\n",
    "\n",
    "# Run OCR on an image file and save it as a PDF object (not file)\n",
    "# within Python.\n",
    "pdf = pytesseract.image_to_pdf_or_hocr(file, extension='pdf')\n",
    "\n",
    "# Create a new empty pdf.\n",
    "with open(fileName + \".pdf\", 'w+b') as f:\n",
    "    \n",
    "    # Save the PDF object to the new empty PDF file.\n",
    "    f.write(pdf)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"PDF successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2def4a5",
   "metadata": {},
   "source": [
    "To open this file, use the file menu to select: **File > Open**. You may then choose to view, edit, modify, or download the file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to html file (.html)\n",
    "\n",
    "# File location\n",
    "# You can change the filename in quotes below to OCR a different file.\n",
    "file = \"./data/ocr_sample.jpeg\"\n",
    "\n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "fileName = file.rsplit('/', 1)[-1]\n",
    "fileName = fileName.rsplit('.', 1)[0]\n",
    "\n",
    "# Run OCR on an image file and save it as an HTML object (not file)\n",
    "# within Python.\n",
    "hocr = pytesseract.image_to_pdf_or_hocr(file, extension='hocr')\n",
    "\n",
    "# Create a new empty HTML file. Open it in \"w+b\" mode.\n",
    "# \"w+b\" is a mode that tells Python to write whatever\n",
    "# data we give to a file in binary mode--meaning that \n",
    "# it will not apply any encoding or try to translate\n",
    "# a non-ASCII character to an ASCII character.\n",
    "with open(fileName + \".html\", 'w+b') as f:\n",
    "    \n",
    "    # Save the PDF object to the new empty PDF file.\n",
    "    f.write(hocr)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"HTML successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531cdc27",
   "metadata": {},
   "source": [
    "To open this file, use the file menu to select: **File > Open**. You may then choose to view, edit, modify, or download the file to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b82636",
   "metadata": {},
   "source": [
    "### Languages\n",
    "\n",
    "If we do not include `lang=\"eng\"` when we run the above code, Tesseract will *assume* English. Run the following to get a list of all the language codes. [A table of these is available here.](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of languages in their 3-letter codes supported by Tesseract.\n",
    "print(pytesseract.get_languages(config=''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063d04a",
   "metadata": {},
   "source": [
    "___\n",
    "<h3 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h3>\n",
    "\n",
    "Try OCR'ing the first page from Gabriel Garcia-Marquez's *Cien Años de Soledad* (*One Hundred Years of Solitude*): \n",
    "\n",
    "![The first page from one hundred years of solitude](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/cien-an%CC%83os-de-soledad.png) \n",
    "\n",
    "---\n",
    "The following code will download the file automatically. Can you figure out how to change the [3-letter language code](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html) to match the language in the text?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Sample Page Image\n",
    "# Change the list `download_urls` to bring in other documents\n",
    "\n",
    "import urllib.request\n",
    "download_urls = [\n",
    "    'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/cien-an%CC%83os-de-soledad.png'\n",
    "]\n",
    "\n",
    "for url in download_urls:\n",
    "    urllib.request.urlretrieve(url, url.rsplit('/', 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Spanish language support\n",
    "# Change `spa` to match the language code of your choice\n",
    "!apt-get install tesseract-ocr-spa\n",
    "print('Language installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d2449e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "\n",
    "# Try changing the language paramater to 'eng'. Does it change the output?\n",
    "print(pytesseract.image_to_string(Image.open(\"CHANGE-THE-FILENAME-HERE\"), lang=\"CHANGE-LANGUAGE-CODE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f3bd33",
   "metadata": {},
   "source": [
    "___\n",
    "<h3 style=\"color:red; display:inline\">Try it! &lt; / &gt; </h3>\n",
    "\n",
    "Try OCR'ing a page with multiple languages, such as The Bible in Hindi: ([Source](https://archive.org/details/holybibleinhindi00alla)). The syntax will be `lang=\"lan+gua\"` -- replace `lan` and `gua` with the correct language codes. \n",
    "\n",
    "Can you figure out how to bring the document into the Analytics Lab and OCR with two languages?\n",
    "\n",
    "**Note:** The first language will be the \"primary\" language. Try changing the order of the languages to see how the output changes.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235308f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a specific image file, convert the text in the image to computer-readable text (OCR),\n",
    "# and then print the results for us to see here.\n",
    "\n",
    "# REPLACE THE FILE NAME with one of the sample files above. (bible.png)\n",
    "# REPLACE THE LANGUAGE attribute with the correct language code(s).\n",
    "print(pytesseract.image_to_string(Image.open(\"REPLACE_THIS_FILE_NAME.jpg\"), lang=\"lan+gua\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a7199",
   "metadata": {},
   "source": [
    "## Practice: Apply to your own files\n",
    "\n",
    "Use the following code blocks to try OCR'ing various texts. You could use your own files containing digitized texts or locate files to try via [JStor](https://www.jstor.org/), the [Internet Archive](https://archive.org/), [Chronicling America](https://chroniclingamerica.loc.gov/) or other resources. Try texts in different languages, fonts or types, formats, layouts, etc. \n",
    "\n",
    "### 1. Upload your selected text(s) to the `data/` folder in your space in the Constellate Analytics Lab:\n",
    "\n",
    "- Make sure that the texts you select are stored in an image (.jpg, .png, .tiff) format. If you have selected a text with multiple pages, make sure each page is stored in a separate file. *If you have PDF files and are not sure how to generate images from them, bring them to Lesson 02. We'll be looking at how to generate image files together during the lesson.*\n",
    "\n",
    "1. Select **File > Open**\n",
    "2. Navigate to the `data/` folder.\n",
    "3. Select \"Upload\" and then locate and select the file on your local machine.\n",
    "4. If the filename is long, rename it to something simple.\n",
    "5. *Click \"Upload\" again to start the upload.*\n",
    "\n",
    "### 2. Perform OCR on your image file. \n",
    "\n",
    "Use the code blocks above or start fresh below. Change the language attribute to match the text's language. Try out the various settings we looked at above.\n",
    "\n",
    "Below, make sure to replace \"FOLDER NAME/FILE NAME\" with your folder name and specific file name. For example, `./data/MY_FILE_NAME.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name the image file. (Assign it to a variable.)\n",
    "# REPLACE THE FOLDER AND FILE NAME BELOW.\n",
    "file = \"./data/MY_FILE_NAME.jpg\"\n",
    "\n",
    "custom_oem_psm_config = r'--oem 3 --psm 3'\n",
    "\n",
    "# Open the file named above. \n",
    "# While it's open, do several things:\n",
    "with open(file, 'rb') as inputFile:\n",
    "        \n",
    "    # Read the file using PIL's Image module.\n",
    "    img = Image.open(inputFile)\n",
    "    \n",
    "    # Run OCR on the open file.\n",
    "    ocrText = pytesseract.image_to_string(img, lang=\"eng\", config=custom_oem_psm_config)\n",
    "        \n",
    "# Get a file name--without the extension-- \n",
    "# to use when we name the output file.\n",
    "fileName = file.rsplit('/', 1)[-1]\n",
    "fileName = fileName.rsplit('.', 1)[0]\n",
    "\n",
    "# The image file above will be closed before moving on to this line.\n",
    "# The OCR'ed text has been pulled from the image and stored in\n",
    "# a Python variable for us to continue to use.\n",
    "\n",
    "# Create and open a new text file, name it to match its input file,\n",
    "# declare its encoding to be UTF-8 so that it correctly outputs\n",
    "# non-ASCII characters,\n",
    "with open(fileName + \".txt\", \"w\", encoding=\"utf-8\") as outFile:\n",
    "        \n",
    "    # and write the OCR'ed text to the file.\n",
    "    outFile.write(ocrText)\n",
    "\n",
    "# Display a message to let us know the file has been created\n",
    "# and the script successfully completed.\n",
    "print(fileName, \"text file successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d9509",
   "metadata": {},
   "source": [
    "## Resources <a class=\"anchor\" id=\"resources\"></a>\n",
    "---\n",
    "\n",
    "### Jupyter Notebooks Tutorials & Reference\n",
    "\n",
    "- [Jupyter Notebooks documentation](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html)\n",
    "- Jupyter Notebooks keyboard shortcuts: press Esc+H to show a full list.\n",
    "\n",
    "\n",
    "- [\"Markdown for Jupyter notebooks cheatsheet.\"](https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet) *IBM Watson Studio Local.*\n",
    "- Olivia Smith. [\"Markdown in Jupyter Notebook.\"](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) *Data Camp.*\n",
    "\n",
    "\n",
    "### Readings on OCR\n",
    "\n",
    "- Algun, Selcuk. 2018. [\"Review for Tesseract and Kraken OCR for text recognition.\"](Review for Tesseract and Kraken OCR for text recognition) *Data Driven Investor.*\n",
    "- Bakker, Rebecca. [\"OCR for Digital Collections.\"](https://digitalcommons.fiu.edu/cgi/viewcontent.cgi?article=1047&context=glworks) *FIU Digital Commons.*\n",
    "- Baumman, Ryan. [\"Automatic evaluation of OCR quality.\"](https://ryanfb.github.io/etc/2015/03/16/automatic_evaluation_of_ocr_quality.html) */etc.*\n",
    "- Cordell, R. 2017. [\"Q i-jtb the Raven\": Taking Dirty OCR Seriously.\"](https://ryancordell.org/research/qijtb-the-raven/) *Book History*, 20, 188-225.\n",
    "- Cordell, Ryan. 2019. [\"Why You (A Humanist) Should Care About Optical Character Recognition.\"](https://ryancordell.org/research/why-ocr/) *Ryan Cordell.* \n",
    "- Coyle, Karen. [\"Digital Urtext.\"](https://kcoyle.blogspot.com/2012/04/digital-urtext.html) *Coyle's InFormation.*\n",
    "- Hawk, Brandon W. [\"OCR and Medieval Manuscripts: Establishing a Baseline.\"](https://brandonwhawk.net/2015/04/20/ocr-and-medieval-manuscripts-establishing-a-baseline/) *Brandon W. Hawk.* (This post is a comparison of ABBYY FineReader & Adobe Acrobat OCR technologies as applied to medieval texts.)\n",
    "- Holley, Rose. 2009. [How Good Can It Get? Analysing and Improving OCR Accuracy in Large Scale Historic Newspaper Digitisation Programs,\"](http://www.dlib.org/dlib/march09/holley/03holley.html) *D-Lib Magazine* 15, no. 3/4.\n",
    "- Milligan, I. 2013. [\"Illusionary Order: Online Databases, Optical Character Recognition, and Canadian History, 1997–2010.](https://www.muse.jhu.edu/article/527016) *The Canadian Historical Review* 94(4), 540-569.\n",
    "- Smith, David, and Ryan Cordell. 2018. [\"A Research Agenda for Historical and Multilingual Optical Character Recognition.\"](http://hdl.handle.net/2047/D20297452)\n",
    "- Smith, Ray. 2007. [\"An Overview of the Tesseract OCR Engine.\"](https://tesseract-ocr.github.io/docs/tesseracticdar2007.pdf)\n",
    "- Smith, Ray, Daria Antonova, and Dar-Shyang Lee. 2009. [\"Adapting the Tesseract open source OCR engine for multilingual OCR.\"](https://dl.acm.org/doi/10.1145/1577802.1577804) MOCR '09: Proceedings of the International Workshop on Multilingual OCR.\n",
    "- Tanner, Simon. [\"Deciding whether Optical Character Recognition is feasible.\"](https://www.kb.nl/sites/default/files/docs/OCRFeasibility_final.pdf) *King's Digital Consultancy Services.*\n",
    "\n",
    "\n",
    "### OCR Tutorials & Reference\n",
    "\n",
    "*The following is a list of tutorials that include different scholars' approaches to OCR. Some also use Tesseract, but most use different scripting or programming languages. There is no single best way to do OCR, so if you have the time they worth trying to see which works best for your project.*\n",
    "\n",
    "- Aidan. [\"OCR with Python.\"](https://medhieval.com/classes/hh2019/blog/ocr-with-python/) *Hacking the Humanities 2019.*\n",
    "- Akhlaghi, Andrew. [\"OCR and Machine Translation.\"](http://programminghistorian.org/en/lessons/OCR-and-Machine-Translation) *The Programming Historian.* (Note that this tutorial uses Tesseract but works with the bash scripting language instead of Python.)\n",
    "- Baumman, Ryan. [\"Command-Line OCR with Tesseract on Mac OS X.\"](https://ryanfb.github.io/etc/2014/11/13/command_line_ocr_on_mac_os_x.html) */etc.*\n",
    "- Dull, Joshua. [\"Text Recognition with Adobe Acrobat and ABBYY FineReader.\"](https://github.com/JoshuaDull/Text-Recognition-Introduction/)\n",
    "- Graham, Shawn. [\"Extracting Text from PDFs; Doing OCR; all within R.\"](https://electricarchaeology.ca/2014/07/15/doing-ocr-within-r/) *Electric Archaeology.* (This blog post describes a method for OCR using the R programming language.)\n",
    "- Mähr, Moritz. [\"Working with batches of PDF files.\"](https://programminghistorian.org/en/lessons/working-with-batches-of-pdf-files) *The Programming Historian.* (Note that this tutorial uses Tesseract and works in the command line without Python.)\n",
    "- Shperber, Gidi. [\"A gentle introduction to OCR.\"](https://towardsdatascience.com/a-gentle-introduction-to-ocr-ee1469a201aa) *Toward Data Science.* October 22, 2018.\n",
    "- Tarnopol, Rebecca. [\"How to OCR Documents for Free in Google Drive.\"](https://business.tutsplus.com/tutorials/how-to-ocr-documents-for-free-in-google-drive--cms-20460) *TutsPlus.*\n",
    "\n",
    "\n",
    "- [**PyTesseract documentation**](https://github.com/madmaze/pytesseract)\n",
    "- [**Tesseract documentation**](https://tesseract-ocr.github.io/)\n",
    "\n",
    "\n",
    "### Additional Reading\n",
    "\n",
    "- Rockwell, Geoffrey, and Stéfan Sinclair. 2016. [*Hermeneutica: Computer Assisted Interpretation in the Humanities.*](http://hermeneuti.ca/)\n",
    "- Underwood, Ted. [\"The challenges of digital work on early-19c collections.\"](https://tedunderwood.com/2011/10/07/the-challenges-of-digital-work-on-early-19c-collections/) *The Stone and the Shell.*\n",
    "- [TranScriptorium's handwritten text recognition project results.](https://cordis.europa.eu/project/id/600707/results)\n",
    "- [\"How to Transcribe Documents with Transkribus - Introduction.\"](https://readcoop.eu/transkribus/howto/how-to-transcribe-documents-with-transkribus-introduction/) *Read Coop.*\n",
    "\n",
    "- **[Basics of Fair Use](https://copyright.columbia.edu/basics/fair-use.html)** from Columbia University.\n",
    "\n",
    "\n",
    "### Additional Tutorials -- Thanks to everyone who contributed!\n",
    "\n",
    "- [\"Social Network Analysis from Theory to Applications with Python.\"](https://towardsdatascience.com/social-network-analysis-from-theory-to-applications-with-python-d12e9a34c2c7) *Toward Data Science.*\n",
    "- [TAP Institute Pandas course materials](https://nkelber.github.io/tapi2021/book/courses/pandas.html) -- for \"cleaning\" data using Python.\n",
    "- [\"Cleaning Data with Open Refine.\"](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine) *The Programming Historian.*\n",
    "- [Working with Files in Python](https://automatetheboringstuff.com/chapter8/) from *Automate the Boring Stuff*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
