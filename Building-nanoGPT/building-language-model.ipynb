{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a4c380-766c-443c-8f90-d8ab0bb14562",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "This notebook is created by Zhuo Chen under [Creative Commons CC-BY license](https://creativecommons.org/licenses/by/4.0/) based on [the nanoGPT](https://github.com/karpathy/nanoGPT) created by [Andrej Karpathy](https://karpathy.ai) under [MIT license](https://github.com/karpathy/nanoGPT/blob/master/LICENSE).<br />\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "___\n",
    "\n",
    "\n",
    "# Building a Language Model\n",
    "\n",
    "**Description:** This notebook describes:\n",
    "* how to use PyTorch to build a character-level nanoGPT \n",
    "* how to train the nanoGPT on a tiny piece of Shakespeare\n",
    "* how to use the trained model to generate more Shakespeare-like text\n",
    "* Explore how the transformer architecture is like in code\n",
    " \n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Advanced\n",
    "\n",
    "**Completion Time:** 180 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "* Python Intermediate Series ([Start Python Intermediate 1](../Python-intermediate/python-intermediate-1.ipynb))\n",
    "* Introduction to ChatGPT([Start learning how ChatGPT works](https://join.slack.com/t/ithaka-constellate/shared_invite/zt-2bg6ctcqb-wf~4KVBB6QkE7Q2PdCNy3Q))\n",
    "\n",
    "**Knowledge Recommended:** None\n",
    "\n",
    "**Data Format:** .txt\n",
    "\n",
    "**Libraries Used:** PyTorch\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf21d6-e021-4cad-a169-db15f9bf0788",
   "metadata": {},
   "source": [
    "## Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc36db-828e-4b3d-80c1-c1acecd3bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the most recent version available via pip\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a2bd4-2258-4a92-a649-d04b7b227ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ba6da8-9848-42e2-95b2-ff401da21932",
   "metadata": {},
   "source": [
    "## Download the Shakespeare dataset\n",
    "\n",
    "Let's download the text file containing the data we will use to train our model. The file contains Shakespeare works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5271fd0c-0309-4094-bcb9-2478dba5a742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# download the Shakespeare data\n",
    "url=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "urllib.request.urlretrieve(url, './data/shake.txt')\n",
    "\n",
    "# Success message\n",
    "print('Sample file ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56d930-62df-4dd2-9daa-c3dde2ac60c5",
   "metadata": {},
   "source": [
    "## Prepare the Shakespeare dataset for character-level language modeling\n",
    "In this section, we will get the following data from the downloaded Shakespeare dataset:\n",
    "* all the unique characters that occur in the dataset, i.e. the vocab\n",
    "* a mapping from characters to integers and vice versa, i.e. an encoder and decoder\n",
    "* a training dataset and a validation dataset, i.e. train/val split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03bd3a-2d85-4ffb-b8b4-8b8e4dbb50fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-28T20:55:50.041805Z",
     "iopub.status.busy": "2023-11-28T20:55:50.041495Z",
     "iopub.status.idle": "2023-11-28T20:55:50.044957Z",
     "shell.execute_reply": "2023-11-28T20:55:50.044335Z",
     "shell.execute_reply.started": "2023-11-28T20:55:50.041786Z"
    },
    "tags": []
   },
   "source": [
    "### Get all unique characters\n",
    "We are building a character-level language model. This means the model, when generating new data, will use past characters to predict future characters. The training data and validation data are from the downloaded Shakespeare dataset. The unique characters from the Shakespeare dataset form the vocabulary of the model. Each character is a token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401db592-e0f0-4ff7-accd-207304bb2a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "with open('./data/shake.txt', 'r') as f:\n",
    "    content = f.read()\n",
    "print(f\"length of dataset in characters: {len(content):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818bef5d-97df-4453-bef1-8b9b32a1a72f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get all unique chars\n",
    "chars = sorted(list(set(content)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7775c6-7d09-49a1-ac0d-f97e57d4ffb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get one character token from the list\n",
    "chars[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c20a7ec-d695-49b4-967c-470c9bc59ef8",
   "metadata": {},
   "source": [
    "### Create a mapping from characters to integers and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc08836-ee2d-4683-92c1-826bc5eb230a",
   "metadata": {},
   "source": [
    "Computers process numbers, not letters or characters. This means that ultimately, each character in the vocab will be encoded as a number for the computer to process. Let's make an encoder that encodes characters into numbers. Also, let's make a decoder that decodes numbers into characters. The simplest choice is just to use the index number of the characters in the list `chars` we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75693320-9b64-479d-886a-2994855c6791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a quick reminder of enumerate()\n",
    "for i,ch in enumerate(['a', 'b', 'c']):\n",
    "    print(i, ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667b495-316b-4138-b7d6-2e6713b6a8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a character to int mapping\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "# create a int to character mapping\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "# create a encoder and decoder\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51ada1-9f50-43b0-a95d-4605a86e6520",
   "metadata": {},
   "source": [
    "You can try the encoder and decoder using an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6923585-25aa-4263-bb87-3037effdd4c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try the encoder\n",
    "ls = encode('This is nanoGPT')\n",
    "print(ls)\n",
    "# try the decoder\n",
    "string = decode(ls)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d42b68-75d7-43d6-8dc6-b5ebf79b914c",
   "metadata": {},
   "source": [
    "At this point, for each character in the vocab, you have an integer that maps to it. Basically, each character is indexed with an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383198e2-a7a5-4ccb-a582-f4c32d0b5f48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-22T02:57:52.538545Z",
     "iopub.status.busy": "2023-11-22T02:57:52.538193Z",
     "iopub.status.idle": "2023-11-22T02:57:55.183271Z",
     "shell.execute_reply": "2023-11-22T02:57:55.182722Z",
     "shell.execute_reply.started": "2023-11-22T02:57:52.538516Z"
    },
    "tags": []
   },
   "source": [
    "### Split the dataset into training data and validation data\n",
    "The majority, i.e. 90%, of the Shakespeare dataset will be used as training data. The rest will be used as validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ac1a4-5e6b-4e70-9ba1-4a002ca66333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the train and val splits\n",
    "data = torch.tensor(encode(content), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f37ffc-b856-4841-9fe3-b753c1bf4fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add context window\n",
    "context_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eeaec0-6fba-434d-86f5-0380d1ed7a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### see what training data gives us\n",
    "\n",
    "x = train_data[:context_size] # get one sequence of train_data\n",
    "y = train_data[1:context_size+1] # get one sequence of gold answer from train_data\n",
    "for t in range(context_size):\n",
    "    context = x[:t+1].tolist()\n",
    "    target = [y[t].item()]\n",
    "    print(f\"{decode(context)} -> {decode(target)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c221bc-5b9b-4e64-8c9d-735505a45926",
   "metadata": {},
   "source": [
    "## Write the model\n",
    "In this section, we will use the training data we get from the Shakespeare dataset to train a character-level language model. \n",
    "### Set the hyperparameters\n",
    "Let's spend some time understanding the hyperparameters. \n",
    "* batch_size: this hyperparameter determines how many independent sequences the model will process in parallel\n",
    "* block_size: the name might sound unfamiliar, but it actually means context window, i.e. the maximum context length we use to predict the next token\n",
    "* max_iters: total steps in optimization\n",
    "* eval_interval: interval where we will print the loss\n",
    "* learning_rate: how much we move in a step when minimizing the loss\n",
    "* eval_iters: evaluate every eval_iters batches\n",
    "* n_embd: number of dimensions in a token embedding\n",
    "* n_head: number of heads in a self-attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f412f1ac-1dd0-4367-b422-6f9e999ca81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import modules from pytorch\n",
    "import torch.nn as nn # nn is the module for creating neural network\n",
    "from torch.nn import functional as F # we'll use softmax and cross_entropy from F\n",
    "\n",
    "# set the hyperparameters of the model\n",
    "batch_size = 16 # how many independent sequences of chars model process in parallel\n",
    "block_size = 32 # maximum context length for predictions\n",
    "max_iters = 5000 # total steps in optimization (move one step after processing every eval_iters batches)\n",
    "eval_interval = 100 # print the loss of the model every 100 steps in optimization \n",
    "learning_rate = 1e-3 # how big a step in optimizatin is\n",
    "device = 'cpu'\n",
    "eval_iters = 200 # calculate a mean loss after processing every 200 batches\n",
    "n_embd = 64 # size of the token embedding \n",
    "n_head = 4 # num of heads in a self-attention layer\n",
    "\n",
    "torch.manual_seed(1337) # ensure we all get the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03eb2e0-e8e8-4e4c-9c38-ed5596ec8458",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "We define two functions. \n",
    "The first function loads a batch of data for training or for evaluation. \n",
    "The second function estimates loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa16953-023c-48af-9e32-1334ec74ba91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load one batch of data\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # ix is a tensor of indices, each is the index of the starting char of a sequence in a batch\n",
    "    ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,)) \n",
    "    # x is the batch_size sequences used to predict the next character,each sequence is block_size chars long\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # y is the batch_size sequences used as gold answers, each sequence is block_size char long \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # data and model need to be on the same device, cpu or gpu\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fac147-a9c2-4824-a6ec-18529777bbc8",
   "metadata": {},
   "source": [
    "Recall that `block_size`=32 and `batch_size`=16. Therefore, x is of shape (16, 32) and y is of shape (16, 32) as well.\n",
    "Each time we run get_batch, we will get 16 sequences of characters and they are of length 32. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e30451-8e5d-4357-9b4d-2114949fd692",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### get shape of the x and y\n",
    "ix = torch.randint(low=0, high=len(data) - block_size, size=(batch_size,)) \n",
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "print(x.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e421f6e6-d91d-4b20-ba58-a596f739cad2",
   "metadata": {},
   "source": [
    "Let's use a visualization to help us understand a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b818be2-4a65-429c-93e2-df4dd582237e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimate loss\n",
    "@torch.no_grad() # don't calculate the gradients; intended to improve efficiency\n",
    "def estimate_loss():\n",
    "    \"\"\"take eval_iters many batches; calculate loss on each batch and \n",
    "    take the mean loss and return the mean loss of the eval_iters batches\"\"\"\n",
    "    out = {}\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    for split in ['train', 'val']: \n",
    "        losses = torch.zeros(eval_iters) # place holders for losses\n",
    "        for k in range(eval_iters): \n",
    "            X, Y = get_batch(split) # get one batch of data\n",
    "            logits, loss = model(X, Y) # logits are the raw values, loss is the loss\n",
    "            losses[k] = loss.item() # record the loss at kth batch\n",
    "        out[split] = losses.mean() # get mean of the eval_iters losses\n",
    "    model.train() # set the model to train mode\n",
    "    return out # return train batches mean loss/eval batches mean loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11052c0-1951-4892-984a-c25de38972d8",
   "metadata": {},
   "source": [
    "Recall that `eval_iters`=200 and `batch_size`=16. The model will take 16 random sequences (one batch) from training data for 200 times (200 batches in total), calculate loss 200 times (one for each batch) and take the mean of these 200 losses as the loss of the 200 training samples. Then, it will take 16 random sequences from the evaluation data for 200 times, calculate loss 200 times and take the mean of these 200 losses as the loss of the evaluation samples. The mean losses will be printed out to track the state of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2ace8-5789-4a76-bbac-1562b5989705",
   "metadata": {},
   "source": [
    "### Define a multi-head self-attention block\n",
    "A self-attention block in transformer consists of a multi-head self-attention layer and a feedforward layer. \n",
    "#### Define a single self-attention head\n",
    "Let's define a single self-attention head first. \n",
    "Recall from the `How does ChatGPT work` webinar we have learned a single self-attention head works in the following way:\n",
    "\n",
    "<img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/nanoGPT_singleHead.png\" width=500>\n",
    "\n",
    "1. To use a sequence of tokens to predict the following token, a self-attention head encodes a relation between the tokens in the following way:\n",
    "    * each token t is associated with a query vector, a key vector and a value vector\n",
    "    * query vector **q<sup>t</sup>** represents what the token is looking for\n",
    "    * key vector **k<sup>t</sup>** represents what kind of information the token contains\n",
    "    * value vector **v<sup>t</sup>** represents the actual information the token has\n",
    "    \n",
    "If we have a sequence of tokens, each of which has a query, key vector and a value vector associated with it, then we'll get a query matrix **q**, key matrix **k** and value matrix **v** from the sequence. \n",
    "\n",
    "**q** * **k<sup>T</sup>** gives us a matrix of attention scores; bigger scores means better match between the query vector and key vector of two tokens; for example, if **q<sup>t8</sup>** * **k<sup>t2</sup>** is a big number, this means t2 provides a lot of information that t8 is looking for. \n",
    "\n",
    "\n",
    "2. Language is assumed to have a temporal dimension which determines that the model can only use past tokens to predict future tokens. This means that not all attention scores in the matrix **q** * **k<sup>T</sup>** will be considered. For example, we will not consider **q<sup>t2</sup>** * **k<sup>t8</sup>** because t8 is a future token of t2 and we cannot use t8 to predict t2!\n",
    "\n",
    "3. The matrix of attention scores will be converted to a matrix of attention weights using the softmax function. After that, we'll use the weight matrix to do weighted aggregation of values from the value matrix **v**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8626d41-5533-49c0-8c5f-f4d293b75cae",
   "metadata": {},
   "source": [
    "Even with what we have learned in the `How does ChatGPT work?` webinar, this code cell still looks daunting. So let's use some visualizations to help us understand it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee75fa-f3dd-47cb-8c24-97732a8448be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module): # nn.Module is base class for all neural network modules\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()  # inherit __init__() from base class\n",
    "        # n_embed is num of dimensions in token embedding\n",
    "        # head_size is num of neurons in the respective key/query/value layer\n",
    "        # if an input batch matrix is (16, n_embd), key layer outputs a (16, head_size) matrix\n",
    "        # bias=False means the model will not learn a bias term\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # a matrix of (n_embd, head_size) \n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # register_buffer will be used to mask the attention score matrix\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x): # x is a batch of token sequences \n",
    "        # x is a matrix of shape (B, T, C) where B is batch_size, T is block_size, C is n_embd\n",
    "        B,T,C = x.shape \n",
    "        k = self.key(x)   # k is of shape (B, T, head_size) \n",
    "        q = self.query(x) # q is of shape (B, T, head_size)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        # change all values in wei in the same positions as the 0s in tril to -inf\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        # softmax applied along the last dimension of wei\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T , head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63589f4a-c7da-4325-ae23-53c2a191eb9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Understand how tril does masking\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205faeaf-76b9-40a5-8349-ccba5dd85659",
   "metadata": {},
   "source": [
    "#### Define multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e6a17-ce42-456a-9146-1d7381c88129",
   "metadata": {},
   "source": [
    "Now we are ready to build a multi-head self-attention layer using the Head class we created. Each head outputs a cuboid of shape (B, T, head_size). In total, we have num_heads many heads. A multi-head attention layer will concatenate the output into one cuboid of shape (B, T, num_heads * head_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c730db-62d6-4016-80a9-499906e180e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size): # num_heads=how many heads there are in a sa layer\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for num in range(num_heads)]) # a list of heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        # concatenate the outputs from the heads  \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # x is a batch of data, h(x) is calling the forward() in Head\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487bb4e-2daf-4c04-82bd-73265e0f3db6",
   "metadata": {},
   "source": [
    "Let's also use a visualization to help us understand a multi-head self-attention layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cdf73e-4c04-4df0-b3b5-511890cac5f5",
   "metadata": {},
   "source": [
    "#### Define a feedforward layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77d359-7899-48e2-9d2b-37af0550ea24",
   "metadata": {},
   "source": [
    "After a self-attention layer, we have a feedforward layer to basically process the communication between the characters that we get from self-attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a90897-1cfe-4a23-95e8-04ef6bd5dbac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd), # a layer that does linear computation\n",
    "            nn.ReLU(), # Use ReLU to add non-linearity\n",
    "            nn.Linear(4 * n_embd, n_embd), \n",
    "        )\n",
    "\n",
    "    def forward(self, x): # x is a batch of data\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6774d6-9625-4294-9443-312f35821dad",
   "metadata": {},
   "source": [
    "#### Putting everything into a self-attention block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c1926-1032-428d-a927-c2b8c381da13",
   "metadata": {},
   "source": [
    "A self-attention layer and a feedforward layer together forms a self-attention block in transformer. In a transformer, we can have multiple such self-attention blocks. \n",
    "\n",
    "<img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/ChatGPT_blocks.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db85a2d-12d5-4d88-8cec-5c5d05f0d976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\" \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: num of embedding dimensions, n_head: the number of heads\n",
    "        super().__init__()\n",
    "        # each head in charge of communication between tokens in only part of the dimensions of token embeddings\n",
    "        head_size = n_embd // n_head \n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size) \n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # layer normalization, n_embd is the normalized shape\n",
    "        self.ln2 = nn.LayerNorm(n_embd)        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))  # (B, T, C)\n",
    "        x = x + self.ffwd(self.ln2(x)) # (B, T, C)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47e61e-c814-4fcf-ae50-3863427589a6",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "All the components are ready. Let's put them together to create a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78355aa-0cd3-4c53-8d54-ed106f6faa95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential( # 4 Blocks\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            Block(n_embd, n_head=n_head),\n",
    "            Block(n_embd, n_head=n_head)                                 \n",
    "        )                  \n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape # the front side of the cuboid of a batch we saw in the slides\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, vocab_size = logits.shape \n",
    "            logits = logits.view(B*T, vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, vocab_size)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, vocab_size)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246018bd-25cb-4152-9b1d-c872231cc8ac",
   "metadata": {},
   "source": [
    "## Train the model and generate some new data\n",
    "\n",
    "We can now train the model. After we are done training, we can use it to generate some new Shakespeare-like text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f7d0e-6e59-4069-b3b1-ef466f317235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = NanoGPT()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # initialize the gradients to None\n",
    "    loss.backward() # backpropagation\n",
    "    optimizer.step() # Update parameters based on current gradients\n",
    "\n",
    "# generate from the model\n",
    "context = torch.tensor([encode(\"God save his majesty!\")], device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "py313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
