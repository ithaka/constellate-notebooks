{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Erik Fredner](https://fredner.org) for the 2024 Text Analysis Pedagogy Institute. Revised and expanded by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# Automated Text Classification Using LLMs 1\n",
    "\n",
    "**Description:** This notebook describes:\n",
    "\n",
    "* How to create a project Open AI project API key \n",
    "* How to interact with the OpenAI API\n",
    "* How to do automated text classification using OpenAI API\n",
    "\n",
    "**Use Case:** For Learners and Researchers\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 90 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics 1](../Python-basics/python-basics-1.ipynb))\n",
    "* Python Intermediate Series ([Start Python Intermediate 1](../Python-intermediate/python-intermediate-1.ipynb))\n",
    "* Introduction to LLMs ([Start Intro to LLMs 1](https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/March+20+2024_+How+ChatGPT+works+(Session+1).pdf))\n",
    "\n",
    "**Knowledge Recommended:** Experience with LLM chatbot (e.g. ChatGPT)\n",
    "\n",
    "**Data Format:** JSON\n",
    "\n",
    "**Libraries Used:** openai, dotenv, csv, JSON\n",
    "\n",
    "**Research Pipeline:** \n",
    "1. Play with LLMs if you have not already.\n",
    "2. Test using a chatbot interface for an LLM (like ChatGPT) to perform relevant classifications for your research.\n",
    "3. Evaluate initial results.\n",
    "4. Learn how to interact with an API through this notebook.\n",
    "5. Modify your initial experiments based on what we cover."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "## Create and fund a project API key on OpenAI\n",
    "In order to interact with OpenAI API, we need to create a project API key. In the following, you will find the step-by-step instruction of creating an API key on OpenAI. \n",
    "\n",
    "### Create an API key\n",
    "#### Go to the [OpenAI developer platform](https://platform.openai.com/)\n",
    " \n",
    "If you already have a ChatGPT account, log into your account; if you don't have one yet, sign up for an account.\n",
    "\n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_openai_platform.png\">\n",
    "\n",
    "#### Go to the dashboard\n",
    "  \n",
    "After you are logged in, go to the dashboard.\n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_openai_dashboard.png\">\n",
    "\n",
    "####  Create a new secret key\n",
    "  \n",
    "Click on the green button to create a new key. Give a name to your own key. \n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_create_new_key.png\">\n",
    "\n",
    "#### Save the new key to a text file or other accessible place\n",
    "\n",
    "After you create a new key, you will not be able to view it on your OpenAI account when you come back for security reasons. Therefore, you will need to save your key to a secure and accessible place, e.g. a text file. \n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_save_key.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754c705-2519-4e21-8258-cc224dd43258",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T21:06:41.453676Z",
     "iopub.status.busy": "2024-09-10T21:06:41.453396Z",
     "iopub.status.idle": "2024-09-10T21:06:41.457160Z",
     "shell.execute_reply": "2024-09-10T21:06:41.456560Z",
     "shell.execute_reply.started": "2024-09-10T21:06:41.453657Z"
    }
   },
   "source": [
    "### Add credits to your key\n",
    "To prompt the OpenAI API to do a task, we will need to add credits to the key. The cost is by token. You can think of tokens as pieces of words. According to the pricing page, 1,000 tokens is about 750 words.\n",
    "\n",
    "Pricing is available [on this page](https://openai.com/api/pricing/)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "829e4e53-769f-4fe4-9141-d7434acb374a",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-10T21:06:53.416907Z",
     "iopub.status.busy": "2024-09-10T21:06:53.416623Z",
     "iopub.status.idle": "2024-09-10T21:06:53.420294Z",
     "shell.execute_reply": "2024-09-10T21:06:53.419773Z",
     "shell.execute_reply.started": "2024-09-10T21:06:53.416888Z"
    },
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "| Model         | Type   | Price per 1 million tokens   |\n",
    "|:--------------|:-------|:-----------------------------|\n",
    "| gpt-4o        | input  | $5                           |\n",
    "| gpt-4o        | output | $5                           |\n",
    "| gpt-3.5-turbo | input  | $0.50                        |\n",
    "| gpt-3.5-turbo | output | $1.50                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f928c-7ea1-4850-8eb6-a00ea2cfe2f5",
   "metadata": {},
   "source": [
    "#### Add a payment method\n",
    "Go to **Settings** on the top menu, and then go to **Billings** in the side bar. Under the **Payment methods** tab, you will find the **Add payment method** button.  \n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_billing.png\">\n",
    "\n",
    "#### Add credits to your key\n",
    "After you add a credit card to your account, you can go to the **Overview** tab to find the **Add to credit balance** button. Click on the button to specify how much credits you would like to add to the key. You can start with $5, which is the minimum required amount by OpenAI, for this class. \n",
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/text_classification_add_credits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390534f2-ba82-4135-ad5a-d837308a1a53",
   "metadata": {},
   "source": [
    "Now that we have a project API key with credits, we can go ahead and prepare for the API interaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784f0aa-8fc7-4a1b-827c-5e94df072395",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "To interact with the OpenAI API, we need to install three packages. \n",
    "* [openai](https://pypi.org/project/openai/)\n",
    "\n",
    "The OpenAI Python library provides convenient access to the OpenAI API from applications written in the Python language. \n",
    "\n",
    "* [python-dotenv](https://pypi.org/project/python-dotenv/)\n",
    "\n",
    "python-dotenv reads key-value pairs from a ```.env``` file and can set them as environment variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install Libraries ###\n",
    "%pip install --upgrade openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f491ac-322a-4450-a89e-842f87f8ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "from openai import OpenAI # import the OpenAI class from the openai library\n",
    "from dotenv import load_dotenv # to load the API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f09abf-555a-48fd-964b-f273e58bd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set the httpx logger to only show WARNING and above\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6849b8-fbbc-4bfa-bef3-9874cf64d1fe",
   "metadata": {},
   "source": [
    "## Set the API key\n",
    "Next, we will add the API key we generated before to the ```.env``` file in our working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180deab-52e2-44ac-b242-4b5b9c927633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the generated API key to a variable\n",
    "OPENAI_API_KEY = \" \"  # copy-paste the class key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04428ef2-31bb-4361-8fd7-0bcdef2046f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the API key into the .env file\n",
    "with open(\".env\", \"w\") as f: # open the .env file in write mode\n",
    "    f.write(f\"OPENAI_API_KEY={OPENAI_API_KEY}\") # write the API key into the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779f979-d5ab-415d-8288-26a1d70ad6d0",
   "metadata": {},
   "source": [
    "You might be wondering, where is the ```.env``` file? Note that files with names starting with a dot '.' are invisible. If you would like to confirm that we do have a ```.env``` file in our working directory, you can use a terminal command to do that.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bbead-af62-43d7-8de3-438468a42b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the working directory, including the invisible files\n",
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7899b0f9-e92d-42e8-b25b-b00fcc0610b5",
   "metadata": {},
   "source": [
    "If you would like to confirm further that the API key has been written into the ```.env``` file, you can open the file and read the content in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbad8f6-1cc3-4349-806f-60a067f21528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the .env file\n",
    "with open(\".env\", \"r\") as f: # open the .env file\n",
    "    content = f.read() # read the content in the .env file\n",
    "    print(content) # print out the content "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f3270",
   "metadata": {},
   "source": [
    "Next time you restart this notebook kernel (or open up a new notebook), the `openai` library will read the API key directly from your `.env` file. No need to specify the `api_key=` argument in `OpenAI()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62ae52",
   "metadata": {},
   "source": [
    "# Making your first API call\n",
    "\n",
    "The API calls can be found in the [OpenAI's tutorial](https://platform.openai.com/docs/api-reference/chat/create). Let's load the API key we stored in the ```.env``` file and use it to try a chat completion task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86708558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the API key and set the client to the OpenAI class\n",
    "load_dotenv() # load the environment variables in the .env file, in our case, the API key\n",
    "client = OpenAI() # create an OpenAI instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51913b24-40b2-4b80-ac0b-7a4a418601e2",
   "metadata": {},
   "source": [
    "We can get a list of models available in the OpenAI API by running the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16536419-a3c2-4a4d-b345-e0db6a11e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get a list of all models in OpenAI API\n",
    "models = client.models.list()\n",
    "for model in models.data:\n",
    "    print(model.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aaf0c2-f4b3-427b-970d-60626feb8257",
   "metadata": {},
   "source": [
    "Let's try a simple chat completion task with gpt-4o-2024-08-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interact with the OpenAI API\n",
    "# using a text completion task\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\", # set the model to gpt-4o-2024-08-06, you can try other models as well\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", # system message passed to the API, tell the LLM to be a role\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", # user prompt\n",
    "            \"content\": \"Explain what text classification is in ten or fewer words.\",\n",
    "        },\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870a0ee-2a8c-437d-96de-b7a1b62e59bc",
   "metadata": {},
   "source": [
    " As you can see, to prompt the API to do a chat completion task, we need to specify the values for some parameters. \n",
    " \n",
    " * model\n",
    "\n",
    "First of all, we need to specify which model we would like to use for this chat completion task. Here, we choose the model ```gpt-4o-2024-08-06```, but there are more available for you to try, as you can see from the list of models we've got above.\n",
    "\n",
    "* messages\n",
    "  \n",
    "`messages` is a list of dictionaries containing the messages sent to the `model`. There are two kinds of messages we need to specify. First, a system message that specifies what role you would like the LLM to play. Second, a user message which functions as a prompt to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374e684a-4526-4243-b0c6-86e50f556a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what's inside the completion variable\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c01edb-075a-402a-bde9-fb2f666c5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the chat completion given by the LLM\n",
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5994b9d",
   "metadata": {},
   "source": [
    "Congratulations! You've just made your first API call! \n",
    "\n",
    "## Changing the system message\n",
    "As you can see, there are some variations we can get potentially by giving different values to the parameters in the ```client.chat.completions.create()``` function. Let's experiment with it some more by changing the system message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adef169",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_system_message = \"You are a French tutor. Respond to all prompts in French followed by English in parentheses.\"\n",
    "user_message = \"Explain what text classification is in ten or fewer words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e570f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how a different system message conditions a different output chat completion\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": new_system_message},  # new system message\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,  # same user message\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"user: {user_message}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"gpt-4o-2024-08-06: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd6b597",
   "metadata": {},
   "source": [
    "This makes obvious how changing the `system` message impacts how the model responds to subsequent `user` prompts.\n",
    "\n",
    "You might be wondering, what other roles are there available in the API? According to the documentation, there are three different roles in the Chat Completions API. \n",
    "\n",
    "In the ```messages``` parameter, each message object has a role (either ```system```, ```user```, or ```assistant```) and content.\n",
    "\n",
    "* The system message is optional and can be used to set the behavior of the assistant\n",
    "* The user messages provide requests or comments for the assistant to respond to\n",
    "* Assistant messages store previous assistant responses, but can also be written by you to give examples of desired behavior\n",
    "\n",
    "We will see more in a moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77fd6e",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "It's your turn! Can you change the messages you give to the API and see what response you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb3095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a math teacher.\"  # change this!\n",
    "user_message = \"Replace this message with something of your interest.\"  # change this!\n",
    "model = \"gpt-4o-2024-08-06\" # you can try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new system message and user message to get a response\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89823a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"user: {user_message}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{model}: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620ff53-2488-4c4b-88dd-35365082543b",
   "metadata": {},
   "source": [
    "# Classifying texts using OpenAI API\n",
    "In this section, we will use a dataset shared in a research paper on the sentiments of energy reports produced by university-based energy centers as an example to demonstrate how to use the OpenAI API to classify texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26895bd0-d2ca-4edc-b415-8afed60e98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from jsonl file into a dataframe \n",
    "file_path = '../All-sample-files/natural_gas_sents.jsonl'\n",
    "ng_df = pd.read_json(file_path, lines=True)\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55d34d-152f-4920-b37f-d72c10787749",
   "metadata": {},
   "source": [
    "The ```sentiment``` column stores the gold standard for the classification. Of course, in this lesson, we are going to pretend that we don't have the gold standard because otherwise we don't need the LLM to do the work! In the next lesson, we will compare the LLM output to the gold standard. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e757703-96fd-4e87-a361-5f9e68395ad5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Take your JSONL file you built in the Build a Dataset class; select a subset of it to work on in class. \n",
    "\n",
    "Then, read in the data relevant for classification into a dataframe. \n",
    "\n",
    "As an example, I'll use a Constellate dataset to demonstrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ebbec-4a4f-4f45-8803-6917bef5c7ce",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h3>\n",
    "\n",
    "If you are using a Constellate dataset, following the example below. Tweak the code to serve your own purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1398187-8c7d-4d37-94de-180d9c9c2db2",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the JSONL file containing metadata, ngrams and full texts to the current working directory.&lt; / &gt; </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87b188-75fa-405e-85e1-d3d850e3a9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "# path to the jsonl file in the current directory\n",
    "dataset_file = '' # copy and paste the path to the JSONL file \n",
    "\n",
    "# function that reads a jsonl file into a generator\n",
    "def dataset_reader(file_path):\n",
    "    \"\"\"\n",
    "    Helper to read in gzip files and yield Python dictionary\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adfb5d9-8c6a-41a0-8f9c-fc795f8936ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data as a generator \n",
    "dataset = dataset_reader(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e208012a-8ac2-4a19-b87d-90954e82424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sense of the dataset\n",
    "for doc in dataset:\n",
    "    print(doc.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6cb0c-6c45-4abc-80c0-52924bd9e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Use islice to get only the first 500 documents from the dataset generator\n",
    "subset = islice(dataset, 500)\n",
    "\n",
    "# Create lists for year and title using list comprehensions\n",
    "data_of_interest = [(doc['publicationYear'], doc['title']) for doc in subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5f06d-56df-4821-8ac9-8f2dbaeba8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the data of interest into a dataframe\n",
    "constellate_df = pd.DataFrame(data_of_interest, columns =['Year', 'Title'])\n",
    "constellate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b338b-ce5c-478a-b86f-473321a353f1",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h3>\n",
    "\n",
    "If you are using a jsonl file you built with your own dataset, follow the example below. Tweak the code to serve your own purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ede39-9282-4699-b107-7b55f442dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data as a df\n",
    "file_path = '../All-sample-files/natural_gas_sents.jsonl'\n",
    "big_file_df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf91db1-1d5c-48e1-9e98-67464b0481d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a peek into the file\n",
    "big_file_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48bed7-5072-4527-aa93-3e37ffd72801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of the df\n",
    "example_df = big_file_df.iloc[:500].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6394e9c2-1620-4f76-bf42-bf6f2d2e2e82",
   "metadata": {},
   "source": [
    "## Classification\n",
    "In this section, we are going to use the OpenAI API to do a ternary classification of the texts - whether the sentences mentioning natural gas are positive, negative or neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e209118-41ec-41e8-a95c-6430545ca657",
   "metadata": {},
   "source": [
    "## Classification using OpenAI API\n",
    "### Set the System Message\n",
    "Let's give a system message that instructs the LLM to do a text classification task --- whether the given text containig 'natural gas' is positive, negative or neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fa326-e3ae-4a8a-b8bb-3b758bb8f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the system message\n",
    "system_message = \"\"\"Determine whether the following sentence mentioning natural gas conveys a positive, negative or neutral sentiment.\n",
    "Respond in JSON like so: {\"sentiment\": \"positive\"} or {\"sentiment\": \"negative\"} or {\"sentiment\": \"neutral\"}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93ff8b-678f-4877-b159-f951ef19b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look back at the data df\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa9c877-9339-43de-96d8-227169415dab",
   "metadata": {},
   "source": [
    "As we can see, each row in the dataframe represents one sentence about natural gas. Therefore, to classify each sentence to the category ```{\"sentiment\": \"positive\"}``` or ```{\"sentiment\": \"negative\"}``` or ```{\"sentiment\": \"neutral\"}```, we will need to write a user message for each sentence in the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b77775-5cdb-4bc7-af34-4374eb6c9c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T14:05:34.783322Z",
     "iopub.status.busy": "2024-09-12T14:05:34.783041Z",
     "iopub.status.idle": "2024-09-12T14:05:34.786340Z",
     "shell.execute_reply": "2024-09-12T14:05:34.785743Z",
     "shell.execute_reply.started": "2024-09-12T14:05:34.783303Z"
    }
   },
   "source": [
    "### Write User Message \n",
    "What user message should we give to the LLM for it to do the ternary classification? Well, the text in the `line_text` column! The text is very useful information for the LLM to classify its sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that makes a customized prompt for each row in the data df\n",
    "def make_user_message(row):\n",
    "    user_message = row['line_text']\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a9575-e634-4be2-8a81-c2e9a6f4b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try making a prompt for the first row in the data df\n",
    "user_message = make_user_message(ng_df.iloc[0])\n",
    "print(user_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b2fa1-32ee-431e-adb8-e3119668c788",
   "metadata": {},
   "source": [
    "### Classify the sentences\n",
    "Now that we have the system message and user message ready, we can go ahead and start classfying the sentences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c72cd-a74e-4644-acc0-a8f865fe210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a chat completion function\n",
    "def make_completion(user_message, client=OpenAI(), model='gpt-4o-2024-08-06', print_message=False):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "    if print_message:\n",
    "        print(f\"System message: {system_message}\\n{'-' * 80}\") # print system message\n",
    "        print(f\"User message: {user_message}\\n{'-' * 80}\") # print user message\n",
    "        print(\n",
    "            f\"Assistant response: {completion.choices[0].message.content}\\n{'*' * 80}\" # get the LLM response\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19dead-58c1-4e0b-bd4b-7005ef82658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with the first row from the data df\n",
    "test = make_completion(user_message, client=OpenAI(), model='gpt-4o-2024-08-06', print_message=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f5d28-b0bb-4d83-9560-dcf0f95868e0",
   "metadata": {},
   "source": [
    "### Generalize to all sentences\n",
    "Let's generalize the pipeline we wrote above to all the sentences in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00744a28-7db8-4907-bb0f-b4ac80bc35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an example, we will only use 3 sampled sentences \n",
    "\n",
    "# for demonstration, we will only use the first three questions\n",
    "ng_df = ng_df.sample(3).copy()\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3c2ac-18e8-4d13-88f4-caffa0e79ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the texts\n",
    "ng_df['LLM_output'] = ng_df['line_text'].apply(make_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca433c-5a43-4a85-8b59-23e5d44b486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the updated df\n",
    "ng_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad21257-d76d-49cc-bb3b-342c9b511eea",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "In the following exercise, we will use [Erik Fredner](https://www.google.com/search?client=safari&rls=en&q=erik+fredner&ie=UTF-8&oe=UTF-8)'s example dataset of Jeopardy questions containing 500 selected Jeopardy! questions to practice what you have learned about using the OpenAI API to classify texts. Here are the goals of this classification task. \n",
    "\n",
    "1. For each question, use its ```CATEGORY```, ```CLUE``` and ```ANSWER``` to inform the OpenAI API to classify the question.\n",
    "2. Every question is classified into two categories --- whether it is a question about literature or not.\n",
    "3. Write the output into JSON format. For each question, the output will look like this ```{\"Literature\": true}``` or ```{\"Literature\": false}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30725e9e-472a-4926-b601-33960de3dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the sample dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "file_path = '../All-sample-files/jeopardy_data.csv'\n",
    "\n",
    "# Read in the data\n",
    "jeopardy_df = pd.read_csv(file_path)\n",
    "jeopardy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ef063-8b1f-4a19-a895-ae5461da7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### write a system message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d2278-93ea-4764-bf7c-7973b86a3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "### write a function that generates a user message for each question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0207fab1-8b80-43a1-9f69-6ca8874c1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "### classify the texts, create a new column storing the LLM classification output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8c52b-1e9b-476a-b8a6-c8c1c131e572",
   "metadata": {},
   "source": [
    "___\n",
    "## Lesson Complete\n",
    "Congratulations! You have completed **Automated Classification with LLMs 1**. There are two more lessons in this series:\n",
    "\n",
    "* *Automated Classification with LLMs 2* \n",
    "* *Automated Classification with LLMs 3*\n",
    "\n",
    "### Start Next Lesson: [Automated Classification with LLMs 2](./Automated_Classification_2.ipynb)\n",
    "\n",
    "### Coding Challenge! Solutions\n",
    "\n",
    "There are often many ways to solve programming problems. Here are a few possible ways to solve the challenges, but there are certainly more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8dbd17-39a0-4cde-96c8-4c99922f2ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../All-sample-files/jeopardy_data.csv'\n",
    "\n",
    "# Read in the data\n",
    "jeopardy_df = pd.read_csv(file_path)\n",
    "jeopardy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4984b6-6d9e-4c53-b205-43d528ebf023",
   "metadata": {},
   "outputs": [],
   "source": [
    "### write the system message\n",
    "system_message = \"\"\"Determine whether the following Jeopardy question is about Literature.\n",
    "Respond in JSON like so: {\"Literature\": True}\"\"\"\n",
    "\n",
    "### write the user message\n",
    "jeopardy_df['user_message'] = jeopardy_df.apply(lambda row: f\"\"\"Category: {row['CATEGORY']}\\nClue: {row['CLUE']}\\nAnswer: {row['ANSWER']}\"\"\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268560d3-aa36-49d7-bbb8-b3821cbbca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use OpenAI API to classify the questions\n",
    "\n",
    "# write a chat completion function\n",
    "def make_completion(user_message, client=OpenAI(), model='gpt-4o-2024-08-06', print_message=False):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "        ],\n",
    "    )\n",
    "    if print_message:\n",
    "        print(f\"System message: {system_message}\\n{'-' * 80}\") # print system message\n",
    "        print(f\"User message: {user_message}\\n{'-' * 80}\") # print user message\n",
    "        print(\n",
    "            f\"Assistant response: {completion.choices[0].message.content}\\n{'*' * 80}\" # get the LLM response\n",
    "        )\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "#  classify the texts\n",
    "jeopardy_df['LLM_output'] = jeopardy_df['user_message'].apply(make_completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
