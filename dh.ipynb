{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8955af",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "**This notebook does not contain code but links to notebooks with code.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0310db",
   "metadata": {},
   "source": [
    "# Text Analysis: What Every Digital Humanist Should Know\n",
    "\n",
    "This notebook is a high-level overview of the fundamental text analysis methods common in the digital humanities. This notebook will help any digital humanist interested in text analysis but unsure what methods might help their current research project.\n",
    "\n",
    "This notebook answers:\n",
    "\n",
    "* What text analysis methods are useful for humanists?\n",
    "* What kinds of questions can a particular method answer?\n",
    "* How difficult is a particular method to learn?\n",
    "* Where can I start learning now?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61534746",
   "metadata": {},
   "source": [
    "## What kind of texts do I need for text analysis?\n",
    "\n",
    "Text analysis depends on having a large number of texts in an accessible format. Since many text analysis methods rely on statistical models, it is generally true that having more texts will improve the outcomes of your analysis. It is also generally true that an ideal set of texts—or corpus—will be:\n",
    "\n",
    "* Full-text\n",
    "* Easily readable, such as plaintext files or Python strings\n",
    "\n",
    "In practice, \"easily-readable\" means that you could hypothetically copy and paste the text. It is common, however, when doing text analysis to work with current works which are in copyright. If it is not possible to access \"full-text\" due to applicable copyright laws, the ideal corpus will give readers access to [n-gram](https://constellate.org/docs/key-terms/#n-gram) counts. In the cases where Constellate cannot supply full-text due to copyright laws (JSTOR and Portico content), we supply three n-gram counts:\n",
    "\n",
    "* Unigrams- A single-word construction, for example: \"vegetable\".\n",
    "* Bigrams- An two-word construction, for example: \"vegetable stock\".\n",
    "* Trigrams- A three-word construction, for example: \"homemade vegetable stock\".\n",
    "\n",
    "While having the full texts for the documents in your corpus is ideal, a great deal can be still be discovered through the use of unigrams. Even when researchers have access to the full-texts of a corpus, it is common for them to create a list of n-gram counts for analysis. \n",
    "___\n",
    "<font color=\"red\">Read more</font>\n",
    "* [Constellate Dataset Builder: full-text and n-gram content](https://constellate.org/docs/data-sources)\n",
    "* [Bring your own data into Constellate](https://constellate.org/docs/import-data-into-constellate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cea84c",
   "metadata": {},
   "source": [
    "## I have my own data. What will it take to get it ready?\n",
    "\n",
    "One of the most significant benefits of using Constellate to teach, learn, and do research is that the dataset builder takes out *the vast majority* of effort in doing text analysis. For a major text analysis project, such as UNC Chapel Hill's [On the Books: Jim Crow and Algorithms of Resistance](https://onthebooks.lib.unc.edu/), about 90% of the labor is creating the corpus.\n",
    "\n",
    "If you have your own data, you will need to assess what it will take to make it ready for analysis. Here are some questions you should ask:\n",
    "\n",
    "* How can I convert my data into plain text? \n",
    "    * <font color=\"red\">Start learning</font> [Optical Character Recognition Basics](./ocr-basics.ipynb)\n",
    "* How can I tokenize my texts (break up and separate the words)? \n",
    "    * <font color=\"red\">Start learning</font> [Tokenize Text Files](./tokenizing-text-files.ipynb)\n",
    "    * <font color=\"red\">Start learning</font> [Tokenize Text Files with NLTK](./tokenize-text-files-with-nltk.ipynb)\n",
    "    \n",
    "Consider the data's current form as well as the size and skill of your project staff. The corpus creation process could take anywhere from a few hours to many years of labor. If there is a significant amount of labor, you may need to write a grant proposal to hire help. *If writing a grant, contact your library since funding agencies will require your corpus to be committed to your institutional repository.*\n",
    "\n",
    "In addition to the cleaned-up texts for your corpus, you will also need a strategy for dealing with textual metadata, information such as author, year, etc. Some of this is discussed in [Tokenize Text Files with NLTK](./tokenize-text-files-with-nltk.ipynb), but it would also help to have some experience with working with data at scale with either Excel or, even better, Python Pandas.\n",
    "___\n",
    "\n",
    "<font color=\"red\">Start learning</font>\n",
    "[Pandas 1](./pandas-1.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd22d50f",
   "metadata": {},
   "source": [
    "## What humanities questions can text analysis answer?\n",
    "\n",
    "Humanists use text analysis to answer a wide variety of questions. Here are a few that are common:\n",
    "\n",
    "1. What are these texts about?\n",
    "2. What emotions are expressed?\n",
    "3. What key names can I find?\n",
    "4. Which texts are similar?\n",
    "\n",
    "Let's consider the methods to answer each of these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f472c0",
   "metadata": {},
   "source": [
    "### What are these texts about?\n",
    "\n",
    "When it comes to a large body of texts, humanities scholars tend to be most curious about the text's contents. What are the words, topics, concepts, and significant terms in these documents? There are a number of methods often used which vary in complexity and difficulty.\n",
    "___\n",
    "\n",
    "**Word Frequency** (Beginner Friendly)\n",
    "\n",
    "If you search for digital humanities in [Google image search](https://www.google.com/search?q=digital+humanities&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjmhrrwjeL0AhWvkIkEHfAbCfoQ_AUoA3oECAEQBQ), the most common result is a [word cloud](https://constellate.org/docs/key-terms/#tag-cloud). A word cloud visualizes the most frequent content words in a text or corpus. Before you can create a word cloud, however, you need to collect the word frequencies for all the words in your text. You may also need to use a [stop words list](https://constellate.org/docs/key-terms/#stop-words) to remove common [function words](https://constellate.org/docs/key-terms/#function-words) (grammatical word constructions like \"the\", \"of\", and \"or\").\n",
    "\n",
    "<font color=\"red\">Start learning</font> [Word Frequency Analysis](./exploring-word-frequencies.ipynb) and create a word cloud\n",
    "___\n",
    "\n",
    "**Significant Terms** (Beginner Friendly)\n",
    "\n",
    "Search engines use significant terms analysis to match a user query with a list of appropriate documents. This method could be useful if you want to search your corpus for the most significant texts based on a word (or set of words). It can also be useful in reverse. For a given document, you could create a list of the ten most significant terms. This can be useful for summarizing the content of a document. \n",
    "\n",
    "<font color=\"red\">Start learning</font> [Significant Terms Analysis](./finding-significant-terms.ipynb) and create a simple search engine\n",
    "___\n",
    "**Topic Analysis** or Topic Modeling (Intermediate)\n",
    "\n",
    "While significant terms analysis reveals terms commonly found in a given document, a topic analysis can tell us what words tend to cluster together across a corpus. For example, if we were to study newspapers, we would expect that certain words would cluster together into topics that match the sections of the newspaper. We might see something like:\n",
    "\n",
    "* Topic 1: baseball, ball, player, trade, score, win, defeat\n",
    "* Topic 2: market, dow, bull, trade, run, fund, stock\n",
    "* Topic 3: campaign, democratic, polls, red, vote, defeat, state\n",
    "\n",
    "We can recognize that these words tend to cluster together within newspaper sections such as \"Sports\", \"Finance\", and \"Politics\". If we have never read a set of documents, we might use a topic analysis to get a sense of what topics are in a given corpus. Given that Topic Analysis is an exploratory technique, it may require some expertise to fine-tune and get good results for a given corpus. However, if the topics can be discovered then they could potentially be used to train a model using [Machine Learning](https://constellate.org/docs/key-terms/#machine-learning) to discover the topics in a given document automatically.\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "Keli Du's \"A Survey on LDA Topic Modeling in Digital Humanities\"\n",
    "\n",
    "<font color=\"red\">Start learning</font> [Topic Analysis](./finding-significant-terms.ipynb) and create an interactive visualization\n",
    "___\n",
    "**Concordance** (Beginner Friendly)\n",
    "\n",
    "The concordance has a long history in humanities study and Roberto Busa's concordance *Index Thomisticus*—started in 1946—is arguably the first digital humanities project. Before computers were common, they were printed in large volumes such as John Bartlett's 1982 reference book *A Complete Concordance to Shakespeare*—it was 1909 pages pages long! A concordance gives the context of a given word or phrase in a body of texts. For example, a literary scholar might ask: how often and in what context does Shakespeare use the phrase \"honest Iago\" in Othello? A historian might examine a particular politician's speeches, looking for examples of a particular \"dog whistle\".\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "\n",
    "* Steven E. Jones [Roberto Busa, S.J., and the Emergence of Humanities Computing](https://www.routledge.com/Roberto-Busa-S-J-and-the-Emergence-of-Humanities-Computing-The-Priest/Jones/p/book/9781138587250) (2016)\n",
    "* Julianne Nyhan and Marco Passarotti, eds. [One Origin of Digital Humanities: Fr Roberto Busa in His Own Words](https://www.amazon.com/One-Origin-Digital-Humanities-Roberto/dp/3030183114/) (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f8e76",
   "metadata": {},
   "source": [
    "### What emotions are expressed?\n",
    "\n",
    "**Sentiment Analysis** (Intermediate)\n",
    "\n",
    "Sentiment analysis can help determine the emotions expressed in a given text. This can be determined using rule-based algorithms, [Machine Learning](https://constellate.org/docs/key-terms/#machine-learning), or both.\n",
    "\n",
    "<font color=\"red\">Start learning</font> [Sentiment Analysis with VADER](./finding-significant-terms.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62cd4dd",
   "metadata": {},
   "source": [
    "### What key names can I find?\n",
    "\n",
    "**Named Entity Recognition** or NER (Intermediate)\n",
    "\n",
    "Named Entity Recognition (NER) automatically identifies entities within a text and can helpful for extracting certain kinds of entities such as proper nouns. For example, NER could identify names of organizations, people, and places. It might also help identify things like dates, times, or dollar amounts.\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "\n",
    "* Melanie Walsh [Named Entity Recognition](https://melaniewalsh.github.io/Intro-Cultural-Analytics/05-Text-Analysis/12-Named-Entity-Recognition.html) 2021\n",
    "* Zoe LeBlanc [Named Entity Recognition](https://nkelber.github.io/tapi2021/book/courses/ner.html) TAP Institute 2021\n",
    "* Miguel Won, Patricia Murrieta-Flores, and Bruno Martins [Ensemble Named Entity Recognition (NER): Evaluating NER Tools in the Identification of Place Names in Historical Corpora](https://www.frontiersin.org/articles/10.3389/fdigh.2018.00002/full) (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c255102",
   "metadata": {},
   "source": [
    "### Which texts are similar?\n",
    "\n",
    "**Stylometrics and Authorship Attribution** (Intermediate to Advanced)\n",
    "\n",
    "The digital humanities, and its precursor Humanities Computing, have a long history in the analysis of literature, particularly for analyzing genre and authorship. For example, the New Oxford Shakespeare surprised many scholars by assigning significant authorship of Shakespeare's \"Henry VI,\" Parts 1, 2, and 3. It also lists as co-authors many Shakespeare contemporaries such as Thomas Nashe, George, Peele, Thomas Heywood, Ben Jonson, George Wilkins, Thomas Middleton, and John Fletcher.\n",
    "\n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* Patrick Juola [How a Computer Program Helped Show J.K. Rowling Wrote A Cuckoo's Calling](https://www.scientificamerican.com/article/how-a-computer-program-helped-show-jk-rowling-write-a-cuckoos-calling/) (2013)\n",
    "* Ros Barber [Big data or not enough? Zeta test reliability and the attribution of Henry VI](https://academic.oup.com/dsh/article-abstract/36/3/542/5918973?redirectedFrom=fulltext)\n",
    "\n",
    "___\n",
    "\n",
    "**Supervised Machine Learning** (Intermediate to Advanced)\n",
    "\n",
    "The advent of supervised machine learning techniques have rapidly changed text analysis in the digital humanities. These methods \"train\" computers to identify and classify similar items based on data that has been labeled or tagged by experts. For example, On the Books: Jim Crow and Algorithms of Resistance was able to use machine learning to identify 1939 North Carolina Jim Crow laws enacted between Reconstruction and the Civil Rights Movement. \n",
    "\n",
    "<font color=\"red\">Read more</font>\n",
    "* [Project Outcomes for On the Books](https://onthebooks.lib.unc.edu/otb-research/project-outcomes/)\n",
    "* William Mattingly [Introduction to Machine Learning](https://nkelber.github.io/tapi2021/book/courses/machine-learning-william.html) TAP Institute 2021\n",
    "* Grant Glass [Introduction to Machine Learning](https://nkelber.github.io/tapi2021/book/courses/machine-learning-grant.html) TAP Institute 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84913762",
   "metadata": {},
   "source": [
    "## Looking for more learning materials?\n",
    "\n",
    "* [TAP Institute 2021 DH Courses](https://labs.jstor.org/projects/text-analysis-pedagogy-institute-2/)\n",
    "* [PythonHumanities.com](https://pythonhumanities.com/about/) by [William Mattingly](https://wjbmattingly.com/)\n",
    "* [Programming Historian](https://programminghistorian.org/en/lessons/) by various authors\n",
    "* [The Carpentries](https://carpentries.org/workshops-curricula/) by various authors\n",
    "* [Digital Humanities Research Institutes](https://www.dhinstitutes.org/curricula/) by various authors\n",
    "* [Computational Humanities Research](https://discourse.computational-humanities-research.org/) <br />\n",
    "* [YaleDHLab Lab Workshops](https://github.com/YaleDHLab/lab-workshops) <br />\n",
    "* [Jupyter notebooks for digital humanities](https://github.com/quinnanya/dh-jupyter/blob/master/README.md) curated by [Quinn Dombrowski](https://quinndombrowski.com/)\n",
    "* [Data Sitter's Club](https://datasittersclub.github.io/site/) by various authors\n",
    "* [HathiTrust Digital Library Collections and Tools](https://www.hathitrust.org/htrc_collections_tools)\n",
    "* [Documenting the Now](https://github.com/DocNow)\n",
    "  \n",
    "**Books on Python, Text Analysis, and DH**\n",
    "* *Automate the Boring Stuff with Python: Practical Programming for Total Beginners* (2019) by Al Sweigart\n",
    "* *Python Crash Course: A Handson, project-based introduction to programming* (2019) by Eric Matthes\n",
    "* *Machine Learning with Python Cookbook* (2018) by Chris Albon\n",
    "* *Natural Langauge Processing in Action* (2019)by Hobson Lane, Cole Howard, and Hannes Max Hapke\n",
    "* [*Humanities Data Analysis: Case Studies with Python*](https://www.humanitiesdataanalysis.org/) by Folgert Karsdorp, Mike Kestemont, and Allen Riddell\n",
    "* [Technical Textbooks List](https://cmu-lib.github.io/dhlg/global-resources/educational-resources/textbooks/) by [Scott B. Weingart](https://scottbot.github.io/)\n",
    "* [Introduction to Named Entity Recognition](https://ner.pythonhumanities.com/intro.html) by [William Mattingly](https://wjbmattingly.com/)\n",
    "\n",
    "**Books on Data Ethics**\n",
    "* *Algorithms of Oppression* (2018) by Safiya Noble\n",
    "* *Race After Technology* (2019) by Ruha Benjamin\n",
    "* *Data Feminism* (2020) by Catherine D'Ignazio and Lauren F. Klein\n",
    "\n",
    "**Instructional Video**\n",
    "* [DH, Coding, and Book History](https://www.youtube.com/user/pvierth/videos)\n",
    "* [Python Tutorials for Digital Humanities](https://www.youtube.com/@python-programming)\n",
    "\n",
    "**Course Examples**\n",
    "* [Humanities Analytics](https://humanitiesanalytics.com/) by [Matt Lavin](https://matthew-lavin.com/)\n",
    "* [Introduction to Cultural Analytics and Python](https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html) by [Melanie Walsh](https://melaniewalsh.org/)\n",
    "* [CodeLab](https://github.com/ZoeLeBlanc/CodeLab) by [Shane Lin](https://www.library.virginia.edu/staff/ssl2ab), [Zoe LeBlanc](https://zoeleblanc.com/), and [Brandon Walsh](https://scholarslab.lib.virginia.edu/people/brandon-walsh/)\n",
    "* [Computational and Inferential Thinking: The Foundations of Data Science](https://inferentialthinking.com/chapters/intro.html) by Ani Adhikari, John DeNero, David Wagner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
