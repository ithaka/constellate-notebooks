{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) and Ted Lawless for [JSTOR Labs](https://labs.jstor.org/) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA) Topic Modeling\n",
    "\n",
    "**Description:**\n",
    "This notebook demonstrates how to do topic modeling. The following processes are described:\n",
    "\n",
    "* Filtering based on a pre-processed ID list\n",
    "* Filtering based on a stop words list\n",
    "* Cleaning the tokens in the dataset\n",
    "* Creating a gensim dictionary\n",
    "* Creating a gensim bag of words corpus\n",
    "* Computing a topic list using gensim\n",
    "* Visualizing the topic list with `pyldavis`\n",
    "\n",
    "**Use Case:** For Researchers (Mostly code without explanation, not ideal for learners)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion time:** 60 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics Series ([Start Python Basics I](../Python-basics/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "* [Exploring Metadata](../Exploring-metadata/exploring-metadata.ipynb)\n",
    "* [Python Intermediate 2](../Python-intermediate/python-intermediate-2.ipynb)\n",
    "* [Pandas I](../Pandas-basics/pandas-basics-1.ipynb)\n",
    "* [Creating a Stopwords List](../Stopwords/creating-stopwords-list.ipynb)\n",
    "* A familiarity with [gensim](https://constellate.org/docs/key-terms/#gensim) is helpful but not required.\n",
    "\n",
    "**Data Format:** JSON Lines (.jsonl)\n",
    "\n",
    "**Libraries Used:**\n",
    "* pandas to load a preprocessing list\n",
    "* `csv` to load a custom stopwords list\n",
    "* gensim to accomplish the topic modeling\n",
    "* NLTK to create a stopwords list (if no list is supplied)\n",
    "* `pyldavis` to visualize our topic model\n",
    "\n",
    "**Research Pipeline**\n",
    "1. Build a dataset\n",
    "2. Create a \"Pre-Processing CSV\" with [Exploring Metadata](../Exploring-metadata/exploring-metadata.ipynb) (Optional)\n",
    "3. Create a \"Custom Stopwords List\" with [Creating a Stopwords List](../Stopwords/creating-stopwords-list.ipynb) (Optional)\n",
    "4. Complete the Topic Modeling analysis with this notebook\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and libraries\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Topic Modeling?\n",
    "\n",
    "**Topic modeling** is a **machine learning** technique that attempts to discover groupings of words (called topics) that commonly occur together in a body of texts. The body of texts could be anything from journal articles to newspaper articles to tweets.\n",
    "\n",
    "**Topic modeling** is an unsupervised, clustering technique for text. We give the machine a series of texts that it then attempts to cluster the texts into a given number of topics. There is also a *supervised*, clustering technique called **Topic Classification**, where we supply the machine with examples of pre-labeled topics and then see if the machine can identify them given the examples.\n",
    "\n",
    "**Topic modeling** is usually considered an exploratory technique; it helps us discover new patterns within a set of texts. **Topic Classification**, using labeled data, is intended to be a predictive technique; we want it to find more things like the examples we give it.\n",
    "\n",
    "<font color='red'>Read more</font>\n",
    "\n",
    "* [\"Latent Dirichlet Allocation: Intuition, math, implementation and visualisation with pyLDAvis\" Ioana](https://towardsdatascience.com/latent-dirichlet-allocation-intuition-math-implementation-and-visualisation-63ccb616e094) 2020\n",
    "* [\"Latent Dirichlet Allocation\" Blei, Ng, Jordan](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8) 2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your dataset\n",
    "<h3 style=\"color:red; display:inline\">Note! The following code cell assumes that you have downloaded the JSONL file containing metadata, ngrams and full texts to the current working directory.&lt; / &gt; </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# path to the jsonl file in the current directory\n",
    "dataset_file = '' # copy and paste the path to the JSONL file \n",
    "\n",
    "# function that reads a jsonl file into a generator\n",
    "def dataset_reader(file_path):\n",
    "    \"\"\"\n",
    "    Helper to read in gzip files and yield Python dictionary\n",
    "    documents.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, \"rb\") as input_file:\n",
    "        for row in input_file:\n",
    "            yield json.loads(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords List\n",
    "\n",
    "If you have created a stopword list in the stopwords notebook, we will import it here. (You can always modify the CSV file to add or subtract words then reload the list.) Otherwise, we'll load the NLTK stopwords list automatically.\n",
    "\n",
    "We recommend storing your stopwords in a CSV file as shown in the [Creating Stopwords List](../Stopwords/creating-stopwords-list.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom data/stop_words.csv if available\n",
    "# Otherwise, load the nltk stopwords list in English\n",
    "import csv\n",
    "# Create an empty Python list to hold the stopwords\n",
    "stop_words = []\n",
    "\n",
    "# The filename of the custom data/stop_words.csv file\n",
    "stopwords_path = Path.cwd() / 'data' / 'stop_words.csv'\n",
    "\n",
    "if stopwords_path.exists():\n",
    "    with stopwords_path.open() as f:\n",
    "        stop_words = list(csv.reader(f))[0]\n",
    "    print('Custom stopwords list loaded from CSV')\n",
    "else:\n",
    "    # Load the NLTK stopwords list\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # Create a CSV file to store an initial set of stopwords\n",
    "    with open('./data/stop_words.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(stop_words)\n",
    "    print('NLTK stopwords list loaded and saved to stopwords.csv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview stop words\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Process Tokens\n",
    "Next, we create a short function to clean up our tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_token(token):\n",
    "    token = token.lower()\n",
    "    if token in stop_words:\n",
    "        return\n",
    "    if len(token) < 4:\n",
    "        return\n",
    "    if not(token.isalpha()):\n",
    "        return\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Limit to n documents. Set to None to use all documents.\n",
    "\n",
    "limit = 5000\n",
    "\n",
    "n = 0\n",
    "documents = []\n",
    "for document in dataset_reader(dataset_file):\n",
    "    processed_document = []\n",
    "    unigrams = document.get(\"unigramCount\", {})\n",
    "    for gram, count in unigrams.items():\n",
    "        clean_gram = process_token(gram)\n",
    "        if clean_gram is None:\n",
    "            continue\n",
    "        processed_document += [clean_gram] * count # Add the unigram as many times as it was counted\n",
    "    if len(processed_document) > 0:\n",
    "        documents.append(processed_document)\n",
    "    if n % 1000 == 0:\n",
    "        print(f'Unigrams collected for {n} documents...')\n",
    "    n += 1\n",
    "    if (limit is not None) and (n >= limit):\n",
    "       break\n",
    "print(f'All unigrams collected for {n} documents.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a gensim dictionary corpus and then train the model. More information about parameters can be found at the [Gensim LDA Model page](https://radimrehurek.com/gensim/models/ldamodel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the gensim dictionary\n",
    "dictionary = gensim.corpora.Dictionary(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_count = len(documents)\n",
    "num_topics = 7 # Change the number of topics\n",
    "passes = 5 # The number of passes used to train the model\n",
    "# Remove terms that appear in less than 50 documents and terms that occur in more than 90% of documents.\n",
    "dictionary.filter_extremes(no_below=50, no_above=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the LDA model\n",
    "model = gensim.models.LdaModel(\n",
    "    corpus=bow_corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "After each pass, the LDA model will output a \"perplexity\" score that measures the \"held out log-likelihood\". Perplexity is a measure of how \"surpised\" the machine is to see certain data. In other words, perplexity measures how successfully a trained topic model predicts new data. The model may be trained many times with different parameters, optimizing for the lowest possible perplexity.\n",
    "\n",
    "In general, the perplexity score should trend downward as the machine \"learns\" what to expect from the data. While a low perplexity score may signal the machine has learned the documents' patterns, that does not mean that the topics formed from a model with low perplexity will form the most coherent topics. (See [\"Reading Tea Leaves: How Humans Interpret Topic Models\" Chang, et al. 2009](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html).)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Coherence\n",
    "\n",
    "The failure of perplexity scores to consistently create \"good\" topics has led to new methods in \"topic coherence\". Here we demonstrate two of these methods with Gensim but there are additional methods available. Ideally, a researcher would run many topic models, discovering the optimum settings for topic coherence.\n",
    "\n",
    "Ultimately, however, the best judgment of topic coherence is a disciplinary expert, particularly someone with familiarity with the materials in question.\n",
    "\n",
    "<font color='red'>Read more</font>\n",
    "\n",
    "* [\"Optimizing Semantic Coherence in Topic Models\" Mimno, et al. 2011](http://dirichlet.net/pdf/mimno11optimizing.pdf)\n",
    "* [\"Automatic Evaluation of Topic Coherence\" Newman, et al. 2010](https://mimno.infosci.cornell.edu/info6150/readings/N10-1012.pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the coherence score using UMass\n",
    "# u_mass is measured from -14 to 14, higher is better\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=model,\n",
    "    corpus=bow_corpus,\n",
    "    dictionary=dictionary, \n",
    "    coherence='u_mass'\n",
    ")\n",
    "\n",
    "# Compute Coherence Score using UMass\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a List of Topics\n",
    "Print the most significant terms, as determined by the model, for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_num in range(0, num_topics):\n",
    "    word_ids = model.get_topic_terms(topic_num)\n",
    "    words = []\n",
    "    for wid, weight in word_ids:\n",
    "        word = dictionary.id2token[wid]\n",
    "        words.append(word)\n",
    "    print(\"Topic {}\".format(str(topic_num).ljust(5)), \" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Topic Distances\n",
    "\n",
    "Visualize the model using [`pyLDAvis`](https://pyldavis.readthedocs.io/en/latest/). The visualization will be output to an html file in the data folder. (Right-click on the html file and choose \"Open in New Browser Tab.\")\n",
    "\n",
    "Try choosing a topic and adjusting the λ slider. When λ approaches 0, the words in a given document occur almost entirely in that topic. When λ approaches 1, the words occur more often in other topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export this visualization as an HTML file\n",
    "# An internet connection is still required to view the HTML\n",
    "p = pyLDAvis.gensim.prepare(model, bow_corpus, dictionary)\n",
    "pyLDAvis.save_html(p, './data/my_visualization.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
