{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fac83f3-4536-4584-870a-6bcfec93ea68",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "This notebook is created by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef364-f331-462d-96f1-4cd736b4dcec",
   "metadata": {},
   "source": [
    "# Pandas Basics 3 \n",
    "\n",
    "**Description:** This notebook describes how to:\n",
    "\n",
    "* Use `merge()` and `concat()` to combine multiple dataframes\n",
    "* Handle duplicates and get unique values\n",
    "* Understand vectorized operations in Pandas\n",
    "* Use `apply()` and `where()` to process the data in a dataframe\n",
    "\n",
    "This is the third notebook in a series on learning to use Pandas. \n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Beginner\n",
    "\n",
    "**Knowledge Required:** \n",
    "* [Pandas Basics 1](./pandas-basics-1.ipynb)\n",
    "* [Pandas Basics 2](./pandas-basics-2.ipynb)\n",
    "* Python Basics ([Start Python Basics I](../Python-basics/python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* [Python Intermediate 1](../Python-intermediate/python-intermediate-1.ipynb)\n",
    "* [Python Intermediate 2](../Python-intermediate/python-intermediate-2.ipynb)\n",
    "* [Python Intermediate 4](../Python-intermediate/python-intermediate-4.ipynb)\n",
    "\n",
    "**Completion Time:** 90 minutes\n",
    "\n",
    "**Data Format:** csv \n",
    "\n",
    "**Libraries Used:** Pandas\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7a23bc-0949-4208-85fa-abf811a39325",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install xlsxwriter\n",
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c9608-b5dd-4df4-b04e-1295d787876d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pandas library, `as pd` allows us to shorten typing `pandas` to `pd` when we call pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af26b1",
   "metadata": {},
   "source": [
    "## Merge and concatenate dataframes \n",
    "\n",
    "In this section, we will learn two methods to combine multiple dataframes in Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc68f51-3c12-41a5-aaae-ac2fc6e9bac0",
   "metadata": {},
   "source": [
    "### merge()\n",
    "\n",
    "When dealing with tabular data, we often find ourselves in a situation where we need to merge two or more dataframes as we want a subset of the data from each. Pandas provides a `.merge()` method that allows us to merge dataframes easily.\n",
    "\n",
    "The data we use in this section is the unemployment data in Massachusetts from 2023 January to February."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a1a16-8f21-4b93-8a62-e9d9abea830e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0705fd-09da-469b-b671-05e8ea12d294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dataframe from the csv files\n",
    "unemp_jan = pd.read_csv('../All-sample-files/Pandas3_unemp_jan.csv')\n",
    "unemp_feb = pd.read_csv('../All-sample-files/Pandas3_unemp_feb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05c38c-19ac-4563-a30f-a5ea84e53058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take a look at unemp_jan\n",
    "unemp_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce926316-d009-4ec5-9889-9f648456a9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at unemp_feb\n",
    "unemp_feb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d625ab2c-ce75-4a23-b365-8040eabfdc6c",
   "metadata": {},
   "source": [
    "The two dataframes have a column in common, the `COUNTY` column. This column contains the names of the 14 counties in the state of Massachusetts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2341a-dd6c-4ad0-85af-433106a9671d",
   "metadata": {},
   "source": [
    "### inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c41d1-eab7-44c0-a387-1e9d3325385e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge the two dfs\n",
    "unemp_jan.merge(unemp_feb, on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc39aed-5f64-4ba2-ac69-e2ad572a26f6",
   "metadata": {},
   "source": [
    "By convention, we'll call the dataframe on which we call the `.merge()` method the left dataframe. We'll call the dataframe passed into the `.merge()` method the right dataframe. \n",
    "\n",
    "As you can see, the parameter 'on' specifies the **key column** we use to merge the two dataframes. By default, the type of merge performed is 'inner', which uses the intersection of the keys from the **key column**. You can explicitly specify the type of merge by setting the 'how' parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa2a5e-e081-4fe9-975f-ecdbc5698a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the parameter 'how' to specify the merge type\n",
    "unemp_jan.merge(unemp_feb, how='inner', on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d240801-ad67-43a4-bb6d-235b061b91a2",
   "metadata": {},
   "source": [
    "In an inner join, you'll lose the rows that do not have a matching key in the two dataframes. In other words, only rows from the two dataframes that have a matching value in the **key column** will appear after the merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656a192-6f15-4ab8-a5d2-867e129dd121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see what happens when there are non-matching keys \n",
    "a = unemp_jan.drop(0)\n",
    "b = unemp_feb.drop(13)\n",
    "a.merge(b, on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d182e347-dac7-4979-9341-22aebd3a41be",
   "metadata": {},
   "source": [
    "As you can see, we have lost the data for Barnstable and Worcester after the merge. This is because the former has been dropped from the left dataframe and the latter has been dropped from the right dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367612cc-7113-4a62-8589-e71f9760ab64",
   "metadata": {},
   "source": [
    "### outer join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e82179-6116-4f9e-8ded-07e8bb86c592",
   "metadata": {},
   "source": [
    "There are other types of merge, of course. Instead of using the intersection of keys from two dataframes, we can use the union of keys from two dataframes. The type of merge in this case is 'outer'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d43c4-7915-40fd-8757-a080c363d90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change the merge type and see what happens\n",
    "a = unemp_jan.drop(0)\n",
    "b = unemp_feb.drop(13)\n",
    "a.merge(b, how='outer', on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c87556-3b51-4b57-a5f2-248ba6f5eeca",
   "metadata": {},
   "source": [
    "As you can see, the union of the values in the 'COUNTY' column from the two dataframes is used to merge the two. For keys that only exist in one dataframe, unmatched columns will be filled in with NaN. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641cb335-89cf-4257-b5f3-1d5c7cc3a9a2",
   "metadata": {},
   "source": [
    "### left outer join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8720d3-e855-4629-b1c7-7632acc9b134",
   "metadata": {},
   "source": [
    "Still another type of merge is to use the keys from the left dataframe in the merge. The values in the key column from the left dataframe will be used to merge the two dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c0fb5-5c89-45f7-89cc-510a0176f9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the keys from the left df in the merge\n",
    "a = unemp_jan.drop(0)\n",
    "b = unemp_feb.drop(13)\n",
    "a.merge(b, how='left', on='COUNTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a4e5d-1557-4720-b4d6-2480ad241f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see what happens if right df has a non-matching key\n",
    "unemp_jan.merge(unemp_feb.drop(0), how='left', on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab9c85-d77a-4515-a683-39dccc5ee995",
   "metadata": {},
   "source": [
    "### right outer join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143a4a8-d384-467a-ac4b-c27ef254383a",
   "metadata": {},
   "source": [
    "Or, you can use the keys from the right dataframe in the merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec64f4c-1760-444f-8f60-b2942b6387be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the keys from the right df in the merge\n",
    "unemp_jan.merge(unemp_feb.drop(0), how='right', on='COUNTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeafaa8-f53f-430f-8539-3d33cbe82807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# see what happens if left df has a non-matching key\n",
    "unemp_jan.drop(0).merge(unemp_feb, how='right', on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336bcbd-aa10-4af2-b944-08dd2c2d32ea",
   "metadata": {},
   "source": [
    "### The suffix parameter\n",
    "By default, Pandas does not allow duplicate column names. Therefore, if the two dataframes have a column other than the key column that has the same name, Pandas automatically uses suffixes to distinguish them after the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fabc1-a396-490e-9039-3d0a26a745b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a df with the unemployment data of MA in Feb of 2022\n",
    "unemp_feb_22 = pd.DataFrame({'COUNTY':\n",
    "                               ['BARNSTABLE',\n",
    "                                'BERKSHIRE',\n",
    "                                'BRISTOL',\n",
    "                                'DUKES',\n",
    "                                'ESSEX',\n",
    "                                'FRANKLIN',\n",
    "                                'HAMPDEN',\n",
    "                                'HAMPSHIRE',\n",
    "                                'MIDDLESEX',\n",
    "                                'NANTUCKET',\n",
    "                                'NORFOLK',\n",
    "                                'PLYMOUTH',\n",
    "                                'SUFFOLK',\n",
    "                                'WORCESTER'],\n",
    "                               'Feb Unemployment':\n",
    "                               [7952,\n",
    "                                3366,\n",
    "                                17777,\n",
    "                                804,\n",
    "                                19134,\n",
    "                                1653,\n",
    "                                12830,\n",
    "                                3305,\n",
    "                                29335,\n",
    "                                1071,\n",
    "                                14045,\n",
    "                                14072,\n",
    "                                17043,\n",
    "                                20000]})\n",
    "unemp_feb_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de0eae-a306-4b5a-a522-d2088338031e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge the two and see what happens\n",
    "unemp_feb.merge(unemp_feb_22, on='COUNTY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ad4bd-4b1c-4c16-9747-60bdef1272f8",
   "metadata": {},
   "source": [
    "The suffix '_x' indicates that the column is from the left dataframe. The suffix '_y' indicates that the column is from the right dataframe. You can make the suffixes more descriptive by setting the the value of the parameter 'suffixes'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdbc75-8ee0-469d-8a79-1c976cb6b8eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make the suffixes more descriptive\n",
    "unemp_feb.merge(unemp_feb_22, on='COUNTY', suffixes=[' 2023', ' 2022'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecd80a8-21f2-42ed-90dc-e75693bf181d",
   "metadata": {},
   "source": [
    "### concat()\n",
    "\n",
    "The `.concat()` method stitches two dataframes together along the row axis or the column axis. It is often used to combine two datasets to form a larger one for further processing.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aa5a24-e085-4873-a9c1-005797357f66",
   "metadata": {},
   "source": [
    "By default, the `concat()` method concatenates multiple dataframes along the row axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb7871-0bf4-424d-b631-22bda583d492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate unemp_feb and unemp_feb_22\n",
    "pd.concat([unemp_feb, unemp_feb_22])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db6910-0b81-440a-9bba-f07113ac2720",
   "metadata": {},
   "source": [
    "You can see that the indexes from the original dataframes are preserved after the concatenation. If you would like to reset the index after concatenation, you can set the parameter `ignore_index` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dcb7e9-72b6-4e62-b2be-57d97602818e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set ignore_index to False\n",
    "pd.concat([unemp_feb, unemp_feb_22], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563c637-32ed-4439-b2d3-1c74b59d7b41",
   "metadata": {},
   "source": [
    "The two dataframes `unemp_feb` and `unemp_feb_22` have the same columns. Both have a column `COUNTY` and a column `Feb Unemployment`. What if we have two dataframes that have non-matching columns? For example, the dataframe `unemp_jan` has a `COUNTY` column and a `Jan Unemployment` column. If we concatenate `unemp_feb` and `unemp_jan`, what will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a9be7-81ee-411d-92f5-b36d0b401af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate two dfs that have non-matching columns\n",
    "pd.concat([unemp_feb, unemp_jan])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57734090-b38d-4948-8b00-bac29bd1012b",
   "metadata": {},
   "source": [
    "If you would like to concatenate two dataframes along the column axis, just set the value of the 'axis' parameter to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90f4c4d-bfde-4955-8d9d-011eb15cb5c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate along the column axis\n",
    "pd.concat([unemp_jan.set_index('COUNTY'), unemp_feb.set_index('COUNTY')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ee45b-85d2-468c-b1f5-045696e122f0",
   "metadata": {},
   "source": [
    "## Duplicates and unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24010eee-200e-4999-8550-7e3ce75ca940",
   "metadata": {},
   "source": [
    "After we combine several dataframes into a big one, a common practice is to remove the duplicates in the big dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310a8374-fc04-4545-bf5c-f57db6eee529",
   "metadata": {},
   "source": [
    "### drop_duplicates()\n",
    "\n",
    "There is a handy method in Pandas that can remove duplicates, i.e.  `drop_duplicates()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d872f6-e9e2-4194-9f70-70c214e7d575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a df with duplicates\n",
    "rate = pd.DataFrame([['Pandas', 'Morning', 7], \n",
    "                     ['Pandas', 'Morning', 7],\n",
    "                     ['Pandas', 'Evening', 8],\n",
    "                     ['PythonBasics','Morning', 9]],\n",
    "                    columns=['Course', 'Session', 'Rating'])\n",
    "rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb1147-d94b-49ac-8000-50b30e633977",
   "metadata": {},
   "source": [
    "By default, `.drop_duplicates()` considers all columns when removing duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cc0af-84a0-43d3-884c-90c91bb14c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "rate.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90501edd-16f0-492a-8fbb-6d618d14174c",
   "metadata": {},
   "source": [
    "You can specify the column(s) to look for duplicates by setting the `subset` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5170455-5e66-4508-8fa2-908c997c397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the 'subset' parameter\n",
    "rate.drop_duplicates(subset=['Course'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec340c-7a8a-4fbe-baad-29329bcd6c16",
   "metadata": {},
   "source": [
    "By default, the first occurrence of the duplicates will be kept. If you would like to keep the last occurrence, you can set the `keep` parameter to `last`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a99e5-c0f2-45c4-9a52-b01267cfeadb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the 'keep' parameter to 'last'\n",
    "rate.drop_duplicates(subset=['Course'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f5f6-debf-4a7e-944d-ad11a5252fc2",
   "metadata": {},
   "source": [
    "To drop all duplicates, you can set the `keep` parameter to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dee59-34a5-49ff-8273-e69945d29610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop all duplicates\n",
    "rate.drop_duplicates(subset=['Course'], keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0922c9d9-d193-4389-95f9-a02ccd729937",
   "metadata": {},
   "source": [
    "After you remove all duplicates from a column, the values left in that column are unique. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7be21-3038-4023-9c28-4c481b1537b6",
   "metadata": {},
   "source": [
    "There is another useful method to find the unique values in a certain column in a dataframe is the `.unique()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79df845-ba79-4e2b-85c9-e073104b92a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the unique courses in df rate\n",
    "rate['Course'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ccd41b-6be7-4fd2-9587-a37f431199f5",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "In this coding challenge, we'll work with the data on the 2021 and 2022 Boston Marathon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7445eb-40b4-4f14-ae29-2c4eec328191",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dfs out of the files\n",
    "bm_21 = pd.read_csv('../All-sample-files/DataViz3_BostonMarathon2021.csv')\n",
    "bm_22 = pd.read_csv('../All-sample-files/DataViz3_BostonMarathon2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb945440-941b-4012-b1db-472483d3e39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# explore bm_21\n",
    "bm_21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4f5069-4a17-4ab8-9066-49eecf65ca8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# explore bm_22\n",
    "bm_22.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f88b2a-363b-457f-b387-ac3bff6e0764",
   "metadata": {},
   "source": [
    "Using what you have learned so far, can you find out which countries have runners in 2021 Boston Marathon but not 2022 Boston Marathon? Which countries have runners in 2022 Boston Marathon but not 2021 Boston Marathon? Let's get the countries using the `CountryOfResName` column in this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b3927-a84f-412d-9d33-8bb8b1aaf416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find out which countries have runners in 2021 but not in 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a2b5e-90a2-4524-b2ab-32af945f32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out which countries have runners in 2022 but not in 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399e071-f88f-4579-a450-cd6ccb123017",
   "metadata": {},
   "source": [
    "## Vectorized operations in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3f2c8b-b5a9-4244-a5a3-058a7acc9f60",
   "metadata": {},
   "source": [
    "As you have seen so far, a lot of the methods in Pandas can work on the values in a row or a column in a batch. For example, if you call `.isna()` on a column, it checks, for each value in that column, whether it is NaN or not. This kind of operation where values are taken and operated on in a batch is called **vectorized operations** in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a31148-a3af-405d-b2e1-e2b292d47b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a simple example of vectorized operation\n",
    "df = pd.DataFrame({'number':[1,2,3,4],\n",
    "                  'year': [2020, 2021, 2022, 2023]})\n",
    "\n",
    "# grab the 'number' column and add 2 to it\n",
    "df['number'] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de01b38-bb32-4077-ac27-dd2e7910da17",
   "metadata": {},
   "source": [
    "As you can see, the addition is applied element-wise to the integers in the `number` column. How do we understand this operation? Why don't we need to write a for loop to iterate over the values in the `number` column and add 2 to each of them? \n",
    "\n",
    "Actually, this vectorized addition can be broken down into two steps. \n",
    "* First of all, the addend 2 undergoes a process called **broadcasting**. It is stretched into a vector of the same shape as the `number` column. \n",
    "\n",
    "\n",
    "<center><img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/Pandas3_vectorized2.png\" width=\"40\"></center>\n",
    "\n",
    "\n",
    "* Second, we add the integers in the `number` column and the vector of 2s in the same way that we add two vectors in mathematics. We have **vectorized** the addition in this case. \n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/Pandas3_vector.png\" width=\"200\"></center>\n",
    "\n",
    "When we add two vectors, the two numbers in the corresponding positions in the two vectors are added together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87bf40-ab88-4c15-9435-e3cb0ff24820",
   "metadata": {},
   "source": [
    "### The importance of shape\n",
    "\n",
    "A prerequisite for applying an operation to two vectors is to make sure that they have compatible shapes. In the previous example, we are able to add 2 to the `number` column in a vectorized way because the value 2 can be **broadcast** into a vector whose shape is compatible with the `number` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0493c458-4dd7-4615-a25f-70ec1390b842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# applying an operation to two vectors of incompatible shapes\n",
    "# throws an error\n",
    "df['number'] + [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036648d-8a0a-4227-a066-cb29a0cb7f79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T16:18:12.716605Z",
     "iopub.status.busy": "2023-07-13T16:18:12.715868Z",
     "iopub.status.idle": "2023-07-13T16:18:12.732744Z",
     "shell.execute_reply": "2023-07-13T16:18:12.732257Z",
     "shell.execute_reply.started": "2023-07-13T16:18:12.716539Z"
    },
    "tags": []
   },
   "source": [
    "Note that because the vectorized operations apply element-wise, we do not need to write a for loop to iterate over the values in a column and repeat the desired operation to each value. \n",
    "\n",
    "Whenever you are tempted to write a for loop when processing data in Pandas, take a pause and think about whether there is a vectorized way to do it. In most cases, vectorized operations are **faster and more efficient**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281dd583-41d7-4ec3-ac03-34912f416f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with 1000 rows of random ints between 1 and 1000\n",
    "import numpy as np\n",
    "df_test = pd.DataFrame({'a':np.random.randint(1,1000, 1000),\n",
    "                    'b':np.random.randint(1,1000, 1000)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ea28c-ce17-4c2f-81c7-f39d56175df1",
   "metadata": {},
   "source": [
    "Let's use a for loop to add two columns and get the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f732b-4c89-44a4-a191-f61c91c29a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "for index, row in df_test.iterrows():\n",
    "    row['sum'] = row['a'] + row['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b1e1b-39e6-4bf4-b2e6-4a75ea0128ff",
   "metadata": {},
   "source": [
    "Let's add two columns in a vectorized way and get the execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1cfd9-5568-41a1-a5d4-c481989e7738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "df_test['sum'] = df_test['a'] + df_test['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a426bd-c1b2-4845-be62-19a0e9199534",
   "metadata": {},
   "source": [
    "### Some more examples of vectorized operations\n",
    "With numerical data, it is really convenient to use vectorized operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da42cad-5f37-4954-941d-93aff4ef7ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the unemployment rate in MA in feb 2023\n",
    "unemp_feb['Feb Labor Force'] = [108646,\n",
    "                                61320,\n",
    "                                300028,\n",
    "                                8403,\n",
    "                                424616,\n",
    "                                40085,\n",
    "                                225735,\n",
    "                                90085,\n",
    "                                914953,\n",
    "                                6837,\n",
    "                                395358,\n",
    "                                285373,\n",
    "                                455084,\n",
    "                                446151]\n",
    "unemp_feb['Feb Unemployment Rate'] = (unemp_feb['Feb Unemployment']/\n",
    "                                     unemp_feb['Feb Labor Force'])\n",
    "unemp_feb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942fef1-2f04-4769-a30e-3a47a0371d78",
   "metadata": {},
   "source": [
    "You have learned a bunch of string methods in [Python Basics](../Python-basics/python-basics-1.ipynb). Almost all Python's built-in string methods have counterparts in Pandas's vectorized string methods, e.g., `upper()`, `lower()`, `isalpha()`, `startswith()`, `split()`, so on and so forth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4a770-1df3-4e30-ae33-38a92f1fd54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make all county names in lower case\n",
    "unemp_jan['COUNTY'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca8c53-9e9e-4307-be12-ddc175c42df1",
   "metadata": {},
   "source": [
    "Note that we will first use `.str` to access the values in a column as strings and then call a certain method. Let's see an example of another method `.split()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7a9df-86b9-4774-aaa1-82bfc700a8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a df with some full names\n",
    "full_name = bm_21['FullName'].copy().loc[:5]\n",
    "full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cebe862-3f22-4637-bc70-8a183b1d02a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the names into first names and last names\n",
    "full_name.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf970ec-3758-4779-a392-8e422d169801",
   "metadata": {},
   "source": [
    "With strings, a common operation is to search for a certain substring in them. You have seen an example in Pandas basics 2 where we extract all failed banks whose name contains the word 'Community'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fbc66-0ece-4264-9667-d6e3754886fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a reminder of the contains() method\n",
    "full_name.loc[full_name.str.contains('Leo')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c0b23-3aab-491b-90a7-cb71d6d8c0e5",
   "metadata": {},
   "source": [
    "Another common operation with strings is to find out all strings that match a certain pattern. There are some string methods that accept regular expressions which can be used to specify patterns.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace236c2-0c23-4452-ae05-545b39a65d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# search strings of a certain pattern\n",
    "full_name.str.extract('(^L.*[nu]$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8199a-05d6-4465-8d2f-8e4a8fe4d490",
   "metadata": {},
   "source": [
    "If you are interested in learning more about regular expressions, check out [this](../Regular-expressions/regular-expressions.ipynb) constellate notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dd1eaa-c05e-4e86-aca5-cc7b5b1f2f26",
   "metadata": {},
   "source": [
    "## apply() and where()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1542c7-2563-4c49-8387-60ff7fa48a59",
   "metadata": {},
   "source": [
    "There are a lot of methods in Pandas used for data processing. In this section, we focus our attention on two: `apply()` and `where()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f48d6ed-1049-4ef1-bfab-38ab70b6247c",
   "metadata": {},
   "source": [
    "The `.apply()` method allows you to apply a self-defined function to rows and columns in a dataframe. \n",
    "\n",
    "Suppose you have a dataframe that contains the words produced by a baby each day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f50a0-010e-435b-bd09-c494bfc5bbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use apply to process data\n",
    "vocab = pd.DataFrame([[{'duck', 'dog'}, {'dog', 'bird'}],\n",
    "                    [{'hippo', 'sky'}, {'sky', 'cloud'}]],\n",
    "                    columns=['day1', 'day2'],\n",
    "                    index=['Alex', 'Ben'])\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c675dc4-5f11-4cf9-a5a8-4f01db53186c",
   "metadata": {},
   "source": [
    "Now, suppose for each baby, you would like to extract the words that appear in both days so that you know which words were consistently produced them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012a3f6-c56b-468a-a65d-564179402640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### use apply to get the intersection between sets in each row\n",
    "\n",
    "# define a function that takes a row and returns the desired intersection\n",
    "def get_common_words(r):\n",
    "    return r['day1'] & r['day2'] \n",
    "\n",
    "# pass the function to .apply()\n",
    "vocab.apply(get_common_words, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601801c6-b2c3-4de8-932f-bc19a8c9a5cd",
   "metadata": {},
   "source": [
    "If you are more comfortable with the lambda function, you can pass a lambda function directly to the `.apply()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca63f8-638f-49c7-b0eb-b60b1e6511a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pass a lambda function to apply()\n",
    "vocab.apply(lambda r: r['day1'] & r['day2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722a51eb-f47f-412a-ae98-0e2e09789c11",
   "metadata": {},
   "source": [
    "The `lambda` function may look a little bit complicated to read. A good way to read this function is to break it down into two parts. What goes before the colon is the input to this function; what goes after the colon is the output of this function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f0923-d47f-4236-82a5-ccdef7c24725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T02:36:08.899214Z",
     "iopub.status.busy": "2023-07-14T02:36:08.898523Z",
     "iopub.status.idle": "2023-07-14T02:36:08.908167Z",
     "shell.execute_reply": "2023-07-14T02:36:08.907164Z",
     "shell.execute_reply.started": "2023-07-14T02:36:08.899151Z"
    },
    "tags": []
   },
   "source": [
    "The `.where()` method allows you to manipulate the data using the if-else logic. The syntax of the `.where()` method is `.where(condition, other)`. The values fulfilling the condition will be kept and the values that do not fulfill the condition are changed to the value specified by 'other'. \n",
    "\n",
    "Suppose you are a middle school teacher. You have a report of the grades for the most English test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a816758-f689-496a-a736-68aee6dd0a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a df containing English grades\n",
    "eng = pd.DataFrame({'grade': [90, 88, 70, 55]},\n",
    "                  index=['Alice', 'Becky', 'Cindy', 'Dave'])\n",
    "eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e04080-afbb-4ba5-8f72-61b34ea96f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T12:49:54.943759Z",
     "iopub.status.busy": "2023-07-14T12:49:54.943080Z",
     "iopub.status.idle": "2023-07-14T12:49:54.958571Z",
     "shell.execute_reply": "2023-07-14T12:49:54.957543Z",
     "shell.execute_reply.started": "2023-07-14T12:49:54.943695Z"
    },
    "tags": []
   },
   "source": [
    "And you would like to change any grade below 60 to 'F'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27083c35-fa88-4699-80ee-09e8e1e31cee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use .where() to change any grade below 60 to 'F'\n",
    "eng.where(eng['grade']>=60, 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61b6b5b-8182-4098-a0d5-a151f0f586a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-14T15:53:28.426660Z",
     "iopub.status.busy": "2023-07-14T15:53:28.425954Z",
     "iopub.status.idle": "2023-07-14T15:53:28.441110Z",
     "shell.execute_reply": "2023-07-14T15:53:28.440114Z",
     "shell.execute_reply.started": "2023-07-14T15:53:28.426590Z"
    },
    "tags": []
   },
   "source": [
    "You can do the same thing using the `.apply()` method. However, the if-else logic will need to be written into the function passed into `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bc82d-9392-424f-aec0-1aaf831b98c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use apply to change the grades below 60 to 'F'\n",
    "eng['grade'].apply(lambda x: 'F' if x < 60 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58769d47-af68-466e-82be-3a0b7961e7ad",
   "metadata": {},
   "source": [
    "We have seen how to apply the if-else logic to the data in a dataframe. How about if-elif-else? Suppose you would like to change any grade 90 and above to 'A', any grade between 80-89 to 'B', 70-79 to 'C', 60-69 to 'D', and any grade below 60 to 'F'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df265a-a957-4915-9b74-2639be63b079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change the number grades to letter grades\n",
    "def convert_grade(x):\n",
    "    if x >= 90:\n",
    "        return 'A'\n",
    "    elif 80<x<89:\n",
    "        return 'B'\n",
    "    elif 70<x<79:\n",
    "        return 'C'\n",
    "    elif 60<x<69:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'F'\n",
    "eng['grade'].apply(convert_grade)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889cc0f0-5a74-43e2-a86d-590c1f268ff8",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "In this coding challenge, we'll work with the data on the 2022 Marathon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cd039-8ccf-41a4-8696-28c83683f4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the original data\n",
    "bm_22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035f19a-ca77-44e9-9714-5df10b674012",
   "metadata": {},
   "source": [
    "Can you use `apply()` to update the `OfficialTime` column such that completion time between 2 and 3 hours are changed to the string 'Extremely fast' and completion time between 3 - 4 hours are changed to the string 'fast' with all other values unchanged in this column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a558d-4334-486f-b21b-e6cb2becac8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f2b8c3",
   "metadata": {},
   "source": [
    "## Solutions to exercises\n",
    "\n",
    "Here are the solutions to some of the exercises in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34226900-5813-4bb9-9c09-a83ab76f645e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### find out which countries have runners in 2021 but not in 2022\n",
    "### this solution may look a bit convoluted to you and it's intended to be so\n",
    "\n",
    "# drop duplicate countries in bm_21 and get two columns after dropping the duplicates\n",
    "bm_21_ctry = bm_21.drop_duplicates(subset='CountryOfResName')[['CountryOfResName', 'FullName']]\n",
    "\n",
    "# drop duplicate countries in bm_22 and get the same two columns after dropping the duplicates\n",
    "bm_22_ctry = bm_22.drop_duplicates(subset='CountryOfResName')[['CountryOfResName', 'FullName']]\n",
    "\n",
    "# merge the two new dfs on the CountryOfResName column with the merge type being left outer join\n",
    "bm_merge = bm_21_ctry.merge(bm_22_ctry, how='left', on='CountryOfResName')\n",
    "\n",
    "# get the rows from the merged df where the non-matching keys in bm_22 get a value of NaN\n",
    "bm_merge.loc[bm_merge['FullName_y'].isna()==True, 'CountryOfResName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8f037-f34b-480d-95f3-7fe8ab3f7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "### find out which countries have runners in 2021 but not in 2022\n",
    "# this is a non-convoluted solution to the same exercise\n",
    "bm_21_country = set(bm_21['CountryOfResName'].to_list())\n",
    "bm_22_country = set(bm_22['CountryOfResName'].to_list())\n",
    "bm_21_country - bm_22_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2abcef-457d-46a0-bc13-f7ccad9ffbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use apply() to change the completion time\n",
    "def convert_time(r):\n",
    "    if r['OfficialTime'].startswith('2'):\n",
    "        r['OfficialTime'] = 'Extremely fast'\n",
    "    elif r['OfficialTime'].startswith('3'):\n",
    "        r['OfficialTime'] = 'Fast'\n",
    "    return r\n",
    "bm_22.apply(convert_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7128ace6-9824-41ed-baf8-5a9f576c17d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use lambda function in apply to change the completion time\n",
    "bm_22['OfficialTime'] = bm_22['OfficialTime'].apply(lambda x: 'Extremely fast' \\\n",
    "                                                    if x.startswith('2') \\\n",
    "                                                    else ('Fast' if x.startswith('3') else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae906633-70e6-4c6b-be62-bca15c7dc12e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
