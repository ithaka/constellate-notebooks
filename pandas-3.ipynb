{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fac83f3-4536-4584-870a-6bcfec93ea68",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "This notebook is created by Zhuo Chen under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email zhuo.chen@ithaka.org or nathan.kelber@ithaka.org<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbef364-f331-462d-96f1-4cd736b4dcec",
   "metadata": {},
   "source": [
    "# Pandas 3 \n",
    "\n",
    "**Description:** This notebook describes how to:\n",
    "* Build a dataset from Constellate\n",
    "* Make a dataframe from the dataset\n",
    "* Summarize data in a dataframe\n",
    "* Group and aggregate data\n",
    "* Make pivot tables in Pandas\n",
    "\n",
    "This is the third notebook in a series on learning to use Pandas. \n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Knowledge Required:** \n",
    "* [Pandas 1](./pandas-1.ipynb)\n",
    "* [Pandas 2](./pandas-2.ipynb)\n",
    "* Python Basics ([Start Python Basics I](./python-basics-1.ipynb))\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* [Python Intermediate 1](./python-intermediate-1.ipynb)\n",
    "* [Python Intermediate 2](./python-intermediate-2.ipynb)\n",
    "* [Python Intermediate 4](./python-intermediate-4.ipynb)\n",
    "\n",
    "**Completion Time:** 60 minutes\n",
    "\n",
    "**Data Format:** JSONL \n",
    "\n",
    "**Libraries Used:** Pandas\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af26b1",
   "metadata": {},
   "source": [
    "# Build a dataset from Constellate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46d211",
   "metadata": {},
   "source": [
    "The dataset we are going to use for today's lesson is the documents from JSTOR or Portico with the key word \"machine learning\" or \"artificial intelligence\" about Arts, History, Philosophy, Religion limited to document type(s) article, chapter, book from 2011 - 2020. There are 12,286 documents in this dataset in total, but in class, we will only use the sampled 1500 documents from this dataset. \n",
    "\n",
    "After we build a dataset, we will use the [Constellate Client](https://pypi.org/project/constellate-client/) to download the dataset and read in the documents. \n",
    "\n",
    "If you are running the notebook locally and have never installed the Constellate Client before, you may need to install the Constellate Client using `!pip install constellate-client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ecd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import constellate\n",
    "import constellate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7163ed",
   "metadata": {},
   "source": [
    "After you build a dataset using the Constellate builder, it will show up in the section of `All datasets` in your dashboard. A dataset id will be assigned to your dataset. To download the dataset, you will need to use the dataset id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887ff28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a variable `dataset_id` to hold our dataset ID\n",
    "dataset_id = 'd6232206-93bf-f6b8-9ad2-b2add01cf231'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f7f51",
   "metadata": {},
   "source": [
    "The [Constellate Client]((https://constellate.org/docs/constellate-client)) has several methods we can use to download a dataset. You can choose the type of data you would like to download. The data available for downloading include the metadata, full data, unigrams, bigrams and trigrams. \n",
    "\n",
    "The `get_metadata()` method downloads the dataset metadata (sampled to 1500 documents) for a dataset in csv format.\n",
    "\n",
    "The `get_dataset()` method downloads the dataset full data  (sampled to 1500 documents) in the Constellate Document Format (jsonl).\n",
    "\n",
    "The `download()` method can download the non-sampled metadata, full data and ngram counts. \n",
    "\n",
    "See the Constellate Client documentation at: https://constellate.org/docs/constellate-client for more details. \n",
    "\n",
    "Here in the example, we download the sampled 1500 documents using the `get_dataset()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74ccac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use get_dataset() to download the sampled dataset\n",
    "# and give the file a name \n",
    "dataset_file = constellate.get_dataset(dataset_id, 'ML_AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at where the dataset is downloaded to\n",
    "print(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4597e53",
   "metadata": {},
   "source": [
    "If you would like to see where you downloaded file is located, follow these steps. \n",
    "\n",
    "1. Go to `File -> Open`. \n",
    "2. On the upper righthand corner, go to `New ->Terminal`. \n",
    "3. In the Terminal, type the following command `cd ../root/data`. Then type `ls`, you will see the downloaded file. \n",
    "\n",
    "<center><img src='https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/Pandas3_Terminal.png' width=1000></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c58119",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "After we download the dataset, we can use the `dataset_reader()` method to read in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6a5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the .dataset_reader() method to read in the documents\n",
    "docs = constellate.dataset_reader(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c2a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the type of docs\n",
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f883602",
   "metadata": {},
   "source": [
    "Recall from [Python Intermediate 5](./python-intermediate-5.ipynb) that the difference between a list and a generator is that the latter yields only one element at a time. As a result, generators are more memory-efficient than lists. \n",
    "\n",
    "To return the elements in a generator one by one, we use the `next()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257afb6d-ecf5-466a-9ab5-97ceefcac854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Take a look at the first element of the generator docs\n",
    "doc1 = next(docs)\n",
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82ecf9",
   "metadata": {},
   "source": [
    "We can see that the document is loaded as a Python dictionary. You can confirm that it is a dictionary by checking the type of `doc1` using the `type()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use type() to check the type of doc1\n",
    "type(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df0592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all keys from the dict\n",
    "doc1.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d628fb7f-2378-4518-b77b-e54b828f75ee",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Now you know that the full data of a Constellate dataset is read into a generator of dictionaries. Here, let's get the sample dataset we have used in [Pandas 2](./pandas-2_ipynb) again. The sample dataset contains all documents from JSTOR published in Shakespeare Quarterly from 1950 - 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad33160-fbe3-49b2-9a39-aa5c6a559f21",
   "metadata": {},
   "source": [
    "First of all, we read in the data from the dataset and inspect the first document from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e6a08a-fc58-4535-9b22-78864f8fa05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the full data sampled to 1500 documents \n",
    "dataset_id = 'f6ae29d4-3a70-36ee-d601-20a8c0311273'\n",
    "\n",
    "# Use constellate.get_dataset() to download the dataset(sampled to 1500 documents)\n",
    "path = constellate.get_dataset(dataset_id, 'Shake')\n",
    "\n",
    "# Read in the data from the dataset using .dataset_reader()\n",
    "docs_shake = constellate.dataset_reader(path)\n",
    "\n",
    "# Grab the first document from the dataset\n",
    "doc1_shake = next(docs_shake)\n",
    "\n",
    "# Get the keys from the first document doc1_shake\n",
    "doc1_shake.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2d0b5",
   "metadata": {},
   "source": [
    "Can you follow the prompts below to create a dataframe out of the Shakespeare dataset? Use what you have learned from [Python Basics](./python-basics-1.ipynb), [Pandas 1](./pandas-1.ipynb) and [Pandas 2](./pandas-2.ipynb) to do this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f9495",
   "metadata": {},
   "source": [
    "From the keys of the first document, you get an idea of what kind of data are provided for each document in the dataset. In the next code cell, select the keys that are of interest to you and put them in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cd38f-5381-4ed5-aec2-439c74f67a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the keys that are of interest to you and\n",
    "# put them in a list \n",
    "keys_of_interest_shake = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5aadd8-7709-41c0-ba41-788a4a626446",
   "metadata": {},
   "source": [
    "Create a dataframe storing the data of interest to you. The headers of the dataframe are the keys of interest you have just selected. Each row of the dataframe contains the relevant data from one document of the dataset. For example, if you choose 'id' and 'publicationYear' as the keys of interest. Then, the first row of the dataframe will have the id of the first document in the 'id' column and the publication year of the first document in the 'publicationYear' column. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a5c393",
   "metadata": {},
   "source": [
    "To help you figure out how to create the dataframe, I'll use a very simple example to illustrate. Suppose the only information you are interested in is the title of the documents in the Shakepeare dataset. How will we create a one-column dataframe storing the titles of the documents? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example \n",
    "\n",
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list storing all documents titles\n",
    "titles = [] \n",
    "for doc in docs_shake: \n",
    "    titles.append(doc['title'])\n",
    "    \n",
    "# Create a dataframe with only one column called 'title'\n",
    "# with each row storing the title of one document\n",
    "pd.DataFrame({'title':titles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac60f8",
   "metadata": {},
   "source": [
    "Now, create a dataframe storing the data of interest to you from the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f6bd0-adf6-4ba6-9ea5-8854a16e8620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the docs again\n",
    "docs_shake = constellate.dataset_reader(path)\n",
    "\n",
    "# Create a dataframe \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac923c8c",
   "metadata": {},
   "source": [
    "## Create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ddbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable `dataset_id` to hold our dataset ID\n",
    "dataset_id = 'd6232206-93bf-f6b8-9ad2-b2add01cf231'\n",
    "dataset_file = constellate.get_dataset(dataset_id, 'jsonl', 'ML_AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1905848",
   "metadata": {},
   "source": [
    "Suppose not all data in the documents are of interest to us. Let's select the data we are interested in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c74dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data of interest\n",
    "data_of_interest = ['id', 'title', 'docType', 'publicationYear', 'bigramCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b520a-2458-45f1-a1fb-506eb760bbe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the docs again\n",
    "docs = constellate.dataset_reader(dataset_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5f56b",
   "metadata": {},
   "source": [
    "From each doc in docs, we want to grab the values corresponding to the keys in the list of `data_of_interest` and create a dataframe from the data. For a quick review of list comprehensions, take a look at [Python Intermediate 1](./python-intermediate-1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6714596",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the data we need for creating a dataframe\n",
    "data = [\n",
    "            [doc['id'], \n",
    "             doc['title'], \n",
    "             doc['docType'], \n",
    "             doc['publicationYear'], \n",
    "             doc['bigramCount']\n",
    "            ] \n",
    "            for doc in docs\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264828c4-7fd1-4d79-b6cc-2d776e12444d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "df = pd.DataFrame(data, columns=['id', 'title', 'docType', 'publicationYear', 'bigramCount'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4436d-cb79-4ac6-9f2f-d6ee43b7e9a2",
   "metadata": {},
   "source": [
    "## Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aba96",
   "metadata": {},
   "source": [
    "We will often need to do some data cleaning and pre-processing after we create a dataframe. What kind of data cleaning and pre-processing you need to do depends on the specific task at hand. Here, we only give some examples. \n",
    "\n",
    "When we look at the 'id' column, we can see that all document ids start with \"ark://27972/\". We can get rid of this prefix and use the rest of the string as the ids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f00766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shorten the ids\n",
    "prefix_len = len(\"ark://27972/\")\n",
    "def shorten_id(r): \n",
    "    r['id'] = r['id'][prefix_len:]\n",
    "    return r\n",
    "df = df.apply(shorten_id, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dc665-6b4d-4639-9a8b-c35c35153eb0",
   "metadata": {},
   "source": [
    "The bigramCount column gives the number of occurrences of every bigram string in a document. As you can see, the puntuations do not count as a gram. This is why 'a Mistake?' is seen as a bigram, not a trigram. With this in mind, let's make a new column storing the count of the bigram 'machine learning' and a new column storing the bigram 'artificial intelligence'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e96453-464f-4a05-ac26-8d18fe0592f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example solution \n",
    "len_ML = len('machine learning')\n",
    "\n",
    "# Define a function to return the count of 'machine learning' and 'artificial intelligence'\n",
    "def count(r):\n",
    "    count_ML = 0\n",
    "    count_AI = 0\n",
    "    for key in r['bigramCount'].keys():\n",
    "        key_lower = key.lower()\n",
    "        if len(key)<len_ML:\n",
    "            continue\n",
    "        elif 'machine learning' in key_lower:\n",
    "            count_ML += r['bigramCount'][key]\n",
    "        elif 'artificial intelligence' in key_lower:\n",
    "            count_AI += r['bigramCount'][key]\n",
    "    return [count_ML, count_AI]\n",
    "\n",
    "# Create a column with the count of 'machine learning'\n",
    "# and a column with the count of 'artificial intelligence'\n",
    "df[['ML_count', 'AI_count']] = df.apply(count, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b539524-c3f4-41f8-ad79-9ec8d9003da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the updated df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411228cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the bigramCount column\n",
    "df = df.drop('bigramCount', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1dde9b",
   "metadata": {},
   "source": [
    "## Group and aggregate data\n",
    "\n",
    "After data cleaning, filtering and preprocessing, the next step is to summarize the data to extract useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345dccf",
   "metadata": {},
   "source": [
    "Pandas makes summarising a dataframe very easy. For example, we can count how many non-null values there are in each column using the `.count()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514212fd-0a20-4693-a123-a0a38526490a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of non-null values in each column\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5d271-1a71-4370-935f-c20a8166cb93",
   "metadata": {},
   "source": [
    "We can also get the max value or the min value of a column using the `.max()` and `.min()` methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e614-44aa-40a6-942a-bcb6ecc74198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the max value from the year column\n",
    "df['publicationYear'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03e90c3-9831-4645-841c-bbab9a67848a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the min value from the year column\n",
    "df['publicationYear'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c345c-9e7b-4d86-b222-f06f29de02b2",
   "metadata": {},
   "source": [
    "You can refer to the Pandas documentation for more methods that you can use to query the data. \n",
    "\n",
    "When you summarize a dataframe, a very useful method is `.describe()`. It can quickly display the statistics for any group of data it is applied to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d7834-7918-4275-807e-1e5a0cd32fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the .describe() method to explore the year column\n",
    "df['title'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd5c24-aee2-44a8-bffd-a0547914b698",
   "metadata": {},
   "source": [
    "### Groupby()\n",
    "\n",
    "Groupby is a powerful function built into Pandas that you can use to summarize your data. Groupby splits the data into different groups on a variable of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c39249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group the data by docType\n",
    "df.groupby('docType')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c22429-0eea-4110-8f60-027f3be9fff0",
   "metadata": {},
   "source": [
    "The groupby() method returns a GroupBy object which describes how the rows of the original dataset have been split by the selected variable. You can actually see how the rows of the original dataframe have been grouped using the `groups` attribute after applying `groupby()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751abaf1-7595-44e0-b705-4488ba2972ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how the rows have been grouped\n",
    "df.groupby('docType').groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d26d3-1793-4137-ac6f-69b35e90865e",
   "metadata": {},
   "source": [
    "As you can see, a dictionary is returned whose keys are the unique values in docType and whose values are lists of row indexes. Each key corresponds to a list of row indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa87067-707f-4fef-bd6c-6b25b7dd7c7d",
   "metadata": {},
   "source": [
    "You can group the data using multiple variables. For example, you may want to group the documents first by their publication year and then by the document type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbad271-7b0d-49b0-ba25-128ab50bd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple variables \n",
    "# Take a look at the composite keys\n",
    "df.groupby(['publicationYear', 'docType']).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3741f",
   "metadata": {},
   "source": [
    "If you take a look at the groups in the groupby object, you will see that essentially we have a composite key for each group. The first key, for example, is (2011, 'article'). The value associated with this key is a list of indexes, all of which are the rows storing the documents that were published in 2011 and are of the docType 'article'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed5b819-dd08-46df-9378-5b1915d26485",
   "metadata": {},
   "source": [
    "Of course, we don't just stop at grouping data. Grouping data is just a step towards data query. After we apply the `.groupby()` method, we can actually use different Pandas methods to query the data. For example, how do we get the number of documents in each docType by publicationYear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cbcd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a series storing the number of documents in each doc type by year\n",
    "df.groupby(['publicationYear', 'docType']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b3302-c684-4504-8366-bc435e8e85c8",
   "metadata": {},
   "source": [
    "### Agg() \n",
    "\n",
    "After we group the data in a dataframe, we can apply the `agg()` method to calculate multiple statistics per group in one calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3208ee0-8af7-4aca-afe8-95022f1f7a54",
   "metadata": {},
   "source": [
    "For example, let's say we would like to know the sum of the occurrences of the bigram 'machine learning' in all the documents from each year. To achieve this goal, we can group the data by `publicationYear`, and then aggregate the data by summing the numerical values in the column of `ML_count` for each subgroup.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bed1b-ef1c-4cdc-9209-4cd64e83a856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get how many times 'machine learning' is\n",
    "# mentioned in the docs each year\n",
    "df.groupby('publicationYear').agg({'ML_count':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823bee7-1676-467c-a949-5eac4e5755d2",
   "metadata": {},
   "source": [
    "Of course, you can choose other ways to aggregate the data in each subgroup. For example, suppose you are interested in the biggest frequency with which a document mentions 'artificial intelligence' by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15539a07-4726-49f5-a55c-a3cff7ad78d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the biggest frequency of a document mentioning \n",
    "# 'artificial intelligence' by year\n",
    "df.groupby('publicationYear').agg({'AI_count':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd639145-c660-46fc-b1a9-99d347c5801c",
   "metadata": {},
   "source": [
    "We can specify multiple columns to apply a function to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30901a-1fde-4192-ba9b-b22cdcc0bed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply a single function to selected columns in each subgroup\n",
    "df.groupby('publicationYear').agg({'AI_count':'sum', 'ML_count': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06350d24-1b26-4269-8672-7d02cc79cb61",
   "metadata": {},
   "source": [
    "We can also apply multiple functions to each of the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab56a3-7e0b-48a8-955f-af482dae4386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply multiple functions to selected columns in each subgroup\n",
    "df.groupby('publicationYear').agg({'AI_count':['sum', 'max'], 'ML_count':['max', 'count']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c819d1d-edd4-457c-87b9-d9acd4e96acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T22:41:09.904787Z",
     "iopub.status.busy": "2023-03-23T22:41:09.904101Z",
     "iopub.status.idle": "2023-03-23T22:41:09.926725Z",
     "shell.execute_reply": "2023-03-23T22:41:09.926110Z",
     "shell.execute_reply.started": "2023-03-23T22:41:09.904723Z"
    },
    "tags": []
   },
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Take the following dataframe containing the information on the Covid19 cases in the state of Massachusetts. Can you work with the dataframe to find out which month of which year has the most positive new cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbda0c5-3d81-4cd7-94f2-8408802c6081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Get the data of covid19 cases in MA and create a dataframe \n",
    "\n",
    "# Download the .csv file\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "url = 'https://www.mass.gov/doc/covid-19-raw-data-march-9-2023/download'\n",
    "urllib.request.urlretrieve(url, './data/covid_MA.csv')\n",
    "\n",
    "# Success message\n",
    "print('Sample file ready.')\n",
    "\n",
    "# install the openpyxl library\n",
    "!pip install openpyxl\n",
    "\n",
    "# Read in the sheet containing the info about positive cases\n",
    "covid_ma = pd.read_excel('./data/covid_MA.csv', 'Cases (Report Date)')\n",
    "\n",
    "# Change the dtype of the Date column for later use\n",
    "covid_ma['Date'] = covid_ma['Date'].astype(str)\n",
    "\n",
    "# Take a look at the dataframe\n",
    "covid_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e73832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which month of which year has the most positive new cases\n",
    "# Note that the dtype for the values in Date column is str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a332c2-bdbd-4959-8e6c-9c2037af0592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-23T23:45:35.059972Z",
     "iopub.status.busy": "2023-03-23T23:45:35.059285Z",
     "iopub.status.idle": "2023-03-23T23:45:35.081367Z",
     "shell.execute_reply": "2023-03-23T23:45:35.080850Z",
     "shell.execute_reply.started": "2023-03-23T23:45:35.059908Z"
    },
    "tags": []
   },
   "source": [
    "## Make pivot tables in Pandas\n",
    "\n",
    "Pandas has a `.pivot_table()` method we can use to summarize data. It takes a dataframe as argument and has parameters specifying the shape of the pivot table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b8775-fe8d-4fc0-8384-82c1f8c1585d",
   "metadata": {},
   "source": [
    "In the previous section, we have used the `.groupby()` and `agg()` methods to summarize data. For example, we grouped the documents in the dataframe df by their year of publication and calculated the sum of the mentions of the bigram 'artificial intelligence'. We can do the same thing using the `.pivot_table` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3989f0a-095b-4fa5-9643-e69d799a18cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of \n",
    "# the mentions of 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values='AI_count',\n",
    "                      aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e2f88-b916-4e19-8bbf-559cd40aea47",
   "metadata": {},
   "source": [
    "Again, when aggregating the data, you can apply a single function to multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec95c4-8f78-4ac7-92c1-dcd980e6e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of \n",
    "# the mentions of 'machine learning' and 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values=['AI_count', 'ML_count'],\n",
    "                      aggfunc='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3895b079-d235-4106-b0b2-138835662439",
   "metadata": {},
   "source": [
    "You can also apply multiple functions to a single column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd494ac2-f741-4225-a298-6b6cc0b6745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum and the max value of\n",
    "# the mentions of 'artificial intelligence' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values='AI_count',\n",
    "                      aggfunc=['sum', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc07c4a-65a4-4460-b439-8ffc83fa8bea",
   "metadata": {},
   "source": [
    "Or, you can apply different functions to different columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14007f22-f2fe-4a51-816b-a783a646052d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pivot table giving the sum of\n",
    "# the mentions of 'artificial intelligence' by year\n",
    "# and the max value of the mentions of 'machine learning' by year\n",
    "df.pivot_table(index='publicationYear', \n",
    "                       values=['AI_count', 'ML_count'],\n",
    "                      aggfunc={'AI_count':'sum', 'ML_count':'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc76b40-f319-4099-8ea8-e8c7ebcfd3ec",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red; display:inline\">Coding Challenge! &lt; / &gt; </h2>\n",
    "\n",
    "Get the dataframe stored in the variable `covid_ma`. Can you make a pivot table showing the sum of the positive cases from 2020 - 2023 in that table? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53047da1-15ce-4c9a-8eab-45ec10055e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c73e1ef9-49e3-4a93-b6a9-db4878bbb2d0",
   "metadata": {},
   "source": [
    "## A teaser for the Data Visualization class\n",
    "\n",
    "We have learned how to create a dataset from Constellate, how to preprocess the data and how to summarize the data. With the information we get from summarizing the data, we can go ahead and plot it!\n",
    "\n",
    "For example, let's plot the number of docs that mentioned 'artificial intelligence' or 'machine learning' from 2011 - 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bddf0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the graph inside the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9cc31-f218-42c1-aa24-2d89c88bb60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the dataframe for plotting\n",
    "df.groupby('publicationYear').size().plot(kind='bar', ylabel='num_doc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9adb7-a4bd-4840-9b87-9bac380767c7",
   "metadata": {},
   "source": [
    "___\n",
    "# Lesson Complete\n",
    "Congratulations! You've completed the *Pandas* series. \n",
    "\n",
    "Considering the amount of material in *Pandas 1-3* there's a good chance you won't retain it all. That's okay. Programmers often need to look up things to accomplish a task they haven't done in a while, particularly if it is in a language they don't often use. When you're working on a project, you can always come back to these lessons as reference materials. In other words, you've learned an incredible amount, so don't be surprised if it doesn't all stick at first.\n",
    "\n",
    "If you want to help yourself retain what you've learned, the best way is to start putting it into practice. Try your hand at creating some small Pandas projects and recognize that the things you've learned here will cement with time and practice. When you do forget a particular thing&mdash;as we all do&mdash;a quick web search often turns up some useful examples.\n",
    "\n",
    "## Start a Text Analysis Lesson:\n",
    "* [Exploring Metadata](./exploring-metadata.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2b8c3",
   "metadata": {},
   "source": [
    "## Solutions to exercises\n",
    "\n",
    "Here are the solutions to some of the exercises in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find out which month of which year has the most positive cases of covid19 in MA\n",
    "\n",
    "# Download the .csv file\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "# Check if a data folder exists. If not, create it.\n",
    "data_folder = Path('./data/')\n",
    "data_folder.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "url = 'https://www.mass.gov/doc/covid-19-raw-data-march-9-2023/download'\n",
    "urllib.request.urlretrieve(url, './data/covid_MA.csv')\n",
    "\n",
    "# Success message\n",
    "print('Sample file ready.')\n",
    "\n",
    "# Read in the sheet containing the info about positive cases\n",
    "covid_ma = pd.read_excel('./data/covid_MA.csv', 'Cases (Report Date)')\n",
    "\n",
    "# Change the dtype of the Date column for later use\n",
    "covid_ma['Date'] = covid_ma['Date'].astype(str)\n",
    "\n",
    "# Extract the year and month from the date column\n",
    "covid_ma['group_var'] = covid_ma['Date'].apply(lambda r: r.rsplit('-', 1)[0])\n",
    "\n",
    "# Use the new column as a grouping variable and divide the data into subgroups\n",
    "# Aggregate the data using 'sum' function and sort the results in descending order\n",
    "covid_ma.groupby('group_var').agg({'Positive New':'sum'}).sort_values('Positive New', ascending=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c4c99-6865-463f-8814-345a7c8c4ec3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Make a pivot table showing the sum of positive covid19 cases by year in covid_ma\n",
    "\n",
    "covid_ma['Year'] = covid_ma['Date'].str.slice(0,4)\n",
    "covid_ma.pivot_table(index='Year',\n",
    "                    values='Positive Total',\n",
    "                    aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
