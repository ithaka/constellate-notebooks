{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9357d7-7721-4d80-8533-37241c93753e",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e77886-9fb4-4afc-948b-0191b4941272",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Models 1\n",
    "\n",
    "**Description:** This lesson offers some examples of language models, giving a basic outline of concepts such as:\n",
    "\n",
    "* Historical Approaches to NLP\n",
    "* Word embeddings\n",
    "* Transformers\n",
    "\n",
    "Learners will use the Gensim and ü§ó Transformers library to explore aspects of language models including:\n",
    "\n",
    "* Word Vectors\n",
    "* Text Generation\n",
    "* Sentiment Analysis\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Summarization\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "* Pandas Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "* Pandas Intermediate\n",
    "* A Basic Grasp of Neural Networks\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** \n",
    "* [Gensim](https://radimrehurek.com/gensim/)- for examining word embeddings\n",
    "* [ü§ó Transformers](https://huggingface.co/docs/transformers/index)- provides APIs and tools to easily download and train pretrained models\n",
    "* [Pytorch](https://pytorch.org/)- a popular machine learning framework\n",
    "* [xFormers](https://github.com/facebookresearch/xformers)- for improving transformer computation speed\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353d4d-022a-4bdc-a990-af4c22f160ae",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Note:</h3>\n",
    "\n",
    "Language models come in many sizes. The models for this notebook were tested on the given tasks, but for other models/tasks it is a good idea to check the model size and requirements. If you load or use a language model that is too big, you may fill all of the available space (10 GB) and/or memory (8 GB) in your lab. If the memory is full, try restarting the kernel (or restarting the lab). If the disk space is full, before deleting your own files, delete the .cache directory to clear out downloaded models from your space. You can do this by running the following code cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fe65c-f54e-423c-b35d-bbcc60a5cac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the .cache folder\n",
    "!rm -r /home/jovyan/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc6e99-1b0e-4001-9a0f-d7459cf5cc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check current disk space usage\n",
    "!df -h /home/jovyan/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0f886-4bec-4118-82b7-2948a0ffd124",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c1bf6-8a19-4a20-ac39-c342ccedc47c",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3f28-ceb3-4d11-8516-23202371a1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Click, a dependency for Gensim\n",
    "!pip install click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1236011-051a-4964-b416-9963252f16b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Pytorch, a popular machine learning framework\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473e54f-bf9d-44b1-90e7-782e0522973e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install en_core_web_md, the spaCy English pipeline with word embeddings\n",
    "#!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2db38d-5dce-4860-8036-286a6f7f5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install ü§ó Transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47850e62-ae76-41de-94e1-39c50ef15348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Xformers\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235be26-6532-4494-aad9-ffc265707478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Sentencepiece, a  a subword tokenizer and detokenizer for natural language processing\n",
    "# that uses byte-pair-encoding (BPE)\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c3796-254e-4846-9b7f-d5750a07514a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sacremoses, a Python port of the Moses tokenizer\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2d815-cceb-4cd8-a885-260d441ba401",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a19ebe-f810-4262-9d51-075a2091c312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a123506-c9eb-4177-ba31-f5681e9f58f2",
   "metadata": {},
   "source": [
    "# Word Vectors with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346c604-2eaa-4501-95f2-57263c9a07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the pre-trained embeddings available from Gensim\n",
    "list(gensim.downloader.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6879f-e493-4e3b-9aa5-184035551f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download an example set of pretrained embeddings: glove-wiki-gigaword-100\n",
    "# \"Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/)\n",
    "# The final number for each model is the number of dimensions/parameters trained, more parameters means larger size files but hopefully also more accurate vectors\n",
    "trained_model = gensim.downloader.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a486ee-f6e1-41a1-93db-f64a35f73dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the most similar words to a given word\n",
    "trained_model.most_similar('fish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452bdb0-05c9-4130-811f-b0f26823665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the most famous example used to describe Word Vectors\n",
    "trained_model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a46a3-1201-4e4a-bfcd-9da613be8b4a",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "By default, the ü§ó Transformers library text generation pipeline uses the Generative Pre-trained Transformer 2 (GPT-2) model by [OpenAI](https://openai.com/). This is a precursor of GPT-3.5, the model used for ChatGPT. This model was released in 2019 and you can find more information by reading its [model card](https://huggingface.co/gpt2/tree/main) on the ü§ó Transformers website. We include here several parameters:\n",
    "\n",
    "* `set_seed` Remove the randomness of the text generation by supplying the same seed value each time.\n",
    "* `prompt` The prompt that the text generator uses to build the sequence.\n",
    "* `max_length` The length of the text returned. More text requires more time and the limit is defined by the model.\n",
    "* `num_return_sequences` Allows more than one sequence to be returned for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bb0d0-bd68-4ca9-9fdd-a8b809ae1ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "input_text = \"The Legend of Zelda: Tears of the Kingdom is a video game that\"\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def create_text(prompt):\n",
    "    #set_seed(42)\n",
    "    return generator(prompt, max_length=100, num_return_sequences=3)\n",
    "\n",
    "create_text(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb8929-b2f3-4d3d-976c-c2513135a84b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "By default, the ü§ó Transformers library text-classification pipeline uses the [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model. This model is based on a distilled, uncased version of [BERT](https://huggingface.co/bert-base-uncased) that has been fine-tuned on the [Stanford Sentiment Treebank 2](https://huggingface.co/datasets/sst2) (SST-2) dataset. The SST-2 dataset is a binary classification dataset for training models to learn the sentiment of words, phrases, and sentences. It contains 215,154 unique manually labeled texts of varying lengths. The model card describes SST-2:\n",
    "\n",
    ">\n",
    "The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fdf6b-b3d1-44e6-96fa-5d84f5f71574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Prompts\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "I really wanted to like this game as I enjoyed the first one on the switch. Problem is, tears is FAR more difficult than the original. It‚Äôs not a casual game at all anymore. It takes far too much time, effort, research (online) to figure out where to find hidden areas and there are lots of ‚Äústuck points‚Äù (dead ends) that have no way of getting out except going to previous game save.\n",
    "\n",
    "Pros:\n",
    "1. Beautiful graphics and environments\n",
    "2. Mending/Attaching, Ascending\n",
    "3. Dynamic loading of zones (developers should be commended)\n",
    "\n",
    "Cons:\n",
    "1. The controls are lazy. The jump button is at the top, sprint is where jump should be. Attack is on opposite of where every other game is. It‚Äôs equivalent of having one App on an iPhone that doesn‚Äôt use basic swipe functionality. Quite frankly, it‚Äôs bad design\n",
    "2. The first Zelda showed what/where you‚Äôre supposed to do/get to in a shrine via a quick cut scene. This one does none of that. What‚Äôs worse, is that on the sky island, you‚Äôre actually given a new power and then if you leave (because the puzzle is obscenely hidden) - you don‚Äôt actually have the power. Puzzles are 10x‚Äôs harder. Example of nearly impossible puzzles use the rewind power (talk about next level difficulty: there‚Äôs a two clock handle gate puzzle that took me two days to get through. One puzzle!)\n",
    "3. Far more fighting/combat. Takes away the fun experience of exploring and talking to people doing side quests. And combat is just far more difficult in this game. The first robot shrine to train you, is just hard/horrible for a first tutorial.\n",
    "4. Sky island (first zone) is just overall at a ridiculously high level of difficulty. I got so disappointed (too much running around and end areas/or falling) that I abandoned the game for a few weeks and eventually came back. I had to watch several YouTube videos to figure out where the 4th shrine entrance was in a dark cave.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "Where to begin with this game? Did you enjoy Breath of the Wild? Were you frustrated by certain aspects? If the answer was yes to both, you‚Äôll love this game.\n",
    "\n",
    "I have been a Zelda fan all the way back to playing Zelda 2 when I was 4. And when Breath of the Wild came out, I was blown away, but also frustrated by aspects of it. Yes the game did away with most of the standard Zelda formula, but enough still remained for me to enjoy it as a Zelda game. What blew me away though was the size of the game, the focus on exploration and many other things. However, many things frustrated me like how long it took to get around, warping to shrines was easy, but losing your horse was always a serious pain.\n",
    "\n",
    "Now we arrive at the new game Tears of the Kingdom, clearly a sequel to BOTW that uses the same map, but manages to keep it fresh by introducing sky islands and the depths. And first off, if you felt BOTW had a massive map, this will blow you away. There is so much more to explore, but yet the game moves SO much faster by changing the towers from things you have to slowly climb to activate to just making them usually have a small puzzle to get them activated. Once you have these towers up, the game gets MUCH easier to navigate as each launches you miles into the sky where you can glide down toward the next closest one, or perhaps a shrine or other notable area. This eliminated the problem I had with horses and in fact, I barely used mine over the course of the entire game.\n",
    "\n",
    "The new powers you gain at the start also open this up to a realm of creativity previously offered by other games like Minecraft, but in a Zelda game, it feels fresh. I can only say this game will feel like a dream to any engineers or someone with an interest in building. You can truly get lost in crafting weapons or vehicles, but even if only done when necessary it‚Äôs a lot of fun. I who think the shrine puzzles have gotten much harder (at least for someone like me), but they were still fun and brilliant to experience.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdd0f1-4e47-4d5a-b879-aafabec29d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Pipeline\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553196f-fda6-47db-85d5-506b1d02f629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(prompt):\n",
    "        output = classifier(prompt)\n",
    "        return output\n",
    "\n",
    "classify_sentiment(prompt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccb6a9-45ff-41bc-a1ab-e8225fc7e041",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "The `aggregation_strategy` parameter defines the strategy used to group entity tokens together, like \"New York\". Remember, the tokenization may also be at the subword level, so you could see \"Microsoft\" broken up into \"Micro\" and \"soft\". Additional aggregration strategies such as \"first\", \"average\", and \"max\" are discussed in the ü§ó Transformers [documentation](https://huggingface.co/transformers/v4.7.0/_modules/transformers/pipelines/token_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a95b8c-a1db-41fd-8f58-d10497755967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Named Entity Recognition Pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f69d8-3e28-4847-a7e0-2e718d4f626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(prompt):\n",
    "    output = ner_tagger(prompt)\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "extract_entities(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df519-beab-48fd-816b-da277f4da864",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "The Question Answering pipeline has two required parameters: \n",
    "\n",
    "* `question` The question being asked\n",
    "* `context` The source material that should be used to answer this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a459-f47e-4771-8867-a696bc30b8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "reader = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbd425-4e64-4105-9561-9f7186e23bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the best parts of Tears of the Kingdom?\"\n",
    "\n",
    "def answer_question(question):\n",
    "    output = reader(question=question, context=prompt2)\n",
    "    return pd.DataFrame([output])\n",
    "\n",
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e817-dae4-44a2-a9e8-20603c2361bc",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "The `clean_up_tokenization_spaces` parameter removes extraneous spaces created through the detokenization process. If tokenization breaks up a string into separate tokens, then detokenization joins together a series of tokens into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167b118-95ff-4d95-b2ab-7f7d755a9b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363750c-93eb-4bae-99f3-f13c1a055e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    outputs = summarizer(text, max_length=75, clean_up_tokenization_spaces=True)\n",
    "    return outputs[0]['summary_text']\n",
    "\n",
    "print(summarize(prompt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b9a4-dcb3-491b-abaa-fd8100235a73",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "The translation pipeline may have length limitations based on the model selected. If your text is long, you may need to break it up into smaller chunks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32c484-a641-4dc4-8987-409471734443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk1 = prompt2[:952]\n",
    "chunk2 = prompt2[952:]\n",
    "\n",
    "translator = pipeline('translation_en_to_de', model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(chunk1, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(chunk1 + '\\n')\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc52c0-1a62-46bb-8db6-805e62c7c406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt2[952:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ef337-d6de-442c-b760-e62e86fa01d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
